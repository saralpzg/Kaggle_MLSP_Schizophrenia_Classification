{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter tunning libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ca4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"../data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"../data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on=\"Id\", right_on=\"Id\")\n",
    "data = pd.merge(left=train_labels, right=train, left_on=\"Id\", right_on=\"Id\")\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74697a",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0f9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aae0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"../data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"../data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test_kaggle = pd.merge(left=testFNC, right=testSBM, left_on=\"Id\", right_on=\"Id\")\n",
    "test_kaggle.drop(\"Id\", inplace=True, axis=1)\n",
    "test_kaggle.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26677f2f6d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"linear\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"linear\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(410)(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "# autoencoder.fit(test_kaggle, test_kaggle, epochs=100, batch_size=64, validation_split=0.25)\n",
    "autoencoder.fit(test_kaggle[:10], test_kaggle[:10], epochs=1000, batch_size=64, validation_split=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c774fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como:\n",
    "\n",
    "* MSE = $\\frac{1}{n} \\sum_{i=1}^{n} (Y_{true}^{i} - Y_{pred}^{i})^{2}$\n",
    "* MAPE = $\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_{true}^{i} - Y_{pred}^{i}}{Y_{true}^{i}} \\right|$\n",
    "\n",
    "donde $Y_{true}$ es el valor real, $Y_{pred}$ el valor de la predicción y $n$ el número de predicciones.\n",
    "\n",
    "La anterior fórmula realiza el cálculo para dos \"listas de valores\", por ejemplo, podemos calcular el error MAPE por muestra o por feature. Para el valor del error de la predicción total, se debe promediar el error obtenido para todas las filas/columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a20e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(346.17618, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train (etiquetados)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "print(\"MAPE:\", mape(X_train, X_train_pred))\n",
    "\n",
    "\n",
    "# print(\"Mean squared error in X_train data prediction:\", mse(X_train, X_train_pred).numpy())\n",
    "# sum(abs((test.to_numpy()[:, 0] - test_pred[:, 0]) / test.to_numpy()[:, 0])) * (100 / len(test_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b675b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.1136737291194"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = X_train_pred.shape\n",
    "\n",
    "mape_list = np.empty((410))\n",
    "\n",
    "# El bucle calcula el MAPE por cada columna\n",
    "for i in range(dims[1]):\n",
    "    mape_list[i] = sum(abs((X_train.iloc[:, i] - X_train_pred[:, i]) / X_train.iloc[:, i])) / dims[0]\n",
    "    \n",
    "# 1/N ~ 1/(longitud de cada columna) = 1/(nº de filas)\n",
    "# mape_list.mean() * (100 / dims[0])\n",
    "mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e45bf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 410)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt_tmp =  X_train\n",
    "# Xt_tmp =  test_kaggle[:10]\n",
    "\n",
    "Xp_tmp =  autoencoder.predict(Xt_tmp)\n",
    "\n",
    "mape_tmp = np.abs((Xt_tmp - Xp_tmp) / Xt_tmp)\n",
    "    \n",
    "# 1/N ~ 1/(longitud de cada columna) = 1/(nº de filas)\n",
    "# mape_list.mean() * (100 / dims[0])\n",
    "mape_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "532afe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mape_tmp.mean(axis=0).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d4a96a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041463414634146344"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mape_tmp.mean(axis=0) < 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ea7fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419.3223672248703"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp.to_numpy().sum() / 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "569e4d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346.17618712801715"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MAPE_(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "MAPE_(Xt_tmp.to_numpy(), Xp_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d1acce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(346.17618, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE:\", mape(\n",
    "    Xt_tmp.to_numpy(),\n",
    "    autoencoder.predict(Xt_tmp)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d2852c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105652</td>\n",
       "      <td>0.713284</td>\n",
       "      <td>0.117756</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.261004</td>\n",
       "      <td>0.025560</td>\n",
       "      <td>0.046394</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>5.384216</td>\n",
       "      <td>0.443847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041728</td>\n",
       "      <td>0.043366</td>\n",
       "      <td>0.027362</td>\n",
       "      <td>0.016941</td>\n",
       "      <td>0.946280</td>\n",
       "      <td>0.037685</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>0.106136</td>\n",
       "      <td>0.458846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.905592</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.149755</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.177056</td>\n",
       "      <td>0.089562</td>\n",
       "      <td>0.812193</td>\n",
       "      <td>0.266793</td>\n",
       "      <td>1.234998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174119</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.193251</td>\n",
       "      <td>1.462902</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.105090</td>\n",
       "      <td>0.111466</td>\n",
       "      <td>0.854434</td>\n",
       "      <td>0.039484</td>\n",
       "      <td>0.716544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.563342</td>\n",
       "      <td>0.082334</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>0.079150</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.058281</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.054094</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319610</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.136801</td>\n",
       "      <td>0.079314</td>\n",
       "      <td>0.072752</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.052331</td>\n",
       "      <td>0.199987</td>\n",
       "      <td>0.020903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072648</td>\n",
       "      <td>1.741236</td>\n",
       "      <td>0.084134</td>\n",
       "      <td>0.098497</td>\n",
       "      <td>1.268262</td>\n",
       "      <td>0.262483</td>\n",
       "      <td>0.040180</td>\n",
       "      <td>0.555057</td>\n",
       "      <td>0.414130</td>\n",
       "      <td>0.105717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102842</td>\n",
       "      <td>0.078708</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.030085</td>\n",
       "      <td>0.055274</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.236990</td>\n",
       "      <td>0.044356</td>\n",
       "      <td>0.200856</td>\n",
       "      <td>0.032583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041855</td>\n",
       "      <td>0.065261</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>1.936107</td>\n",
       "      <td>0.079343</td>\n",
       "      <td>2.744549</td>\n",
       "      <td>0.070099</td>\n",
       "      <td>0.070784</td>\n",
       "      <td>0.289318</td>\n",
       "      <td>0.099786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.187252</td>\n",
       "      <td>0.305902</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>0.224215</td>\n",
       "      <td>0.031928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.111856</td>\n",
       "      <td>0.070482</td>\n",
       "      <td>0.259590</td>\n",
       "      <td>0.316117</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>1.198931</td>\n",
       "      <td>0.145355</td>\n",
       "      <td>0.048370</td>\n",
       "      <td>0.129245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073222</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.595943</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>0.244114</td>\n",
       "      <td>0.112386</td>\n",
       "      <td>0.319973</td>\n",
       "      <td>0.137131</td>\n",
       "      <td>0.184293</td>\n",
       "      <td>0.106029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.161998</td>\n",
       "      <td>0.197088</td>\n",
       "      <td>0.054220</td>\n",
       "      <td>0.047310</td>\n",
       "      <td>0.248198</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.350624</td>\n",
       "      <td>31.450700</td>\n",
       "      <td>0.037707</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051351</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>6.402787</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>0.084176</td>\n",
       "      <td>3.459184</td>\n",
       "      <td>0.106564</td>\n",
       "      <td>0.036843</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.067987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.086829</td>\n",
       "      <td>0.135140</td>\n",
       "      <td>0.177873</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.094558</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.085125</td>\n",
       "      <td>0.648491</td>\n",
       "      <td>0.059527</td>\n",
       "      <td>0.184419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218452</td>\n",
       "      <td>0.023091</td>\n",
       "      <td>1.515034</td>\n",
       "      <td>0.007845</td>\n",
       "      <td>0.033975</td>\n",
       "      <td>0.055049</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.052644</td>\n",
       "      <td>0.045325</td>\n",
       "      <td>0.029986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.486476</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.104811</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>0.065570</td>\n",
       "      <td>0.050730</td>\n",
       "      <td>0.074255</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.160290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335864</td>\n",
       "      <td>0.331166</td>\n",
       "      <td>0.055139</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.909687</td>\n",
       "      <td>0.193612</td>\n",
       "      <td>0.049962</td>\n",
       "      <td>0.026795</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>0.030228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.099597</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.345776</td>\n",
       "      <td>0.112386</td>\n",
       "      <td>0.153996</td>\n",
       "      <td>0.126455</td>\n",
       "      <td>0.055246</td>\n",
       "      <td>0.114718</td>\n",
       "      <td>0.283661</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.021377</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.031167</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>0.084642</td>\n",
       "      <td>0.047585</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.371574</td>\n",
       "      <td>0.033509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.105652  0.713284  0.117756  0.007909  0.261004  0.025560  0.046394   \n",
       "1  0.905592  0.011334  0.008692  0.149755  0.015489  0.177056  0.089562   \n",
       "2  0.003439  0.563342  0.082334  0.085693  0.079150  0.011671  0.058281   \n",
       "3  0.072648  1.741236  0.084134  0.098497  1.268262  0.262483  0.040180   \n",
       "4  0.041855  0.065261  0.011034  1.936107  0.079343  2.744549  0.070099   \n",
       "5  0.111856  0.070482  0.259590  0.316117  0.170210  0.002749  1.198931   \n",
       "6  0.161998  0.197088  0.054220  0.047310  0.248198  0.020759  0.350624   \n",
       "7  0.086829  0.135140  0.177873  0.042072  0.094558  0.073267  0.085125   \n",
       "8  7.486476  0.040394  0.104811  0.058178  0.065570  0.050730  0.074255   \n",
       "9  0.099597  0.420259  0.345776  0.112386  0.153996  0.126455  0.055246   \n",
       "\n",
       "        FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0   0.077519  5.384216  0.443847  ...   0.041728   0.043366   0.027362   \n",
       "1   0.812193  0.266793  1.234998  ...   0.174119   0.012137   0.193251   \n",
       "2   0.005805  0.054094  0.027805  ...   0.319610   0.005038   0.014532   \n",
       "3   0.555057  0.414130  0.105717  ...   0.102842   0.078708   0.008026   \n",
       "4   0.070784  0.289318  0.099786  ...   0.009040   0.032716   0.043766   \n",
       "5   0.145355  0.048370  0.129245  ...   0.073222   0.045192   0.595943   \n",
       "6  31.450700  0.037707  0.064935  ...   0.051351   0.021254   6.402787   \n",
       "7   0.648491  0.059527  0.184419  ...   0.218452   0.023091   1.515034   \n",
       "8   0.082521  0.010319  0.160290  ...   0.335864   0.331166   0.055139   \n",
       "9   0.114718  0.283661  0.025211  ...   0.002366   0.021377   0.013038   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   0.016941   0.946280   0.037685   0.034822   0.007316   0.106136   0.458846  \n",
       "1   1.462902   0.020454   0.105090   0.111466   0.854434   0.039484   0.716544  \n",
       "2   0.136801   0.079314   0.072752   0.003710   0.052331   0.199987   0.020903  \n",
       "3   0.030085   0.055274   0.056710   0.236990   0.044356   0.200856   0.032583  \n",
       "4   0.187252   0.305902   0.026350   0.005066   0.234740   0.224215   0.031928  \n",
       "5   0.024438   0.244114   0.112386   0.319973   0.137131   0.184293   0.106029  \n",
       "6   0.054551   0.084176   3.459184   0.106564   0.036843   0.219600   0.067987  \n",
       "7   0.007845   0.033975   0.055049   0.002271   0.052644   0.045325   0.029986  \n",
       "8   0.011705   0.909687   0.193612   0.049962   0.026795   0.023666   0.030228  \n",
       "9   0.031167   0.035927   0.084642   0.047585   0.047887   0.371574   0.033509  \n",
       "\n",
       "[10 rows x 410 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28b820c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xt_tmp.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "348d0ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.1136737291194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobación: el valor debe ser el mismo por filas o por columnas (por samples o por variables)\n",
    "dims = X_train_pred.shape\n",
    "\n",
    "mape_list_row = np.empty((68))\n",
    "\n",
    "# El bucle calcula el MAPE por cada fila\n",
    "for i in range(dims[0]):\n",
    "    mape_list_row[i] = sum(abs((X_train.iloc[i, :] - X_train_pred[i, :]) / X_train.iloc[i, :]))\n",
    "    \n",
    "# 1/N ~ 1/(longitud de cada fila) = 1/(nº de columnas)\n",
    "mape_list_row.mean() * (100 / dims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b52107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(597.33813, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "\n",
    "test_kaggle_pred = autoencoder.predict(test_kaggle)\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7b1308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(65.644714, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "\n",
    "test_kaggle_pred = autoencoder.predict(test_kaggle[:10])\n",
    "print(\"MAPE:\", mape(test_kaggle[:10], test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9829f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(242.9066, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados) REDUCIDO A LAS 68 PRIMERAS SAMPLES\n",
    "\n",
    "test_kaggle_reduc = test_kaggle.iloc[500:568, :]\n",
    "\n",
    "y_pred_kaggle_reduc = autoencoder.predict(test_kaggle_reduc)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle_reduc, y_pred_kaggle_reduc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93517edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 406.34344482421875 +- 356.6924133300781\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados) REDUCIDO A LAS 68 PRIMERAS SAMPLES\n",
    "\n",
    "samples_num = 100\n",
    "mape_samples = []\n",
    "\n",
    "for _ in range(samples_num):\n",
    "    \n",
    "    args = np.random.choice(a=np.arange(0, test_kaggle.shape[0]),\n",
    "                            size=68,\n",
    "                           replace=False,)\n",
    "\n",
    "    test_kaggle_reduc = test_kaggle.iloc[args, :]\n",
    "\n",
    "    y_pred_kaggle_reduc = autoencoder.predict(test_kaggle_reduc)\n",
    "    tmp_mape = mape(test_kaggle_reduc, y_pred_kaggle_reduc)\n",
    "    mape_samples.append(tmp_mape)\n",
    "    \n",
    "#     print(\"MAPE:\", tmp_mape)\n",
    "\n",
    "print(f'MAPE: {np.mean(mape_samples)} +- {np.std(mape_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0d348",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{EL ERROR ES MAYOR EN EL CJTO. DE KAGGLE, INCLUSO EL REDUCIDO}}$\n",
    "\n",
    "Al sumar menos registros (68 primeros de ``test_kaggle``), se suman cantidades errores comparables, aún así es mayor el error en los datos con los que se ha entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c38d1",
   "metadata": {},
   "source": [
    "Un error de MAPE superior al 100% indica un error muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4813a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_encoder = encoder # loading the previously saved model.\n",
    "\n",
    "new_encoder = models.Sequential()\n",
    "new_encoder.add(prev_encoder)\n",
    "new_encoder.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 18s 367ms/step - loss: 0.6492 - acc: 0.6667 - val_loss: 0.8262 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6411 - acc: 0.6667 - val_loss: 0.8238 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6352 - acc: 0.6667 - val_loss: 0.8217 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6305 - acc: 0.7059 - val_loss: 0.8198 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6268 - acc: 0.7059 - val_loss: 0.8183 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6231 - acc: 0.7059 - val_loss: 0.8168 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6198 - acc: 0.7059 - val_loss: 0.8151 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6167 - acc: 0.7059 - val_loss: 0.8133 - val_acc: 0.5882\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6133 - acc: 0.7059 - val_loss: 0.8115 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6106 - acc: 0.7255 - val_loss: 0.8102 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6077 - acc: 0.7255 - val_loss: 0.8091 - val_acc: 0.5882\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6048 - acc: 0.7255 - val_loss: 0.8080 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6024 - acc: 0.7255 - val_loss: 0.8071 - val_acc: 0.5882\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5995 - acc: 0.7255 - val_loss: 0.8055 - val_acc: 0.5882\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5970 - acc: 0.7255 - val_loss: 0.8038 - val_acc: 0.5882\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5943 - acc: 0.7255 - val_loss: 0.8026 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5921 - acc: 0.7255 - val_loss: 0.8016 - val_acc: 0.5882\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5894 - acc: 0.7255 - val_loss: 0.8001 - val_acc: 0.5882\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5870 - acc: 0.7255 - val_loss: 0.7990 - val_acc: 0.5882\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5848 - acc: 0.7255 - val_loss: 0.7974 - val_acc: 0.5882\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5822 - acc: 0.7255 - val_loss: 0.7959 - val_acc: 0.5882\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5798 - acc: 0.7451 - val_loss: 0.7951 - val_acc: 0.5882\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5777 - acc: 0.7451 - val_loss: 0.7937 - val_acc: 0.5882\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5750 - acc: 0.7451 - val_loss: 0.7925 - val_acc: 0.5882\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5729 - acc: 0.7451 - val_loss: 0.7916 - val_acc: 0.5882\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5704 - acc: 0.7451 - val_loss: 0.7906 - val_acc: 0.5882\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5679 - acc: 0.7451 - val_loss: 0.7894 - val_acc: 0.5882\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5652 - acc: 0.7451 - val_loss: 0.7883 - val_acc: 0.5882\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5627 - acc: 0.7451 - val_loss: 0.7870 - val_acc: 0.5882\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5604 - acc: 0.7451 - val_loss: 0.7862 - val_acc: 0.5882\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5583 - acc: 0.7451 - val_loss: 0.7850 - val_acc: 0.5882\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5560 - acc: 0.7451 - val_loss: 0.7842 - val_acc: 0.5882\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 255ms/step - loss: 0.5535 - acc: 0.7451 - val_loss: 0.7833 - val_acc: 0.5294\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5515 - acc: 0.7451 - val_loss: 0.7822 - val_acc: 0.5294\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5493 - acc: 0.7255 - val_loss: 0.7812 - val_acc: 0.5294\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5472 - acc: 0.7451 - val_loss: 0.7803 - val_acc: 0.5294\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5450 - acc: 0.7451 - val_loss: 0.7791 - val_acc: 0.5294\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5430 - acc: 0.7451 - val_loss: 0.7783 - val_acc: 0.5294\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5407 - acc: 0.7255 - val_loss: 0.7774 - val_acc: 0.4706\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5383 - acc: 0.7255 - val_loss: 0.7765 - val_acc: 0.4706\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5363 - acc: 0.7255 - val_loss: 0.7758 - val_acc: 0.4706\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5343 - acc: 0.7255 - val_loss: 0.7749 - val_acc: 0.4706\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5323 - acc: 0.7255 - val_loss: 0.7739 - val_acc: 0.4706\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5303 - acc: 0.7451 - val_loss: 0.7737 - val_acc: 0.4706\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5284 - acc: 0.7451 - val_loss: 0.7730 - val_acc: 0.4706\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5261 - acc: 0.7451 - val_loss: 0.7719 - val_acc: 0.4706\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5241 - acc: 0.7647 - val_loss: 0.7708 - val_acc: 0.4706\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5219 - acc: 0.7647 - val_loss: 0.7697 - val_acc: 0.4706\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5200 - acc: 0.7647 - val_loss: 0.7683 - val_acc: 0.4706\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5182 - acc: 0.7647 - val_loss: 0.7675 - val_acc: 0.4706\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5163 - acc: 0.7647 - val_loss: 0.7660 - val_acc: 0.4706\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5144 - acc: 0.7647 - val_loss: 0.7654 - val_acc: 0.4706\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5124 - acc: 0.7647 - val_loss: 0.7649 - val_acc: 0.4706\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5105 - acc: 0.7647 - val_loss: 0.7637 - val_acc: 0.4706\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5089 - acc: 0.7647 - val_loss: 0.7635 - val_acc: 0.4706\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5068 - acc: 0.7647 - val_loss: 0.7622 - val_acc: 0.4706\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5049 - acc: 0.7647 - val_loss: 0.7613 - val_acc: 0.4706\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5030 - acc: 0.7647 - val_loss: 0.7604 - val_acc: 0.4706\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5011 - acc: 0.7647 - val_loss: 0.7591 - val_acc: 0.4706\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4996 - acc: 0.7647 - val_loss: 0.7580 - val_acc: 0.4706\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4975 - acc: 0.8039 - val_loss: 0.7573 - val_acc: 0.4118\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4956 - acc: 0.8039 - val_loss: 0.7564 - val_acc: 0.4118\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4940 - acc: 0.8039 - val_loss: 0.7556 - val_acc: 0.4118\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4921 - acc: 0.8039 - val_loss: 0.7548 - val_acc: 0.4118\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4905 - acc: 0.8235 - val_loss: 0.7544 - val_acc: 0.4118\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4889 - acc: 0.8235 - val_loss: 0.7541 - val_acc: 0.4118\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4873 - acc: 0.8235 - val_loss: 0.7529 - val_acc: 0.4118\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4854 - acc: 0.8235 - val_loss: 0.7519 - val_acc: 0.4118\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4837 - acc: 0.8235 - val_loss: 0.7513 - val_acc: 0.4118\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4822 - acc: 0.8235 - val_loss: 0.7497 - val_acc: 0.4118\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4807 - acc: 0.8235 - val_loss: 0.7493 - val_acc: 0.4118\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4790 - acc: 0.8235 - val_loss: 0.7484 - val_acc: 0.4118\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4775 - acc: 0.8235 - val_loss: 0.7473 - val_acc: 0.4118\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4757 - acc: 0.8235 - val_loss: 0.7471 - val_acc: 0.4118\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4742 - acc: 0.8235 - val_loss: 0.7455 - val_acc: 0.4118\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4726 - acc: 0.8235 - val_loss: 0.7446 - val_acc: 0.4118\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4709 - acc: 0.8235 - val_loss: 0.7443 - val_acc: 0.4118\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4693 - acc: 0.8235 - val_loss: 0.7436 - val_acc: 0.4118\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4680 - acc: 0.8235 - val_loss: 0.7434 - val_acc: 0.4118\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4667 - acc: 0.8235 - val_loss: 0.7430 - val_acc: 0.4118\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4648 - acc: 0.8235 - val_loss: 0.7428 - val_acc: 0.4118\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4634 - acc: 0.8235 - val_loss: 0.7420 - val_acc: 0.4118\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4621 - acc: 0.8235 - val_loss: 0.7415 - val_acc: 0.4118\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4608 - acc: 0.8235 - val_loss: 0.7415 - val_acc: 0.4118\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4589 - acc: 0.8235 - val_loss: 0.7413 - val_acc: 0.4118\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4577 - acc: 0.8235 - val_loss: 0.7410 - val_acc: 0.4118\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4564 - acc: 0.8235 - val_loss: 0.7402 - val_acc: 0.4118\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4549 - acc: 0.8235 - val_loss: 0.7399 - val_acc: 0.4118\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4536 - acc: 0.8235 - val_loss: 0.7391 - val_acc: 0.4118\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4522 - acc: 0.8235 - val_loss: 0.7387 - val_acc: 0.4118\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4507 - acc: 0.8235 - val_loss: 0.7384 - val_acc: 0.4118\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4493 - acc: 0.8235 - val_loss: 0.7378 - val_acc: 0.4118\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4484 - acc: 0.8235 - val_loss: 0.7374 - val_acc: 0.4118\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4465 - acc: 0.8235 - val_loss: 0.7371 - val_acc: 0.4118\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4452 - acc: 0.8235 - val_loss: 0.7365 - val_acc: 0.4118\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4439 - acc: 0.8235 - val_loss: 0.7353 - val_acc: 0.4118\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4426 - acc: 0.8235 - val_loss: 0.7345 - val_acc: 0.4118\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4414 - acc: 0.8235 - val_loss: 0.7340 - val_acc: 0.4118\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4400 - acc: 0.8235 - val_loss: 0.7331 - val_acc: 0.4118\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4386 - acc: 0.8235 - val_loss: 0.7323 - val_acc: 0.4118\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5242 - acc: 0.7222\n",
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1913fc",
   "metadata": {},
   "source": [
    "Tras unas pocas iteraciones, descongelamos todas las capas y hacemos unas pocas épocas más entrenando y actualizando los pesos para el modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f79e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.4514 - acc: 0.8235 - val_loss: 0.7365 - val_acc: 0.5294\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3222 - acc: 0.9020 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1790 - acc: 0.9608 - val_loss: 0.7514 - val_acc: 0.6471\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0955 - acc: 0.9804 - val_loss: 0.8395 - val_acc: 0.6471\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0626 - acc: 0.9804 - val_loss: 1.0120 - val_acc: 0.8235\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0582 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.6471\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.7721 - val_acc: 0.6471\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.9838 - val_acc: 0.7059\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.9407 - val_acc: 0.6471\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9743 - val_acc: 0.6471\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.0043 - val_acc: 0.6471\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.6471\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.0612 - val_acc: 0.6471\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.6471\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0967 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7808 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c906d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 421ms/step - loss: 0.9253 - acc: 0.5098 - val_loss: 0.8377 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5403 - acc: 0.7647 - val_loss: 0.7234 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2367 - acc: 0.9608 - val_loss: 0.7454 - val_acc: 0.7647\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1574 - acc: 0.9804 - val_loss: 0.7459 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1253 - acc: 0.9608 - val_loss: 0.8012 - val_acc: 0.8235\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1049 - acc: 0.9804 - val_loss: 0.7114 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0666 - acc: 0.9804 - val_loss: 0.7522 - val_acc: 0.5294\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0584 - acc: 1.0000 - val_loss: 0.8406 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0270 - acc: 1.0000 - val_loss: 0.7222 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.7665 - val_acc: 0.7059\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.7859 - val_acc: 0.7059\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.8420 - val_acc: 0.7059\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.8112 - val_acc: 0.7059\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.7059\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.8409 - val_acc: 0.7059\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.8710 - val_acc: 0.7059\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.8838 - val_acc: 0.7059\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.8801 - val_acc: 0.7059\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.8880 - val_acc: 0.7059\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.8940 - val_acc: 0.7059\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9305 - val_acc: 0.7059\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9308 - val_acc: 0.7059\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9220 - val_acc: 0.7059\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.9520 - val_acc: 0.7059\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.9628 - val_acc: 0.7059\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.9004e-04 - acc: 1.0000 - val_loss: 0.9631 - val_acc: 0.7059\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.6636e-04 - acc: 1.0000 - val_loss: 0.9983 - val_acc: 0.7059\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.6340e-04 - acc: 1.0000 - val_loss: 1.0185 - val_acc: 0.7059\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.8053e-04 - acc: 1.0000 - val_loss: 1.0406 - val_acc: 0.7059\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.9853e-04 - acc: 1.0000 - val_loss: 1.0389 - val_acc: 0.7059\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.3641e-04 - acc: 1.0000 - val_loss: 1.0269 - val_acc: 0.7059\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 3.7636e-04 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.7059\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2180e-04 - acc: 1.0000 - val_loss: 1.0749 - val_acc: 0.7059\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7592e-04 - acc: 1.0000 - val_loss: 1.0844 - val_acc: 0.7059\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4860e-04 - acc: 1.0000 - val_loss: 1.1044 - val_acc: 0.7059\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.1217e-04 - acc: 1.0000 - val_loss: 1.1038 - val_acc: 0.7059\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8341e-04 - acc: 1.0000 - val_loss: 1.1078 - val_acc: 0.7059\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.5996e-04 - acc: 1.0000 - val_loss: 1.1237 - val_acc: 0.7059\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.4100e-04 - acc: 1.0000 - val_loss: 1.1393 - val_acc: 0.7059\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.2332e-04 - acc: 1.0000 - val_loss: 1.1477 - val_acc: 0.7059\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.1002e-04 - acc: 1.0000 - val_loss: 1.1634 - val_acc: 0.7059\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.6241e-05 - acc: 1.0000 - val_loss: 1.1663 - val_acc: 0.7059\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.4933e-05 - acc: 1.0000 - val_loss: 1.1611 - val_acc: 0.7059\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.3320e-05 - acc: 1.0000 - val_loss: 1.1791 - val_acc: 0.7059\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 6.4648e-05 - acc: 1.0000 - val_loss: 1.1969 - val_acc: 0.7059\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.7221e-05 - acc: 1.0000 - val_loss: 1.2058 - val_acc: 0.7059\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.0442e-05 - acc: 1.0000 - val_loss: 1.2246 - val_acc: 0.7059\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.3616e-05 - acc: 1.0000 - val_loss: 1.2298 - val_acc: 0.7059\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 3.9317e-05 - acc: 1.0000 - val_loss: 1.2567 - val_acc: 0.7059\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4057e-05 - acc: 1.0000 - val_loss: 1.2540 - val_acc: 0.7059\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.9733e-05 - acc: 1.0000 - val_loss: 1.2541 - val_acc: 0.7059\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.6064e-05 - acc: 1.0000 - val_loss: 1.2711 - val_acc: 0.7059\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.2891e-05 - acc: 1.0000 - val_loss: 1.2864 - val_acc: 0.7059\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.0595e-05 - acc: 1.0000 - val_loss: 1.2748 - val_acc: 0.7059\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.7891e-05 - acc: 1.0000 - val_loss: 1.2937 - val_acc: 0.7059\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.5618e-05 - acc: 1.0000 - val_loss: 1.3039 - val_acc: 0.7059\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.4100e-05 - acc: 1.0000 - val_loss: 1.3065 - val_acc: 0.7059\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2333e-05 - acc: 1.0000 - val_loss: 1.3466 - val_acc: 0.7059\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0616e-05 - acc: 1.0000 - val_loss: 1.3405 - val_acc: 0.7059\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.4768e-06 - acc: 1.0000 - val_loss: 1.3589 - val_acc: 0.7059\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.3408e-06 - acc: 1.0000 - val_loss: 1.3717 - val_acc: 0.7059\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 7.4195e-06 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 0.7059\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.6597e-06 - acc: 1.0000 - val_loss: 1.4061 - val_acc: 0.7059\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.8877e-06 - acc: 1.0000 - val_loss: 1.3961 - val_acc: 0.7059\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.1211e-06 - acc: 1.0000 - val_loss: 1.4195 - val_acc: 0.7059\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.5277e-06 - acc: 1.0000 - val_loss: 1.4312 - val_acc: 0.7059\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.0455e-06 - acc: 1.0000 - val_loss: 1.4429 - val_acc: 0.7059\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.5952e-06 - acc: 1.0000 - val_loss: 1.4412 - val_acc: 0.7059\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1821e-06 - acc: 1.0000 - val_loss: 1.4658 - val_acc: 0.7059\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8013e-06 - acc: 1.0000 - val_loss: 1.4794 - val_acc: 0.7059\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5098e-06 - acc: 1.0000 - val_loss: 1.4894 - val_acc: 0.7059\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.2005e-06 - acc: 1.0000 - val_loss: 1.4911 - val_acc: 0.7059\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.9492e-06 - acc: 1.0000 - val_loss: 1.4928 - val_acc: 0.7059\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.7297e-06 - acc: 1.0000 - val_loss: 1.5091 - val_acc: 0.7059\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.5434e-06 - acc: 1.0000 - val_loss: 1.5199 - val_acc: 0.7059\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.3915e-06 - acc: 1.0000 - val_loss: 1.5435 - val_acc: 0.7059\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.2301e-06 - acc: 1.0000 - val_loss: 1.5527 - val_acc: 0.7059\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0952e-06 - acc: 1.0000 - val_loss: 1.5497 - val_acc: 0.7059\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.8013e-07 - acc: 1.0000 - val_loss: 1.5576 - val_acc: 0.7059\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.9515e-07 - acc: 1.0000 - val_loss: 1.5575 - val_acc: 0.7059\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.7784e-07 - acc: 1.0000 - val_loss: 1.5782 - val_acc: 0.7059\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.9228e-07 - acc: 1.0000 - val_loss: 1.5990 - val_acc: 0.7059\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.1483e-07 - acc: 1.0000 - val_loss: 1.6042 - val_acc: 0.7059\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.5079e-07 - acc: 1.0000 - val_loss: 1.6158 - val_acc: 0.7059\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.0683e-07 - acc: 1.0000 - val_loss: 1.6415 - val_acc: 0.7059\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.4517e-07 - acc: 1.0000 - val_loss: 1.6442 - val_acc: 0.7059\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.0028e-07 - acc: 1.0000 - val_loss: 1.6601 - val_acc: 0.7059\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.5649e-07 - acc: 1.0000 - val_loss: 1.6618 - val_acc: 0.7059\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1781e-07 - acc: 1.0000 - val_loss: 1.6794 - val_acc: 0.7059\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8540e-07 - acc: 1.0000 - val_loss: 1.6917 - val_acc: 0.7059\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5565e-07 - acc: 1.0000 - val_loss: 1.6983 - val_acc: 0.7059\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.3288e-07 - acc: 1.0000 - val_loss: 1.7153 - val_acc: 0.7059\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.1116e-07 - acc: 1.0000 - val_loss: 1.7233 - val_acc: 0.7059\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.9030e-07 - acc: 1.0000 - val_loss: 1.7158 - val_acc: 0.7059\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.6958e-07 - acc: 1.0000 - val_loss: 1.7191 - val_acc: 0.7059\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.5099e-07 - acc: 1.0000 - val_loss: 1.7305 - val_acc: 0.7059\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.3555e-07 - acc: 1.0000 - val_loss: 1.7435 - val_acc: 0.7059\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2413e-07 - acc: 1.0000 - val_loss: 1.7631 - val_acc: 0.7059\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1110e-07 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 0.7059\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0272e-07 - acc: 1.0000 - val_loss: 1.7651 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.9267 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer2 = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded2 = layers.Dense(200, activation=\"linear\")(input_layer2)\n",
    "encoded2 = layers.Dense(100, activation=\"linear\")(encoded2)\n",
    "encoded2 = layers.Dense(50, activation=\"linear\")(encoded2)\n",
    "encoded2 = layers.Dense(1, activation=\"sigmoid\")(encoded2)\n",
    "\n",
    "encoder2 = models.Model(input_layer2, encoded2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder2.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1258",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, test_id=testFNC[\"Id\"]):\n",
    "    '''\n",
    "    Función para generar un csv con las predicciones de un modelo para participar en la competición de Kaggle\n",
    "    '''\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape) # Comprobación del tamaño, debe ser: (119748, 2)\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    parent_path = current_path.parent\n",
    "    submissionDF.to_csv(f\"{parent_path}\\submissions\\MLSP_submission_AE_{current_time}.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
