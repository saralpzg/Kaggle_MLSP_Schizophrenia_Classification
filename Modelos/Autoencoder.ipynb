{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb1ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1cda16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tot = pd.concat((X_train, X_test), axis=0)\n",
    "X_train_tot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1404/1404 [==============================] - 7s 4ms/step - loss: 0.1411 - val_loss: 0.1272\n",
      "Epoch 2/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.1208 - val_loss: 0.1164\n",
      "Epoch 3/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1154 - val_loss: 0.1174\n",
      "Epoch 4/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1136 - val_loss: 0.1120\n",
      "Epoch 5/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.1108 - val_loss: 0.1107\n",
      "Epoch 6/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 7/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1074 - val_loss: 0.1093\n",
      "Epoch 8/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1063 - val_loss: 0.1052\n",
      "Epoch 9/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1047 - val_loss: 0.1057\n",
      "Epoch 10/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1043 - val_loss: 0.1069\n",
      "Epoch 11/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1040 - val_loss: 0.1049\n",
      "Epoch 12/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1037 - val_loss: 0.1033\n",
      "Epoch 13/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1027 - val_loss: 0.1026\n",
      "Epoch 14/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1021 - val_loss: 0.1029\n",
      "Epoch 15/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.1018 - val_loss: 0.1017\n",
      "Epoch 16/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1015 - val_loss: 0.1018\n",
      "Epoch 17/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1013 - val_loss: 0.1014\n",
      "Epoch 18/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1011 - val_loss: 0.1005\n",
      "Epoch 19/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1009 - val_loss: 0.1004\n",
      "Epoch 20/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1007 - val_loss: 0.1009\n",
      "Epoch 21/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1006 - val_loss: 0.1002\n",
      "Epoch 22/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1004 - val_loss: 0.1027\n",
      "Epoch 23/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1003 - val_loss: 0.1010\n",
      "Epoch 24/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1001 - val_loss: 0.1012\n",
      "Epoch 25/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 26/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0998 - val_loss: 0.1001\n",
      "Epoch 27/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0997 - val_loss: 0.1002\n",
      "Epoch 28/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0996 - val_loss: 0.0995\n",
      "Epoch 29/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0995 - val_loss: 0.0999\n",
      "Epoch 30/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0993 - val_loss: 0.1000\n",
      "Epoch 31/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0988 - val_loss: 0.0983\n",
      "Epoch 32/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0979 - val_loss: 0.0973\n",
      "Epoch 33/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0977 - val_loss: 0.0991\n",
      "Epoch 34/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0975 - val_loss: 0.0969\n",
      "Epoch 35/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0974 - val_loss: 0.0975\n",
      "Epoch 36/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0972 - val_loss: 0.0991\n",
      "Epoch 37/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0971 - val_loss: 0.0979\n",
      "Epoch 38/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0970 - val_loss: 0.0972\n",
      "Epoch 39/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 40/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0968 - val_loss: 0.0963\n",
      "Epoch 41/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0967 - val_loss: 0.0977\n",
      "Epoch 42/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0966 - val_loss: 0.0972\n",
      "Epoch 43/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 44/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0965 - val_loss: 0.0955\n",
      "Epoch 45/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 46/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0964 - val_loss: 0.0970\n",
      "Epoch 47/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 48/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0963 - val_loss: 0.0960\n",
      "Epoch 49/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0963 - val_loss: 0.0963\n",
      "Epoch 50/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0962 - val_loss: 0.0974\n",
      "Epoch 51/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0962 - val_loss: 0.0955\n",
      "Epoch 52/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0961 - val_loss: 0.0965\n",
      "Epoch 53/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0961 - val_loss: 0.0953\n",
      "Epoch 54/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0960 - val_loss: 0.0963\n",
      "Epoch 55/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0960 - val_loss: 0.0959\n",
      "Epoch 56/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0959 - val_loss: 0.0966\n",
      "Epoch 57/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0959 - val_loss: 0.0955\n",
      "Epoch 58/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0958 - val_loss: 0.0957\n",
      "Epoch 59/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0958 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0958 - val_loss: 0.0961\n",
      "Epoch 61/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0957 - val_loss: 0.0960\n",
      "Epoch 62/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0957 - val_loss: 0.0966\n",
      "Epoch 63/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0954 - val_loss: 0.0941\n",
      "Epoch 64/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0945 - val_loss: 0.0944\n",
      "Epoch 65/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0943 - val_loss: 0.0933\n",
      "Epoch 66/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0942 - val_loss: 0.0954\n",
      "Epoch 67/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0941 - val_loss: 0.0936\n",
      "Epoch 68/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0941 - val_loss: 0.0937\n",
      "Epoch 69/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0940 - val_loss: 0.0945\n",
      "Epoch 70/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0940 - val_loss: 0.0938\n",
      "Epoch 71/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0933\n",
      "Epoch 72/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0951\n",
      "Epoch 73/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0944\n",
      "Epoch 74/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0944\n",
      "Epoch 75/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0938\n",
      "Epoch 76/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0935\n",
      "Epoch 77/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0939\n",
      "Epoch 78/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0937 - val_loss: 0.0940\n",
      "Epoch 79/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0937 - val_loss: 0.0946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0937 - val_loss: 0.0939\n",
      "Epoch 81/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0937 - val_loss: 0.0938\n",
      "Epoch 82/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0937 - val_loss: 0.0942\n",
      "Epoch 83/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0936 - val_loss: 0.0938\n",
      "Epoch 84/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0936 - val_loss: 0.0944\n",
      "Epoch 85/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0936 - val_loss: 0.0940\n",
      "Epoch 86/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0936 - val_loss: 0.0941\n",
      "Epoch 87/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0935 - val_loss: 0.0933\n",
      "Epoch 88/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0935 - val_loss: 0.0938\n",
      "Epoch 89/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0935 - val_loss: 0.0934\n",
      "Epoch 90/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0935 - val_loss: 0.0936\n",
      "Epoch 91/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0935 - val_loss: 0.0936\n",
      "Epoch 92/100\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 0.0935 - val_loss: 0.0928\n",
      "Epoch 93/100\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 0.0934 - val_loss: 0.0948\n",
      "Epoch 94/100\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 0.0934 - val_loss: 0.0946\n",
      "Epoch 95/100\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 96/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 97/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0946\n",
      "Epoch 98/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 99/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0934 - val_loss: 0.0937\n",
      "Epoch 100/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0933 - val_loss: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e839759d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"relu\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"relu\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(410, activation=\"linear\")(decoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder.fit(test_kaggle, test_kaggle, epochs=100, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c774fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "747650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como:\n",
    "\n",
    "* MSE = $\\frac{1}{n} \\sum_{i=1}^{n} (Y_{true}^{i} - Y_{pred}^{i})^{2}$\n",
    "* MAPE = $\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_{true}^{i} - Y_{pred}^{i}}{Y_{true}^{i}} \\right|$\n",
    "\n",
    "donde $Y_{true}$ es el valor real, $Y_{pred}$ el valor de la predicción y $n$ el número de predicciones.\n",
    "\n",
    "La anterior fórmula realiza el cálculo para dos \"listas de valores\", por ejemplo, podemos calcular el error MAPE por muestra o por feature. Para el valor del error de la predicción total, se debe promediar el error obtenido para todas las filas/columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a20e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(315.31876, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.08369573, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train_tot (etiquetados)\n",
    "X_train_tot_pred = autoencoder.predict(X_train_tot)\n",
    "\n",
    "print(\"MAPE:\", mape(X_train_tot, X_train_tot_pred))\n",
    "print(\"MSE:\", mse(X_train_tot, X_train_tot_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1e672",
   "metadata": {},
   "source": [
    "El error es bastante elevado. Vamos a comprobar cómo se distribuyen los valores de error MAPE en cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e45bf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp = np.abs((X_train_tot - X_train_tot_pred) / X_train_tot) * 100\n",
    "mape_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532afe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mape_tmp.mean(axis=0))\n",
    "plt.xlabel(\"MAPE error\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe35255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mape_tmp.mean(axis=0), bins=1000)\n",
    "plt.xlim([0, 500])\n",
    "plt.xlabel(\"MAPE error\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a96a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08292682926829269"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mape_tmp.mean(axis=0) < 100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dace6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd17d6",
   "metadata": {},
   "source": [
    "La anterior expresión indica que únicamente en torno al 8% de las variables en X_train, tienen un error de MAPE que es inferior al 100%. \n",
    "\n",
    "Podíamos pensar que la primera barra en el anterior histograma esconde una gran concentración de valores con errores inferiores al 100% y que el valor final queda desviado por aquellos con valores muy superiores, sin embargo, esta última comprobación lo desmiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(426.45328, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.09347953, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "test_kaggle_pred = autoencoder.predict(test_kaggle)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))\n",
    "print(\"MSE:\", mse(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90c952",
   "metadata": {},
   "source": [
    "Sobre el conjunto de entrenamiento ``test_kaggle``, vamos a seleccionar 100 veces 86 muestras y a promediar los valores de desviación típica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9d3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 415.7068786621094 +- 354.8747253417969\n"
     ]
    }
   ],
   "source": [
    "samples_num = 100\n",
    "mape_samples = []\n",
    "\n",
    "for _ in range(samples_num):\n",
    "    \n",
    "    args = np.random.choice(a=np.arange(0, test_kaggle.shape[0]), size=86, replace=False)\n",
    "\n",
    "    test_kaggle_reduc = test_kaggle.iloc[args, :]\n",
    "\n",
    "    y_pred_kaggle_reduc = autoencoder.predict(test_kaggle_reduc)\n",
    "    tmp_mape = mape(test_kaggle_reduc, y_pred_kaggle_reduc)\n",
    "    mape_samples.append(tmp_mape)\n",
    "    \n",
    "print(f'MAPE: {np.mean(mape_samples)} +- {np.std(mape_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692b393",
   "metadata": {},
   "source": [
    "El valor del MAPE en ``test_kaggle`` es también muy elevado, con grandes desviaciones entre muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c74a80",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Pasamos al problema de clasificación.\n",
    "\n",
    "\n",
    "_NOTA: si no se usa el método ``clone_model``, se van a sobreescribir los pesos del autoencoder original._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4813a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "copy_autoencoder = models.clone_model(autoencoder)\n",
    "copy_autoencoder.build((None, 410))\n",
    "copy_autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "copy_autoencoder.set_weights(autoencoder.get_weights())\n",
    "\n",
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in copy_autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "encoder_classification = models.Sequential()\n",
    "encoder_classification.add(encoder)\n",
    "encoder_classification.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6905 - acc: 0.5490 - val_loss: 0.6894 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6890 - acc: 0.5490 - val_loss: 0.6891 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6874 - acc: 0.5490 - val_loss: 0.6886 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6864 - acc: 0.5490 - val_loss: 0.6883 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6860 - acc: 0.5490 - val_loss: 0.6881 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6851 - acc: 0.5490 - val_loss: 0.6879 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6875 - acc: 0.5490 - val_loss: 0.6878 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6840 - acc: 0.5490 - val_loss: 0.6877 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6840 - acc: 0.5490 - val_loss: 0.6875 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6832 - acc: 0.5490 - val_loss: 0.6874 - val_acc: 0.6471\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6829 - acc: 0.5490 - val_loss: 0.6873 - val_acc: 0.6471\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6830 - acc: 0.5294 - val_loss: 0.6872 - val_acc: 0.6471\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6830 - acc: 0.5490 - val_loss: 0.6871 - val_acc: 0.6471\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6818 - acc: 0.5294 - val_loss: 0.6870 - val_acc: 0.6471\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6816 - acc: 0.5294 - val_loss: 0.6869 - val_acc: 0.6471\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6811 - acc: 0.5294 - val_loss: 0.6869 - val_acc: 0.6471\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6821 - acc: 0.5294 - val_loss: 0.6868 - val_acc: 0.6471\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6812 - acc: 0.5294 - val_loss: 0.6868 - val_acc: 0.6471\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6805 - acc: 0.5294 - val_loss: 0.6867 - val_acc: 0.6471\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6806 - acc: 0.5294 - val_loss: 0.6867 - val_acc: 0.6471\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6804 - acc: 0.5294 - val_loss: 0.6866 - val_acc: 0.6471\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6797 - acc: 0.5490 - val_loss: 0.6865 - val_acc: 0.6471\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6793 - acc: 0.5490 - val_loss: 0.6864 - val_acc: 0.6471\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6795 - acc: 0.5294 - val_loss: 0.6863 - val_acc: 0.6471\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6787 - acc: 0.5490 - val_loss: 0.6863 - val_acc: 0.6471\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6783 - acc: 0.5490 - val_loss: 0.6861 - val_acc: 0.6471\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6785 - acc: 0.5294 - val_loss: 0.6861 - val_acc: 0.6471\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6779 - acc: 0.5294 - val_loss: 0.6860 - val_acc: 0.6471\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6773 - acc: 0.5490 - val_loss: 0.6858 - val_acc: 0.6471\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6772 - acc: 0.5294 - val_loss: 0.6858 - val_acc: 0.6471\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6772 - acc: 0.5490 - val_loss: 0.6856 - val_acc: 0.6471\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6763 - acc: 0.5294 - val_loss: 0.6855 - val_acc: 0.6471\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6766 - acc: 0.5294 - val_loss: 0.6854 - val_acc: 0.7059\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6756 - acc: 0.5490 - val_loss: 0.6853 - val_acc: 0.7059\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6753 - acc: 0.5490 - val_loss: 0.6852 - val_acc: 0.7059\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6749 - acc: 0.5490 - val_loss: 0.6851 - val_acc: 0.7059\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6745 - acc: 0.5490 - val_loss: 0.6850 - val_acc: 0.7059\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6742 - acc: 0.5490 - val_loss: 0.6849 - val_acc: 0.7059\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6738 - acc: 0.5490 - val_loss: 0.6848 - val_acc: 0.7059\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6738 - acc: 0.5490 - val_loss: 0.6847 - val_acc: 0.7059\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6730 - acc: 0.5490 - val_loss: 0.6847 - val_acc: 0.7059\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6726 - acc: 0.5490 - val_loss: 0.6846 - val_acc: 0.7059\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6725 - acc: 0.5490 - val_loss: 0.6845 - val_acc: 0.7059\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6722 - acc: 0.5490 - val_loss: 0.6845 - val_acc: 0.7059\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6728 - acc: 0.5490 - val_loss: 0.6844 - val_acc: 0.6471\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6716 - acc: 0.5490 - val_loss: 0.6843 - val_acc: 0.6471\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6711 - acc: 0.5490 - val_loss: 0.6842 - val_acc: 0.7059\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6712 - acc: 0.5490 - val_loss: 0.6842 - val_acc: 0.6471\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6704 - acc: 0.5490 - val_loss: 0.6841 - val_acc: 0.6471\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6701 - acc: 0.5490 - val_loss: 0.6840 - val_acc: 0.6471\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6704 - acc: 0.5490 - val_loss: 0.6839 - val_acc: 0.6471\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6697 - acc: 0.5686 - val_loss: 0.6838 - val_acc: 0.7059\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6695 - acc: 0.5490 - val_loss: 0.6838 - val_acc: 0.6471\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6690 - acc: 0.5490 - val_loss: 0.6836 - val_acc: 0.7059\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6690 - acc: 0.5490 - val_loss: 0.6836 - val_acc: 0.6471\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6683 - acc: 0.5490 - val_loss: 0.6835 - val_acc: 0.7059\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6681 - acc: 0.5490 - val_loss: 0.6833 - val_acc: 0.7059\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6680 - acc: 0.5490 - val_loss: 0.6833 - val_acc: 0.7059\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6673 - acc: 0.5490 - val_loss: 0.6832 - val_acc: 0.7059\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6680 - acc: 0.5490 - val_loss: 0.6831 - val_acc: 0.7059\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6668 - acc: 0.5490 - val_loss: 0.6831 - val_acc: 0.7059\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6667 - acc: 0.5490 - val_loss: 0.6830 - val_acc: 0.6471\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6668 - acc: 0.5490 - val_loss: 0.6830 - val_acc: 0.6471\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6663 - acc: 0.5882 - val_loss: 0.6829 - val_acc: 0.7059\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6658 - acc: 0.5490 - val_loss: 0.6828 - val_acc: 0.6471\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6654 - acc: 0.5490 - val_loss: 0.6828 - val_acc: 0.6471\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6651 - acc: 0.5490 - val_loss: 0.6827 - val_acc: 0.6471\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6647 - acc: 0.5490 - val_loss: 0.6826 - val_acc: 0.7059\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6643 - acc: 0.5490 - val_loss: 0.6825 - val_acc: 0.7059\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6640 - acc: 0.5490 - val_loss: 0.6824 - val_acc: 0.7059\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6638 - acc: 0.5490 - val_loss: 0.6823 - val_acc: 0.7059\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6632 - acc: 0.5490 - val_loss: 0.6822 - val_acc: 0.7059\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6629 - acc: 0.5490 - val_loss: 0.6821 - val_acc: 0.7059\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6625 - acc: 0.5490 - val_loss: 0.6820 - val_acc: 0.7059\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6621 - acc: 0.5490 - val_loss: 0.6819 - val_acc: 0.7059\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6617 - acc: 0.5490 - val_loss: 0.6818 - val_acc: 0.7059\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6613 - acc: 0.5490 - val_loss: 0.6817 - val_acc: 0.7059\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6607 - acc: 0.5686 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6604 - acc: 0.5686 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6609 - acc: 0.5882 - val_loss: 0.6815 - val_acc: 0.7059\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6595 - acc: 0.5686 - val_loss: 0.6815 - val_acc: 0.7059\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6592 - acc: 0.5686 - val_loss: 0.6813 - val_acc: 0.7059\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6596 - acc: 0.6078 - val_loss: 0.6813 - val_acc: 0.7059\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6586 - acc: 0.5686 - val_loss: 0.6812 - val_acc: 0.7059\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6586 - acc: 0.5686 - val_loss: 0.6811 - val_acc: 0.7059\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6578 - acc: 0.5686 - val_loss: 0.6811 - val_acc: 0.7059\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6587 - acc: 0.5686 - val_loss: 0.6810 - val_acc: 0.7059\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6572 - acc: 0.5686 - val_loss: 0.6809 - val_acc: 0.7059\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6569 - acc: 0.5882 - val_loss: 0.6808 - val_acc: 0.7059\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6566 - acc: 0.5686 - val_loss: 0.6808 - val_acc: 0.7059\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6563 - acc: 0.5686 - val_loss: 0.6807 - val_acc: 0.7059\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6561 - acc: 0.5686 - val_loss: 0.6806 - val_acc: 0.7059\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6566 - acc: 0.5882 - val_loss: 0.6805 - val_acc: 0.7059\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6558 - acc: 0.6078 - val_loss: 0.6804 - val_acc: 0.7059\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6559 - acc: 0.5882 - val_loss: 0.6804 - val_acc: 0.7059\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6548 - acc: 0.5686 - val_loss: 0.6803 - val_acc: 0.7059\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6545 - acc: 0.5882 - val_loss: 0.6802 - val_acc: 0.7059\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6555 - acc: 0.6275 - val_loss: 0.6801 - val_acc: 0.7059\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6541 - acc: 0.5882 - val_loss: 0.6800 - val_acc: 0.7059\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6540 - acc: 0.6078 - val_loss: 0.6799 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6556 - acc: 0.6667\n",
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_classification.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_classification.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_classification.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder_classification.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1913fc",
   "metadata": {},
   "source": [
    "Tras unas pocas iteraciones, descongelamos todas las capas y hacemos unas pocas épocas más entrenando y actualizando los pesos para el modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f79e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 168ms/step - loss: 0.6467 - acc: 0.6275 - val_loss: 0.6560 - val_acc: 0.7059\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5473 - acc: 0.7647 - val_loss: 0.6390 - val_acc: 0.7059\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4519 - acc: 0.8627 - val_loss: 0.6167 - val_acc: 0.7647\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3589 - acc: 0.9608 - val_loss: 0.6060 - val_acc: 0.7647\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2769 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.7647\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2072 - acc: 1.0000 - val_loss: 0.6054 - val_acc: 0.8235\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1894 - acc: 0.9608 - val_loss: 0.6122 - val_acc: 0.5882\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1306 - acc: 1.0000 - val_loss: 0.6185 - val_acc: 0.7647\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0868 - acc: 1.0000 - val_loss: 0.5912 - val_acc: 0.7059\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0661 - acc: 1.0000 - val_loss: 0.6080 - val_acc: 0.7059\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0500 - acc: 1.0000 - val_loss: 0.6247 - val_acc: 0.7059\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.6685 - val_acc: 0.7059\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.6483 - val_acc: 0.7059\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.6782 - val_acc: 0.7059\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.6694 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5718 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Descongelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_classification.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_classification.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_classification.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_encoder_classification = encoder_classification.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_encoder_classification * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86d4c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_classification.save(\"encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e613064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_pre_train_encoder = encoder_classification.predict(test_kaggle)\n",
    "y_pred_pre_train_encoder = np.around(y_pred_pre_train_encoder, decimals=0).ravel()\n",
    "\n",
    "create_submission(y_pred_pre_train_encoder, \"NN_pre_train_autoencoder1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69276e22",
   "metadata": {},
   "source": [
    "El objetivo es poder comparar con la misma configuración de red que la encoder sin utilizar los pesos pre-entrenados de esta y observar si hay mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd893f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "2/2 [==============================] - 1s 184ms/step - loss: 0.6693 - acc: 0.5098 - val_loss: 0.6605 - val_acc: 0.5882\n",
      "Epoch 2/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4618 - acc: 0.8824 - val_loss: 0.6751 - val_acc: 0.7059\n",
      "Epoch 3/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3048 - acc: 0.9608 - val_loss: 0.6953 - val_acc: 0.7059\n",
      "Epoch 4/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1893 - acc: 0.9804 - val_loss: 0.7126 - val_acc: 0.7647\n",
      "Epoch 5/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1170 - acc: 1.0000 - val_loss: 0.7912 - val_acc: 0.7647\n",
      "Epoch 6/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0785 - acc: 1.0000 - val_loss: 0.7260 - val_acc: 0.6471\n",
      "Epoch 7/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0511 - acc: 1.0000 - val_loss: 0.7264 - val_acc: 0.5294\n",
      "Epoch 8/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0435 - acc: 1.0000 - val_loss: 0.8056 - val_acc: 0.7647\n",
      "Epoch 9/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.6471\n",
      "Epoch 10/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.7798 - val_acc: 0.7059\n",
      "Epoch 11/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.8141 - val_acc: 0.7059\n",
      "Epoch 12/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 0.7647\n",
      "Epoch 13/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.8361 - val_acc: 0.6471\n",
      "Epoch 14/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.8772 - val_acc: 0.7059\n",
      "Epoch 15/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.8699 - val_acc: 0.7059\n",
      "Epoch 16/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.9101 - val_acc: 0.7059\n",
      "Epoch 17/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.9444 - val_acc: 0.7647\n",
      "Epoch 18/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9446 - val_acc: 0.7059\n",
      "Epoch 19/115\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9507 - val_acc: 0.7059\n",
      "Epoch 20/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9587 - val_acc: 0.7059\n",
      "Epoch 21/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.9892 - val_acc: 0.7059\n",
      "Epoch 22/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9875 - val_acc: 0.7059\n",
      "Epoch 23/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9866 - val_acc: 0.7059\n",
      "Epoch 24/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 9.0909e-04 - acc: 1.0000 - val_loss: 1.0305 - val_acc: 0.7647\n",
      "Epoch 25/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 7.7527e-04 - acc: 1.0000 - val_loss: 1.0220 - val_acc: 0.7059\n",
      "Epoch 26/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.6360e-04 - acc: 1.0000 - val_loss: 1.0279 - val_acc: 0.7059\n",
      "Epoch 27/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.7166e-04 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.7647\n",
      "Epoch 28/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.9303e-04 - acc: 1.0000 - val_loss: 1.0816 - val_acc: 0.7059\n",
      "Epoch 29/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.2080e-04 - acc: 1.0000 - val_loss: 1.0976 - val_acc: 0.7647\n",
      "Epoch 30/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.6369e-04 - acc: 1.0000 - val_loss: 1.1087 - val_acc: 0.7647\n",
      "Epoch 31/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3.1627e-04 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.7059\n",
      "Epoch 32/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.7655e-04 - acc: 1.0000 - val_loss: 1.1090 - val_acc: 0.7647\n",
      "Epoch 33/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.3353e-04 - acc: 1.0000 - val_loss: 1.1250 - val_acc: 0.7647\n",
      "Epoch 34/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.0175e-04 - acc: 1.0000 - val_loss: 1.1378 - val_acc: 0.7647\n",
      "Epoch 35/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.7746e-04 - acc: 1.0000 - val_loss: 1.1766 - val_acc: 0.7647\n",
      "Epoch 36/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.5307e-04 - acc: 1.0000 - val_loss: 1.1649 - val_acc: 0.8235\n",
      "Epoch 37/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.3158e-04 - acc: 1.0000 - val_loss: 1.1762 - val_acc: 0.8235\n",
      "Epoch 38/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1439e-04 - acc: 1.0000 - val_loss: 1.1882 - val_acc: 0.8235\n",
      "Epoch 39/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9.9930e-05 - acc: 1.0000 - val_loss: 1.2040 - val_acc: 0.8235\n",
      "Epoch 40/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 8.7287e-05 - acc: 1.0000 - val_loss: 1.2140 - val_acc: 0.8235\n",
      "Epoch 41/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.6753e-05 - acc: 1.0000 - val_loss: 1.2289 - val_acc: 0.8235\n",
      "Epoch 42/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.6743e-05 - acc: 1.0000 - val_loss: 1.2334 - val_acc: 0.8235\n",
      "Epoch 43/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.8153e-05 - acc: 1.0000 - val_loss: 1.2421 - val_acc: 0.8235\n",
      "Epoch 44/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.0445e-05 - acc: 1.0000 - val_loss: 1.2615 - val_acc: 0.8235\n",
      "Epoch 45/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.4469e-05 - acc: 1.0000 - val_loss: 1.2803 - val_acc: 0.8235\n",
      "Epoch 46/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.8873e-05 - acc: 1.0000 - val_loss: 1.2871 - val_acc: 0.8235\n",
      "Epoch 47/115\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.3714e-05 - acc: 1.0000 - val_loss: 1.2888 - val_acc: 0.8235\n",
      "Epoch 48/115\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9351e-05 - acc: 1.0000 - val_loss: 1.3006 - val_acc: 0.8235\n",
      "Epoch 49/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.5925e-05 - acc: 1.0000 - val_loss: 1.3219 - val_acc: 0.8235\n",
      "Epoch 50/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.2679e-05 - acc: 1.0000 - val_loss: 1.3078 - val_acc: 0.8235\n",
      "Epoch 51/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.9890e-05 - acc: 1.0000 - val_loss: 1.3200 - val_acc: 0.8235\n",
      "Epoch 52/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.7367e-05 - acc: 1.0000 - val_loss: 1.3332 - val_acc: 0.8235\n",
      "Epoch 53/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5276e-05 - acc: 1.0000 - val_loss: 1.3545 - val_acc: 0.8235\n",
      "Epoch 54/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.3623e-05 - acc: 1.0000 - val_loss: 1.3302 - val_acc: 0.8235\n",
      "Epoch 55/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1923e-05 - acc: 1.0000 - val_loss: 1.3651 - val_acc: 0.8235\n",
      "Epoch 56/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.0437e-05 - acc: 1.0000 - val_loss: 1.3611 - val_acc: 0.8235\n",
      "Epoch 57/115\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.3209e-06 - acc: 1.0000 - val_loss: 1.3616 - val_acc: 0.8235\n",
      "Epoch 58/115\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 8.1329e-06 - acc: 1.0000 - val_loss: 1.4058 - val_acc: 0.8235\n",
      "Epoch 59/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.1535e-06 - acc: 1.0000 - val_loss: 1.3867 - val_acc: 0.8235\n",
      "Epoch 60/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.3501e-06 - acc: 1.0000 - val_loss: 1.4234 - val_acc: 0.8235\n",
      "Epoch 61/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.6073e-06 - acc: 1.0000 - val_loss: 1.4211 - val_acc: 0.8235\n",
      "Epoch 62/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.9681e-06 - acc: 1.0000 - val_loss: 1.4391 - val_acc: 0.8235\n",
      "Epoch 63/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.4195e-06 - acc: 1.0000 - val_loss: 1.4541 - val_acc: 0.8235\n",
      "Epoch 64/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3.8758e-06 - acc: 1.0000 - val_loss: 1.4368 - val_acc: 0.8235\n",
      "Epoch 65/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3.4163e-06 - acc: 1.0000 - val_loss: 1.4705 - val_acc: 0.8235\n",
      "Epoch 66/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3.0136e-06 - acc: 1.0000 - val_loss: 1.4726 - val_acc: 0.8235\n",
      "Epoch 67/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.6696e-06 - acc: 1.0000 - val_loss: 1.4833 - val_acc: 0.8235\n",
      "Epoch 68/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.3741e-06 - acc: 1.0000 - val_loss: 1.4731 - val_acc: 0.8235\n",
      "Epoch 69/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.1013e-06 - acc: 1.0000 - val_loss: 1.5017 - val_acc: 0.8235\n",
      "Epoch 70/115\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.8568e-06 - acc: 1.0000 - val_loss: 1.5054 - val_acc: 0.8235\n",
      "Epoch 71/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.6485e-06 - acc: 1.0000 - val_loss: 1.5266 - val_acc: 0.8235\n",
      "Epoch 72/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.4561e-06 - acc: 1.0000 - val_loss: 1.5191 - val_acc: 0.8235\n",
      "Epoch 73/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.2867e-06 - acc: 1.0000 - val_loss: 1.5250 - val_acc: 0.8235\n",
      "Epoch 74/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1402e-06 - acc: 1.0000 - val_loss: 1.5370 - val_acc: 0.8235\n",
      "Epoch 75/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0109e-06 - acc: 1.0000 - val_loss: 1.5459 - val_acc: 0.8235\n",
      "Epoch 76/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 9.0304e-07 - acc: 1.0000 - val_loss: 1.5627 - val_acc: 0.8235\n",
      "Epoch 77/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 8.0273e-07 - acc: 1.0000 - val_loss: 1.5694 - val_acc: 0.8235\n",
      "Epoch 78/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.1617e-07 - acc: 1.0000 - val_loss: 1.5684 - val_acc: 0.8235\n",
      "Epoch 79/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.4340e-07 - acc: 1.0000 - val_loss: 1.5763 - val_acc: 0.8235\n",
      "Epoch 80/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.8612e-07 - acc: 1.0000 - val_loss: 1.5682 - val_acc: 0.8235\n",
      "Epoch 81/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.1891e-07 - acc: 1.0000 - val_loss: 1.5983 - val_acc: 0.8235\n",
      "Epoch 82/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.6103e-07 - acc: 1.0000 - val_loss: 1.6143 - val_acc: 0.8235\n",
      "Epoch 83/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.1284e-07 - acc: 1.0000 - val_loss: 1.6132 - val_acc: 0.8235\n",
      "Epoch 84/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3.6864e-07 - acc: 1.0000 - val_loss: 1.6312 - val_acc: 0.8235\n",
      "Epoch 85/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.3675e-07 - acc: 1.0000 - val_loss: 1.6614 - val_acc: 0.8235\n",
      "Epoch 86/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3.0140e-07 - acc: 1.0000 - val_loss: 1.6562 - val_acc: 0.8235\n",
      "Epoch 87/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.7313e-07 - acc: 1.0000 - val_loss: 1.6785 - val_acc: 0.8235\n",
      "Epoch 88/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.4393e-07 - acc: 1.0000 - val_loss: 1.6712 - val_acc: 0.8235\n",
      "Epoch 89/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.1880e-07 - acc: 1.0000 - val_loss: 1.6813 - val_acc: 0.8235\n",
      "Epoch 90/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.9830e-07 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 0.8235\n",
      "Epoch 91/115\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.8013e-07 - acc: 1.0000 - val_loss: 1.6994 - val_acc: 0.8235\n",
      "Epoch 92/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.6470e-07 - acc: 1.0000 - val_loss: 1.7166 - val_acc: 0.8235\n",
      "Epoch 93/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.5197e-07 - acc: 1.0000 - val_loss: 1.7270 - val_acc: 0.8235\n",
      "Epoch 94/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.3877e-07 - acc: 1.0000 - val_loss: 1.7034 - val_acc: 0.8235\n",
      "Epoch 95/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.2558e-07 - acc: 1.0000 - val_loss: 1.7084 - val_acc: 0.8235\n",
      "Epoch 96/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.1337e-07 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 0.8235\n",
      "Epoch 97/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0333e-07 - acc: 1.0000 - val_loss: 1.7283 - val_acc: 0.8235\n",
      "Epoch 98/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 9.4769e-08 - acc: 1.0000 - val_loss: 1.7571 - val_acc: 0.8235\n",
      "Epoch 99/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.6968e-08 - acc: 1.0000 - val_loss: 1.7564 - val_acc: 0.8235\n",
      "Epoch 100/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 8.0738e-08 - acc: 1.0000 - val_loss: 1.7418 - val_acc: 0.8235\n",
      "Epoch 101/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 7.2858e-08 - acc: 1.0000 - val_loss: 1.7559 - val_acc: 0.8235\n",
      "Epoch 102/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 6.7201e-08 - acc: 1.0000 - val_loss: 1.7734 - val_acc: 0.8235\n",
      "Epoch 103/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.1381e-08 - acc: 1.0000 - val_loss: 1.7806 - val_acc: 0.8235\n",
      "Epoch 104/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.6480e-08 - acc: 1.0000 - val_loss: 1.7856 - val_acc: 0.8235\n",
      "Epoch 105/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.2409e-08 - acc: 1.0000 - val_loss: 1.7968 - val_acc: 0.8235\n",
      "Epoch 106/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8561e-08 - acc: 1.0000 - val_loss: 1.8040 - val_acc: 0.8235\n",
      "Epoch 107/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.4462e-08 - acc: 1.0000 - val_loss: 1.7960 - val_acc: 0.8235\n",
      "Epoch 108/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.1380e-08 - acc: 1.0000 - val_loss: 1.7946 - val_acc: 0.8235\n",
      "Epoch 109/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3.8501e-08 - acc: 1.0000 - val_loss: 1.8029 - val_acc: 0.8235\n",
      "Epoch 110/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.5565e-08 - acc: 1.0000 - val_loss: 1.8077 - val_acc: 0.8235\n",
      "Epoch 111/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.3097e-08 - acc: 1.0000 - val_loss: 1.8297 - val_acc: 0.8235\n",
      "Epoch 112/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.0843e-08 - acc: 1.0000 - val_loss: 1.8331 - val_acc: 0.8235\n",
      "Epoch 113/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.9146e-08 - acc: 1.0000 - val_loss: 1.8365 - val_acc: 0.8235\n",
      "Epoch 114/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.7477e-08 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 0.8235\n",
      "Epoch 115/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.5902e-08 - acc: 1.0000 - val_loss: 1.8488 - val_acc: 0.8235\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6085 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "no_pre_train = models.Sequential()\n",
    "no_pre_train.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "no_pre_train.add(layers.Dense(100, activation=\"relu\"))\n",
    "no_pre_train.add(layers.Dense(50, activation=\"relu\"))\n",
    "no_pre_train.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "no_pre_train.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "no_pre_train.fit(X_train, y_train, epochs=115, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_no_pre_train = no_pre_train.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_no_pre_train * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "adce72f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_no_pre_train = no_pre_train.predict(test_kaggle)\n",
    "y_pred_no_pre_train = np.around(y_pred_no_pre_train, decimals=0).ravel()\n",
    "\n",
    "create_submission(y_pred_no_pre_train, \"NN_no_pre_train1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f58ea6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_acc = [accuracy_no_pre_train, accuracy_encoder_classification]\n",
    "labels = [\"Red sin pre-entrenar\", \"Red pre-entrenanda\\n con autoencoder\"]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "barlist = plt.bar(labels, results_acc, width=0.25, align=\"center\")\n",
    "barlist[0].set_color(\"r\")\n",
    "plt.xlim(-0.45, len(labels) -1 + 0.45)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364e970",
   "metadata": {},
   "source": [
    "**Vamos a probar con una configuración de autoencoder más compleja**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4a3da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1778 - val_loss: 0.1727\n",
      "Epoch 2/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1672 - val_loss: 0.1634\n",
      "Epoch 3/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1604 - val_loss: 0.1585\n",
      "Epoch 4/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1564 - val_loss: 0.1551\n",
      "Epoch 5/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1532 - val_loss: 0.1525\n",
      "Epoch 6/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1508 - val_loss: 0.1500\n",
      "Epoch 7/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1485 - val_loss: 0.1478\n",
      "Epoch 8/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1468 - val_loss: 0.1475\n",
      "Epoch 9/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1458 - val_loss: 0.1473\n",
      "Epoch 10/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1451 - val_loss: 0.1474\n",
      "Epoch 11/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1446 - val_loss: 0.1459\n",
      "Epoch 12/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1440 - val_loss: 0.1456\n",
      "Epoch 13/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1434 - val_loss: 0.1452\n",
      "Epoch 14/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1426 - val_loss: 0.1431\n",
      "Epoch 15/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1415 - val_loss: 0.1436\n",
      "Epoch 16/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1408 - val_loss: 0.1429\n",
      "Epoch 17/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1401 - val_loss: 0.1409\n",
      "Epoch 18/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1393 - val_loss: 0.1409\n",
      "Epoch 19/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1384 - val_loss: 0.1404\n",
      "Epoch 20/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1376 - val_loss: 0.1388\n",
      "Epoch 21/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1369 - val_loss: 0.1409\n",
      "Epoch 22/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1362 - val_loss: 0.1390\n",
      "Epoch 23/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1356 - val_loss: 0.1364\n",
      "Epoch 24/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1350 - val_loss: 0.1361\n",
      "Epoch 25/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1343 - val_loss: 0.1354\n",
      "Epoch 26/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1338 - val_loss: 0.1360\n",
      "Epoch 27/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1333 - val_loss: 0.1369\n",
      "Epoch 28/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1328 - val_loss: 0.1346\n",
      "Epoch 29/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1323 - val_loss: 0.1337\n",
      "Epoch 30/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1319 - val_loss: 0.1345\n",
      "Epoch 31/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1317 - val_loss: 0.1329\n",
      "Epoch 32/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1313 - val_loss: 0.1321\n",
      "Epoch 33/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1308 - val_loss: 0.1318\n",
      "Epoch 34/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1307 - val_loss: 0.1312\n",
      "Epoch 35/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1306 - val_loss: 0.1301\n",
      "Epoch 36/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1304 - val_loss: 0.1308\n",
      "Epoch 37/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1300 - val_loss: 0.1307\n",
      "Epoch 38/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1301 - val_loss: 0.1312\n",
      "Epoch 39/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1297 - val_loss: 0.1299\n",
      "Epoch 40/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1290 - val_loss: 0.1308\n",
      "Epoch 41/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1290 - val_loss: 0.1311\n",
      "Epoch 42/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1287 - val_loss: 0.1295\n",
      "Epoch 43/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1282 - val_loss: 0.1286\n",
      "Epoch 44/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1275 - val_loss: 0.1293\n",
      "Epoch 45/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1273 - val_loss: 0.1282\n",
      "Epoch 46/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1267 - val_loss: 0.1288\n",
      "Epoch 47/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1282\n",
      "Epoch 48/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1265 - val_loss: 0.1281\n",
      "Epoch 49/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1269 - val_loss: 0.1277\n",
      "Epoch 50/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1268 - val_loss: 0.1280\n",
      "Epoch 51/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1268 - val_loss: 0.1260\n",
      "Epoch 52/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1261 - val_loss: 0.1278\n",
      "Epoch 53/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1257 - val_loss: 0.1263\n",
      "Epoch 54/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1256 - val_loss: 0.1278\n",
      "Epoch 55/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1289\n",
      "Epoch 56/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1261 - val_loss: 0.1262\n",
      "Epoch 57/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1258 - val_loss: 0.1281\n",
      "Epoch 58/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1262 - val_loss: 0.1269\n",
      "Epoch 59/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1257 - val_loss: 0.1277\n",
      "Epoch 60/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1257 - val_loss: 0.1263\n",
      "Epoch 61/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1259 - val_loss: 0.1262\n",
      "Epoch 62/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1263 - val_loss: 0.1261\n",
      "Epoch 63/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1271 - val_loss: 0.1278\n",
      "Epoch 64/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1265 - val_loss: 0.1273\n",
      "Epoch 65/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1256 - val_loss: 0.1262\n",
      "Epoch 66/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1301\n",
      "Epoch 67/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1262 - val_loss: 0.1270\n",
      "Epoch 68/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1266 - val_loss: 0.1273\n",
      "Epoch 69/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1266 - val_loss: 0.1265\n",
      "Epoch 70/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1273 - val_loss: 0.1287\n",
      "Epoch 71/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1262\n",
      "Epoch 72/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1265 - val_loss: 0.1275\n",
      "Epoch 73/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1265 - val_loss: 0.1296\n",
      "Epoch 74/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1275 - val_loss: 0.1293\n",
      "Epoch 75/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1274 - val_loss: 0.1310\n",
      "Epoch 76/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1271 - val_loss: 0.1287\n",
      "Epoch 77/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1289\n",
      "Epoch 78/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1270 - val_loss: 0.1284\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1273 - val_loss: 0.1283\n",
      "Epoch 80/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1272 - val_loss: 0.1279\n",
      "Epoch 81/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1274 - val_loss: 0.1289\n",
      "Epoch 82/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1277 - val_loss: 0.1268\n",
      "Epoch 83/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1278 - val_loss: 0.1292\n",
      "Epoch 84/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1276 - val_loss: 0.1299\n",
      "Epoch 85/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1279 - val_loss: 0.1283\n",
      "Epoch 86/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1273 - val_loss: 0.1291\n",
      "Epoch 87/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1278 - val_loss: 0.1282\n",
      "Epoch 88/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1269 - val_loss: 0.1273\n",
      "Epoch 89/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1277 - val_loss: 0.1274\n",
      "Epoch 90/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1274 - val_loss: 0.1287\n",
      "Epoch 91/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1276 - val_loss: 0.1299\n",
      "Epoch 92/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1276 - val_loss: 0.1283\n",
      "Epoch 93/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1272 - val_loss: 0.1298\n",
      "Epoch 94/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1290 - val_loss: 0.1282\n",
      "Epoch 95/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1279 - val_loss: 0.1301\n",
      "Epoch 96/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1282 - val_loss: 0.1295\n",
      "Epoch 97/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1282 - val_loss: 0.1283\n",
      "Epoch 98/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1283 - val_loss: 0.1320\n",
      "Epoch 99/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1286 - val_loss: 0.1312\n",
      "Epoch 100/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1274 - val_loss: 0.1290\n",
      "Epoch 101/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1273 - val_loss: 0.1264\n",
      "Epoch 102/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1276 - val_loss: 0.1279\n",
      "Epoch 103/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1284 - val_loss: 0.1284\n",
      "Epoch 104/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1282 - val_loss: 0.1296\n",
      "Epoch 105/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1301\n",
      "Epoch 106/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1285 - val_loss: 0.1324\n",
      "Epoch 107/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1293 - val_loss: 0.1290\n",
      "Epoch 108/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1290 - val_loss: 0.1301\n",
      "Epoch 109/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1290 - val_loss: 0.1282\n",
      "Epoch 110/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1285 - val_loss: 0.1304\n",
      "Epoch 111/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1290 - val_loss: 0.1284\n",
      "Epoch 112/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1288 - val_loss: 0.1305\n",
      "Epoch 113/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1291 - val_loss: 0.1277\n",
      "Epoch 114/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1309\n",
      "Epoch 115/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1286 - val_loss: 0.1306\n",
      "Epoch 116/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1285 - val_loss: 0.1320\n",
      "Epoch 117/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1302 - val_loss: 0.1294\n",
      "Epoch 118/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1287 - val_loss: 0.1307\n",
      "Epoch 119/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1288 - val_loss: 0.1299\n",
      "Epoch 120/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1289 - val_loss: 0.1314\n",
      "Epoch 121/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1284 - val_loss: 0.1282\n",
      "Epoch 122/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1287 - val_loss: 0.1296\n",
      "Epoch 123/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1289 - val_loss: 0.1310\n",
      "Epoch 124/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1299 - val_loss: 0.1330\n",
      "Epoch 125/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1306 - val_loss: 0.1310\n",
      "Epoch 126/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1296 - val_loss: 0.1307\n",
      "Epoch 127/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1293 - val_loss: 0.1350\n",
      "Epoch 128/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1303 - val_loss: 0.1327\n",
      "Epoch 129/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1298 - val_loss: 0.1307\n",
      "Epoch 130/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1296 - val_loss: 0.1309\n",
      "Epoch 131/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1293 - val_loss: 0.1289\n",
      "Epoch 132/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1289 - val_loss: 0.1308\n",
      "Epoch 133/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1285 - val_loss: 0.1292\n",
      "Epoch 134/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1287 - val_loss: 0.1305\n",
      "Epoch 135/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1298\n",
      "Epoch 136/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1286 - val_loss: 0.1292\n",
      "Epoch 137/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1304\n",
      "Epoch 138/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1281 - val_loss: 0.1305\n",
      "Epoch 139/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1280 - val_loss: 0.1291\n",
      "Epoch 140/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1281 - val_loss: 0.1293\n",
      "Epoch 141/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1287 - val_loss: 0.1315\n",
      "Epoch 142/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1296 - val_loss: 0.1306\n",
      "Epoch 143/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1283 - val_loss: 0.1312\n",
      "Epoch 144/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1284 - val_loss: 0.1289\n",
      "Epoch 145/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1280 - val_loss: 0.1292\n",
      "Epoch 146/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1285 - val_loss: 0.1310\n",
      "Epoch 147/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1284 - val_loss: 0.1301\n",
      "Epoch 148/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1280 - val_loss: 0.1273\n",
      "Epoch 149/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1279 - val_loss: 0.1288\n",
      "Epoch 150/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1279 - val_loss: 0.1296\n",
      "Epoch 151/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1286 - val_loss: 0.1297\n",
      "Epoch 152/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1282 - val_loss: 0.1285\n",
      "Epoch 153/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1280 - val_loss: 0.1284\n",
      "Epoch 154/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1281 - val_loss: 0.1314\n",
      "Epoch 155/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1280 - val_loss: 0.1270\n",
      "Epoch 156/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1282 - val_loss: 0.1306\n",
      "Epoch 157/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1279 - val_loss: 0.1286\n",
      "Epoch 158/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1276 - val_loss: 0.1303\n",
      "Epoch 159/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1283 - val_loss: 0.1275\n",
      "Epoch 160/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1275 - val_loss: 0.1293\n",
      "Epoch 161/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1275 - val_loss: 0.1288\n",
      "Epoch 162/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1285 - val_loss: 0.1315\n",
      "Epoch 163/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1289 - val_loss: 0.1295\n",
      "Epoch 164/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1280 - val_loss: 0.1296\n",
      "Epoch 165/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1285 - val_loss: 0.1298\n",
      "Epoch 166/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1284 - val_loss: 0.1265\n",
      "Epoch 167/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1280 - val_loss: 0.1287\n",
      "Epoch 168/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1292 - val_loss: 0.1322\n",
      "Epoch 169/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1285 - val_loss: 0.1295\n",
      "Epoch 170/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1287 - val_loss: 0.1319\n",
      "Epoch 171/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1290 - val_loss: 0.1307\n",
      "Epoch 172/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1289 - val_loss: 0.1322\n",
      "Epoch 173/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1307\n",
      "Epoch 174/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1286 - val_loss: 0.1310\n",
      "Epoch 175/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1285 - val_loss: 0.1335\n",
      "Epoch 176/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1292 - val_loss: 0.1284\n",
      "Epoch 177/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1283 - val_loss: 0.1282\n",
      "Epoch 178/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1291 - val_loss: 0.1302\n",
      "Epoch 179/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1294 - val_loss: 0.1303\n",
      "Epoch 180/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1286 - val_loss: 0.1343\n",
      "Epoch 181/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1286 - val_loss: 0.1288\n",
      "Epoch 182/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1283 - val_loss: 0.1298\n",
      "Epoch 183/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1291 - val_loss: 0.1301\n",
      "Epoch 184/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1294 - val_loss: 0.1323\n",
      "Epoch 185/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1292 - val_loss: 0.1327\n",
      "Epoch 186/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1295 - val_loss: 0.1320\n",
      "Epoch 187/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1294 - val_loss: 0.1305\n",
      "Epoch 188/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1294 - val_loss: 0.1284\n",
      "Epoch 189/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1296 - val_loss: 0.1309\n",
      "Epoch 190/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1291 - val_loss: 0.1307\n",
      "Epoch 191/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1290 - val_loss: 0.1284\n",
      "Epoch 192/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1294 - val_loss: 0.1306\n",
      "Epoch 193/200\n",
      "1404/1404 [==============================] - 18s 13ms/step - loss: 0.1288 - val_loss: 0.1298\n",
      "Epoch 194/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1293 - val_loss: 0.1296\n",
      "Epoch 195/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1290 - val_loss: 0.1290\n",
      "Epoch 196/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1285 - val_loss: 0.1286\n",
      "Epoch 197/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1290 - val_loss: 0.1301\n",
      "Epoch 198/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1284 - val_loss: 0.1325\n",
      "Epoch 199/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1294 - val_loss: 0.1281\n",
      "Epoch 200/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1293 - val_loss: 0.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e86e039a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer2 = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded2 = layers.Dense(300, activation=\"relu\")(input_layer2)\n",
    "encoded2 = layers.Dense(250, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(200, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(150, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(100, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(50, activation=\"relu\")(encoded2)\n",
    "# Capas red decoder\n",
    "decoded2 = layers.Dense(50, activation=\"relu\")(encoded2)\n",
    "decoded2 = layers.Dense(100, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(150, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(200, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(250, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(300, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(410, activation=\"linear\")(decoded2)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder2 = models.Model(input_layer2, decoded2)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder2.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder2.fit(test_kaggle, test_kaggle, epochs=200, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cac4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2.save(\"autoencoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f747b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2 = models.load_model(\"autoencoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b26090fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(367.13974, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.121041566, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train_tot (etiquetados)\n",
    "X_train_tot_pred = autoencoder2.predict(X_train_tot)\n",
    "\n",
    "print(\"MAPE:\", mape(X_train_tot, X_train_tot_pred))\n",
    "print(\"MSE:\", mse(X_train_tot, X_train_tot_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11ef5c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp = np.abs((X_train_tot - X_train_tot_pred) / X_train_tot) * 100\n",
    "mape_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f05e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mape_tmp.mean(axis=0))\n",
    "plt.xlabel(\"MAPE error\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a98cb2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mape_tmp.mean(axis=0), bins=1000)\n",
    "plt.xlim([0, 500])\n",
    "plt.xlabel(\"MAPE error\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d99e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03414634146341464"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mape_tmp.mean(axis=0) < 100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5379c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(452.5427, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.12940164, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "test_kaggle_pred = autoencoder2.predict(test_kaggle)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))\n",
    "print(\"MSE:\", mse(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fca423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 434.4195556640625 +- 313.5384826660156\n"
     ]
    }
   ],
   "source": [
    "samples_num = 100\n",
    "mape_samples = []\n",
    "\n",
    "for _ in range(samples_num):\n",
    "    \n",
    "    args = np.random.choice(a=np.arange(0, test_kaggle.shape[0]), size=86, replace=False)\n",
    "\n",
    "    test_kaggle_reduc = test_kaggle.iloc[args, :]\n",
    "\n",
    "    y_pred_kaggle_reduc = autoencoder2.predict(test_kaggle_reduc)\n",
    "    tmp_mape = mape(test_kaggle_reduc, y_pred_kaggle_reduc)\n",
    "    mape_samples.append(tmp_mape)\n",
    "    \n",
    "print(f'MAPE: {np.mean(mape_samples)} +- {np.std(mape_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7608b",
   "metadata": {},
   "source": [
    "En este caso, aumentar la complejidad del autoencoder no mejora excesivamente los resultados.\n",
    "\n",
    "Vamos a ver si se observa algún tipo de mejoría en los resultados de la red de clasificación (encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebcb4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "copy_autoencoder2 = models.clone_model(autoencoder2)\n",
    "copy_autoencoder2.build((None, 410))\n",
    "copy_autoencoder2.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "copy_autoencoder2.set_weights(autoencoder2.get_weights())\n",
    "\n",
    "encoder_input2 = layers.Input(shape=(410,))\n",
    "encoder2 = encoder_input2\n",
    "for layer in copy_autoencoder2.layers[1:7]:\n",
    "    encoder2 = layer(encoder2)\n",
    "encoder2 = models.Model(inputs=encoder_input2, outputs=encoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88afd9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               123300    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 250)               75250     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 200)               50200     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299,050\n",
      "Trainable params: 299,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f591180",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clasificacion2 = models.Sequential()\n",
    "encoder_clasificacion2.add(encoder2)\n",
    "encoder_clasificacion2.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3e3d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 1.9184 - acc: 0.5098 - val_loss: 1.2401 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.7995 - acc: 0.5098 - val_loss: 1.1947 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.7178 - acc: 0.5098 - val_loss: 1.1591 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.6495 - acc: 0.5098 - val_loss: 1.1319 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.5925 - acc: 0.5098 - val_loss: 1.1096 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.5412 - acc: 0.5098 - val_loss: 1.0903 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4982 - acc: 0.5294 - val_loss: 1.0741 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4548 - acc: 0.5098 - val_loss: 1.0642 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4227 - acc: 0.4902 - val_loss: 1.0541 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.3909 - acc: 0.5098 - val_loss: 1.0466 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.3624 - acc: 0.5098 - val_loss: 1.0403 - val_acc: 0.5882\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.3394 - acc: 0.5294 - val_loss: 1.0346 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3151 - acc: 0.5294 - val_loss: 1.0293 - val_acc: 0.6471\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2892 - acc: 0.5490 - val_loss: 1.0260 - val_acc: 0.6471\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2678 - acc: 0.5686 - val_loss: 1.0239 - val_acc: 0.6471\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.2475 - acc: 0.5686 - val_loss: 1.0203 - val_acc: 0.6471\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.2383 - acc: 0.5686 - val_loss: 1.0178 - val_acc: 0.6471\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.2235 - acc: 0.5882 - val_loss: 1.0156 - val_acc: 0.6471\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.2103 - acc: 0.5882 - val_loss: 1.0140 - val_acc: 0.6471\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1984 - acc: 0.5882 - val_loss: 1.0125 - val_acc: 0.6471\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1903 - acc: 0.5882 - val_loss: 1.0104 - val_acc: 0.6471\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1785 - acc: 0.6275 - val_loss: 1.0087 - val_acc: 0.6471\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.1670 - acc: 0.6275 - val_loss: 1.0080 - val_acc: 0.6471\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1578 - acc: 0.6471 - val_loss: 1.0077 - val_acc: 0.6471\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1478 - acc: 0.6275 - val_loss: 1.0064 - val_acc: 0.6471\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1391 - acc: 0.6275 - val_loss: 1.0051 - val_acc: 0.6471\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1368 - acc: 0.6078 - val_loss: 1.0023 - val_acc: 0.6471\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.1259 - acc: 0.6078 - val_loss: 1.0013 - val_acc: 0.6471\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1186 - acc: 0.6078 - val_loss: 1.0012 - val_acc: 0.6471\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1130 - acc: 0.6078 - val_loss: 0.9984 - val_acc: 0.6471\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1102 - acc: 0.6078 - val_loss: 1.0010 - val_acc: 0.6471\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1006 - acc: 0.6078 - val_loss: 1.0017 - val_acc: 0.6471\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0979 - acc: 0.6078 - val_loss: 1.0047 - val_acc: 0.6471\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.0913 - acc: 0.6275 - val_loss: 1.0064 - val_acc: 0.6471\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0848 - acc: 0.6275 - val_loss: 1.0021 - val_acc: 0.6471\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.0802 - acc: 0.6078 - val_loss: 1.0016 - val_acc: 0.6471\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0759 - acc: 0.6078 - val_loss: 0.9972 - val_acc: 0.6471\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0713 - acc: 0.6078 - val_loss: 0.9953 - val_acc: 0.6471\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0672 - acc: 0.6078 - val_loss: 0.9907 - val_acc: 0.6471\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.0610 - acc: 0.6078 - val_loss: 0.9872 - val_acc: 0.6471\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0567 - acc: 0.6078 - val_loss: 0.9870 - val_acc: 0.6471\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0518 - acc: 0.6078 - val_loss: 0.9818 - val_acc: 0.6471\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.0492 - acc: 0.6078 - val_loss: 0.9745 - val_acc: 0.6471\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0445 - acc: 0.6078 - val_loss: 0.9676 - val_acc: 0.6471\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.0401 - acc: 0.6078 - val_loss: 0.9619 - val_acc: 0.6471\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0315 - acc: 0.6078 - val_loss: 0.9592 - val_acc: 0.6471\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0278 - acc: 0.6078 - val_loss: 0.9569 - val_acc: 0.6471\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0254 - acc: 0.6078 - val_loss: 0.9507 - val_acc: 0.6471\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0187 - acc: 0.6078 - val_loss: 0.9504 - val_acc: 0.6471\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0136 - acc: 0.6078 - val_loss: 0.9500 - val_acc: 0.6471\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0121 - acc: 0.6078 - val_loss: 0.9435 - val_acc: 0.6471\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0065 - acc: 0.6078 - val_loss: 0.9429 - val_acc: 0.6471\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0000 - acc: 0.6078 - val_loss: 0.9377 - val_acc: 0.6471\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9991 - acc: 0.6078 - val_loss: 0.9391 - val_acc: 0.6471\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9914 - acc: 0.6078 - val_loss: 0.9332 - val_acc: 0.6471\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9874 - acc: 0.6078 - val_loss: 0.9320 - val_acc: 0.6471\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9838 - acc: 0.6078 - val_loss: 0.9304 - val_acc: 0.6471\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.9807 - acc: 0.6078 - val_loss: 0.9222 - val_acc: 0.6471\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.9736 - acc: 0.6078 - val_loss: 0.9179 - val_acc: 0.6471\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9753 - acc: 0.6078 - val_loss: 0.9110 - val_acc: 0.6471\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9658 - acc: 0.6078 - val_loss: 0.9062 - val_acc: 0.6471\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9614 - acc: 0.6078 - val_loss: 0.9033 - val_acc: 0.6471\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9591 - acc: 0.6078 - val_loss: 0.8975 - val_acc: 0.6471\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9556 - acc: 0.6078 - val_loss: 0.8982 - val_acc: 0.6471\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9516 - acc: 0.6078 - val_loss: 0.8933 - val_acc: 0.6471\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9473 - acc: 0.6078 - val_loss: 0.8881 - val_acc: 0.6471\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9457 - acc: 0.6078 - val_loss: 0.8821 - val_acc: 0.6471\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.9387 - acc: 0.6078 - val_loss: 0.8799 - val_acc: 0.6471\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9348 - acc: 0.6078 - val_loss: 0.8788 - val_acc: 0.6471\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.9318 - acc: 0.6078 - val_loss: 0.8778 - val_acc: 0.6471\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.9295 - acc: 0.6078 - val_loss: 0.8717 - val_acc: 0.6471\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9233 - acc: 0.6078 - val_loss: 0.8678 - val_acc: 0.6471\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9194 - acc: 0.6078 - val_loss: 0.8657 - val_acc: 0.6471\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9142 - acc: 0.6078 - val_loss: 0.8640 - val_acc: 0.6471\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9112 - acc: 0.6078 - val_loss: 0.8603 - val_acc: 0.6471\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9063 - acc: 0.6078 - val_loss: 0.8573 - val_acc: 0.6471\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9033 - acc: 0.6078 - val_loss: 0.8525 - val_acc: 0.6471\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8972 - acc: 0.6078 - val_loss: 0.8512 - val_acc: 0.6471\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8942 - acc: 0.6078 - val_loss: 0.8491 - val_acc: 0.6471\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8919 - acc: 0.6078 - val_loss: 0.8503 - val_acc: 0.6471\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8851 - acc: 0.6078 - val_loss: 0.8483 - val_acc: 0.6471\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8828 - acc: 0.6078 - val_loss: 0.8422 - val_acc: 0.6471\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8848 - acc: 0.6078 - val_loss: 0.8441 - val_acc: 0.6471\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8738 - acc: 0.6078 - val_loss: 0.8388 - val_acc: 0.6471\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8700 - acc: 0.6078 - val_loss: 0.8352 - val_acc: 0.6471\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8675 - acc: 0.6078 - val_loss: 0.8342 - val_acc: 0.6471\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8661 - acc: 0.6078 - val_loss: 0.8278 - val_acc: 0.6471\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8592 - acc: 0.6078 - val_loss: 0.8250 - val_acc: 0.6471\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8560 - acc: 0.6078 - val_loss: 0.8236 - val_acc: 0.6471\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8530 - acc: 0.6078 - val_loss: 0.8228 - val_acc: 0.6471\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8496 - acc: 0.6078 - val_loss: 0.8200 - val_acc: 0.6471\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8466 - acc: 0.6078 - val_loss: 0.8154 - val_acc: 0.6471\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8424 - acc: 0.6078 - val_loss: 0.8102 - val_acc: 0.6471\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8407 - acc: 0.6275 - val_loss: 0.8114 - val_acc: 0.6471\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8379 - acc: 0.6078 - val_loss: 0.8129 - val_acc: 0.6471\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8312 - acc: 0.6078 - val_loss: 0.8110 - val_acc: 0.6471\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8294 - acc: 0.6078 - val_loss: 0.8105 - val_acc: 0.6471\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8320 - acc: 0.6078 - val_loss: 0.8042 - val_acc: 0.6471\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8215 - acc: 0.6275 - val_loss: 0.8024 - val_acc: 0.6471\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8245 - acc: 0.6275 - val_loss: 0.8044 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0039 - acc: 0.5556\n",
      "Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_clasificacion2.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_clasificacion2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_clasificacion2.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder_clasificacion2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef566bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 234ms/step - loss: 0.8294 - acc: 0.6078 - val_loss: 0.8819 - val_acc: 0.4706\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9482 - acc: 0.5882 - val_loss: 0.9035 - val_acc: 0.5882\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5651 - acc: 0.7451 - val_loss: 0.7633 - val_acc: 0.6471\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3117 - acc: 0.8431 - val_loss: 0.7735 - val_acc: 0.6471\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2729 - acc: 0.8431 - val_loss: 0.7867 - val_acc: 0.6471\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2737 - acc: 0.8824 - val_loss: 0.7244 - val_acc: 0.6471\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2190 - acc: 0.9412 - val_loss: 0.9827 - val_acc: 0.4706\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2071 - acc: 0.9412 - val_loss: 0.8128 - val_acc: 0.6471\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1655 - acc: 0.9412 - val_loss: 0.8008 - val_acc: 0.6471\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1285 - acc: 1.0000 - val_loss: 0.8149 - val_acc: 0.6471\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1213 - acc: 0.9804 - val_loss: 0.8701 - val_acc: 0.7059\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0993 - acc: 1.0000 - val_loss: 0.8138 - val_acc: 0.6471\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0873 - acc: 1.0000 - val_loss: 0.9433 - val_acc: 0.5882\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0843 - acc: 1.0000 - val_loss: 0.8418 - val_acc: 0.6471\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0594 - acc: 1.0000 - val_loss: 0.8730 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1405 - acc: 0.5556\n",
      "Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Descongelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_clasificacion2.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_clasificacion2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_clasificacion2.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_encoder_clasificacion2 = encoder_clasificacion2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_encoder_clasificacion2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "358a4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clasificacion2.save(\"encoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcb73b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_pre_train_encoder2 = encoder_clasificacion2.predict(test_kaggle)\n",
    "y_pred_pre_train_encoder2 = np.around(y_pred_pre_train_encoder2, decimals=0).ravel()\n",
    "\n",
    "create_submission(y_pred_pre_train_encoder2, \"NN_pre_train_autoencoder2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aae118",
   "metadata": {},
   "source": [
    "Red sin pre-entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "574fa0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "2/2 [==============================] - 12s 6s/step - loss: 0.6886 - acc: 0.5098 - val_loss: 0.6569 - val_acc: 0.5882\n",
      "Epoch 2/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6410 - acc: 0.5294 - val_loss: 0.6890 - val_acc: 0.5882\n",
      "Epoch 3/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5060 - acc: 0.7647 - val_loss: 0.7015 - val_acc: 0.7059\n",
      "Epoch 4/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2672 - acc: 0.9216 - val_loss: 1.0483 - val_acc: 0.7059\n",
      "Epoch 5/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2531 - acc: 0.9216 - val_loss: 0.7821 - val_acc: 0.7647\n",
      "Epoch 6/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0724 - acc: 1.0000 - val_loss: 0.8761 - val_acc: 0.7647\n",
      "Epoch 7/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.9357 - val_acc: 0.7647\n",
      "Epoch 8/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.7647\n",
      "Epoch 9/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 1.1005 - val_acc: 0.7647\n",
      "Epoch 10/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 1.1301 - val_acc: 0.7647\n",
      "Epoch 11/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.1710 - val_acc: 0.7647\n",
      "Epoch 12/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.2232 - val_acc: 0.7647\n",
      "Epoch 13/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.2658 - val_acc: 0.7647\n",
      "Epoch 14/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.1661e-04 - acc: 1.0000 - val_loss: 1.3022 - val_acc: 0.7647\n",
      "Epoch 15/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 7.0584e-04 - acc: 1.0000 - val_loss: 1.3250 - val_acc: 0.7647\n",
      "Epoch 16/115\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 5.5042e-04 - acc: 1.0000 - val_loss: 1.3559 - val_acc: 0.7647\n",
      "Epoch 17/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.4291e-04 - acc: 1.0000 - val_loss: 1.3869 - val_acc: 0.7647\n",
      "Epoch 18/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.5953e-04 - acc: 1.0000 - val_loss: 1.4141 - val_acc: 0.7647\n",
      "Epoch 19/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.9168e-04 - acc: 1.0000 - val_loss: 1.4433 - val_acc: 0.7647\n",
      "Epoch 20/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.3566e-04 - acc: 1.0000 - val_loss: 1.4684 - val_acc: 0.7647\n",
      "Epoch 21/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.9117e-04 - acc: 1.0000 - val_loss: 1.4941 - val_acc: 0.7647\n",
      "Epoch 22/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.5894e-04 - acc: 1.0000 - val_loss: 1.5143 - val_acc: 0.7647\n",
      "Epoch 23/115\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 1.3297e-04 - acc: 1.0000 - val_loss: 1.5261 - val_acc: 0.7647\n",
      "Epoch 24/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1326e-04 - acc: 1.0000 - val_loss: 1.5643 - val_acc: 0.7647\n",
      "Epoch 25/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 9.4585e-05 - acc: 1.0000 - val_loss: 1.5786 - val_acc: 0.7647\n",
      "Epoch 26/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 8.0807e-05 - acc: 1.0000 - val_loss: 1.5837 - val_acc: 0.7647\n",
      "Epoch 27/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.7214e-05 - acc: 1.0000 - val_loss: 1.6110 - val_acc: 0.7647\n",
      "Epoch 28/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 5.6928e-05 - acc: 1.0000 - val_loss: 1.6343 - val_acc: 0.7647\n",
      "Epoch 29/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4.8536e-05 - acc: 1.0000 - val_loss: 1.6564 - val_acc: 0.7647\n",
      "Epoch 30/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4.1561e-05 - acc: 1.0000 - val_loss: 1.6830 - val_acc: 0.7647\n",
      "Epoch 31/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3.5801e-05 - acc: 1.0000 - val_loss: 1.6856 - val_acc: 0.7647\n",
      "Epoch 32/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.0266e-05 - acc: 1.0000 - val_loss: 1.7046 - val_acc: 0.7647\n",
      "Epoch 33/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 2.5852e-05 - acc: 1.0000 - val_loss: 1.7306 - val_acc: 0.7647\n",
      "Epoch 34/115\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2004e-05 - acc: 1.0000 - val_loss: 1.7477 - val_acc: 0.7647\n",
      "Epoch 35/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.9068e-05 - acc: 1.0000 - val_loss: 1.7800 - val_acc: 0.7647\n",
      "Epoch 36/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.6423e-05 - acc: 1.0000 - val_loss: 1.7902 - val_acc: 0.7647\n",
      "Epoch 37/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.4133e-05 - acc: 1.0000 - val_loss: 1.8139 - val_acc: 0.7647\n",
      "Epoch 38/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.2139e-05 - acc: 1.0000 - val_loss: 1.8289 - val_acc: 0.7647\n",
      "Epoch 39/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.0526e-05 - acc: 1.0000 - val_loss: 1.8409 - val_acc: 0.7647\n",
      "Epoch 40/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 9.1986e-06 - acc: 1.0000 - val_loss: 1.8601 - val_acc: 0.7647\n",
      "Epoch 41/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 8.1170e-06 - acc: 1.0000 - val_loss: 1.8876 - val_acc: 0.7647\n",
      "Epoch 42/115\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 7.1142e-06 - acc: 1.0000 - val_loss: 1.9082 - val_acc: 0.7647\n",
      "Epoch 43/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 6.2272e-06 - acc: 1.0000 - val_loss: 1.9201 - val_acc: 0.7647\n",
      "Epoch 44/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.4181e-06 - acc: 1.0000 - val_loss: 1.9486 - val_acc: 0.7647\n",
      "Epoch 45/115\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.7331e-06 - acc: 1.0000 - val_loss: 1.9573 - val_acc: 0.7647\n",
      "Epoch 46/115\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 4.1919e-06 - acc: 1.0000 - val_loss: 1.9825 - val_acc: 0.7647\n",
      "Epoch 47/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.6782e-06 - acc: 1.0000 - val_loss: 1.9931 - val_acc: 0.7647\n",
      "Epoch 48/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.2367e-06 - acc: 1.0000 - val_loss: 2.0155 - val_acc: 0.7647\n",
      "Epoch 49/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.8579e-06 - acc: 1.0000 - val_loss: 2.0425 - val_acc: 0.7647\n",
      "Epoch 50/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.4821e-06 - acc: 1.0000 - val_loss: 2.0530 - val_acc: 0.7647\n",
      "Epoch 51/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.1791e-06 - acc: 1.0000 - val_loss: 2.0589 - val_acc: 0.7647\n",
      "Epoch 52/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.9096e-06 - acc: 1.0000 - val_loss: 2.0731 - val_acc: 0.7647\n",
      "Epoch 53/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.6846e-06 - acc: 1.0000 - val_loss: 2.0933 - val_acc: 0.7647\n",
      "Epoch 54/115\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.4843e-06 - acc: 1.0000 - val_loss: 2.0966 - val_acc: 0.7647\n",
      "Epoch 55/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.3065e-06 - acc: 1.0000 - val_loss: 2.1115 - val_acc: 0.7647\n",
      "Epoch 56/115\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.1562e-06 - acc: 1.0000 - val_loss: 2.1274 - val_acc: 0.7647\n",
      "Epoch 57/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0108e-06 - acc: 1.0000 - val_loss: 2.1321 - val_acc: 0.7647\n",
      "Epoch 58/115\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 8.9515e-07 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.7647\n",
      "Epoch 59/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 7.8420e-07 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.7647\n",
      "Epoch 60/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 7.0636e-07 - acc: 1.0000 - val_loss: 2.2113 - val_acc: 0.7647\n",
      "Epoch 61/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.1825e-07 - acc: 1.0000 - val_loss: 2.2152 - val_acc: 0.7647\n",
      "Epoch 62/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.5016e-07 - acc: 1.0000 - val_loss: 2.2357 - val_acc: 0.7647\n",
      "Epoch 63/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.9145e-07 - acc: 1.0000 - val_loss: 2.2501 - val_acc: 0.7647\n",
      "Epoch 64/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 4.3842e-07 - acc: 1.0000 - val_loss: 2.2545 - val_acc: 0.7647\n",
      "Epoch 65/115\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.9269e-07 - acc: 1.0000 - val_loss: 2.2677 - val_acc: 0.7647\n",
      "Epoch 66/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.5123e-07 - acc: 1.0000 - val_loss: 2.2821 - val_acc: 0.7647\n",
      "Epoch 67/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.1365e-07 - acc: 1.0000 - val_loss: 2.2962 - val_acc: 0.7647\n",
      "Epoch 68/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.7883e-07 - acc: 1.0000 - val_loss: 2.3075 - val_acc: 0.7647\n",
      "Epoch 69/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4870e-07 - acc: 1.0000 - val_loss: 2.3246 - val_acc: 0.7647\n",
      "Epoch 70/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.2195e-07 - acc: 1.0000 - val_loss: 2.3341 - val_acc: 0.7647\n",
      "Epoch 71/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.9996e-07 - acc: 1.0000 - val_loss: 2.3554 - val_acc: 0.7647\n",
      "Epoch 72/115\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.7799e-07 - acc: 1.0000 - val_loss: 2.3582 - val_acc: 0.7647\n",
      "Epoch 73/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.5826e-07 - acc: 1.0000 - val_loss: 2.3757 - val_acc: 0.7647\n",
      "Epoch 74/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.4239e-07 - acc: 1.0000 - val_loss: 2.3760 - val_acc: 0.7647\n",
      "Epoch 75/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.2727e-07 - acc: 1.0000 - val_loss: 2.3922 - val_acc: 0.7647\n",
      "Epoch 76/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1465e-07 - acc: 1.0000 - val_loss: 2.4082 - val_acc: 0.7647\n",
      "Epoch 77/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.0271e-07 - acc: 1.0000 - val_loss: 2.4169 - val_acc: 0.7647\n",
      "Epoch 78/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 9.2484e-08 - acc: 1.0000 - val_loss: 2.4180 - val_acc: 0.7647\n",
      "Epoch 79/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 8.3282e-08 - acc: 1.0000 - val_loss: 2.4315 - val_acc: 0.7647\n",
      "Epoch 80/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 7.6683e-08 - acc: 1.0000 - val_loss: 2.4285 - val_acc: 0.7647\n",
      "Epoch 81/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 6.8546e-08 - acc: 1.0000 - val_loss: 2.4436 - val_acc: 0.7647\n",
      "Epoch 82/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 6.2263e-08 - acc: 1.0000 - val_loss: 2.4655 - val_acc: 0.7647\n",
      "Epoch 83/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.6888e-08 - acc: 1.0000 - val_loss: 2.4752 - val_acc: 0.7647\n",
      "Epoch 84/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.1780e-08 - acc: 1.0000 - val_loss: 2.4880 - val_acc: 0.7647\n",
      "Epoch 85/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.7555e-08 - acc: 1.0000 - val_loss: 2.5038 - val_acc: 0.7647\n",
      "Epoch 86/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.3197e-08 - acc: 1.0000 - val_loss: 2.5106 - val_acc: 0.7647\n",
      "Epoch 87/115\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.9583e-08 - acc: 1.0000 - val_loss: 2.5310 - val_acc: 0.7647\n",
      "Epoch 88/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.6332e-08 - acc: 1.0000 - val_loss: 2.5395 - val_acc: 0.7647\n",
      "Epoch 89/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.3423e-08 - acc: 1.0000 - val_loss: 2.5531 - val_acc: 0.7647\n",
      "Epoch 90/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.1114e-08 - acc: 1.0000 - val_loss: 2.5679 - val_acc: 0.7647\n",
      "Epoch 91/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.8800e-08 - acc: 1.0000 - val_loss: 2.5782 - val_acc: 0.7647\n",
      "Epoch 92/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 2.6734e-08 - acc: 1.0000 - val_loss: 2.5937 - val_acc: 0.7647\n",
      "Epoch 93/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.4783e-08 - acc: 1.0000 - val_loss: 2.6115 - val_acc: 0.7647\n",
      "Epoch 94/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.3187e-08 - acc: 1.0000 - val_loss: 2.6116 - val_acc: 0.7647\n",
      "Epoch 95/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.1303e-08 - acc: 1.0000 - val_loss: 2.6172 - val_acc: 0.7647\n",
      "Epoch 96/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.9404e-08 - acc: 1.0000 - val_loss: 2.6315 - val_acc: 0.7647\n",
      "Epoch 97/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.8039e-08 - acc: 1.0000 - val_loss: 2.6340 - val_acc: 0.7647\n",
      "Epoch 98/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.6567e-08 - acc: 1.0000 - val_loss: 2.6452 - val_acc: 0.7647\n",
      "Epoch 99/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.5325e-08 - acc: 1.0000 - val_loss: 2.6609 - val_acc: 0.7647\n",
      "Epoch 100/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.4633e-08 - acc: 1.0000 - val_loss: 2.6700 - val_acc: 0.7647\n",
      "Epoch 101/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.3697e-08 - acc: 1.0000 - val_loss: 2.6838 - val_acc: 0.7647\n",
      "Epoch 102/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.2929e-08 - acc: 1.0000 - val_loss: 2.7013 - val_acc: 0.7647\n",
      "Epoch 103/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.2200e-08 - acc: 1.0000 - val_loss: 2.7219 - val_acc: 0.7647\n",
      "Epoch 104/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1788e-08 - acc: 1.0000 - val_loss: 2.7434 - val_acc: 0.7647\n",
      "Epoch 105/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1497e-08 - acc: 1.0000 - val_loss: 2.7645 - val_acc: 0.7647\n",
      "Epoch 106/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1241e-08 - acc: 1.0000 - val_loss: 2.7816 - val_acc: 0.7647\n",
      "Epoch 107/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.1009e-08 - acc: 1.0000 - val_loss: 2.7917 - val_acc: 0.7647\n",
      "Epoch 108/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0636e-08 - acc: 1.0000 - val_loss: 2.7956 - val_acc: 0.7647\n",
      "Epoch 109/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 9.9512e-09 - acc: 1.0000 - val_loss: 2.8077 - val_acc: 0.7647\n",
      "Epoch 110/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 9.8032e-09 - acc: 1.0000 - val_loss: 2.8086 - val_acc: 0.7647\n",
      "Epoch 111/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 9.1476e-09 - acc: 1.0000 - val_loss: 2.8180 - val_acc: 0.7647\n",
      "Epoch 112/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 8.7218e-09 - acc: 1.0000 - val_loss: 2.8324 - val_acc: 0.7647\n",
      "Epoch 113/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 8.5252e-09 - acc: 1.0000 - val_loss: 2.8498 - val_acc: 0.7647\n",
      "Epoch 114/115\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 8.3955e-09 - acc: 1.0000 - val_loss: 2.8578 - val_acc: 0.7647\n",
      "Epoch 115/115\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 8.2259e-09 - acc: 1.0000 - val_loss: 2.8695 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.1371 - acc: 0.7778\n",
      "Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "no_pre_train2 = models.Sequential()\n",
    "no_pre_train2.add(layers.Dense(300, activation=\"relu\", input_shape=(410,)))\n",
    "no_pre_train2.add(layers.Dense(250, activation=\"relu\"))\n",
    "no_pre_train2.add(layers.Dense(200, activation=\"relu\"))\n",
    "no_pre_train2.add(layers.Dense(150, activation=\"relu\"))\n",
    "no_pre_train2.add(layers.Dense(100, activation=\"relu\"))\n",
    "no_pre_train2.add(layers.Dense(50, activation=\"relu\"))\n",
    "no_pre_train2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "no_pre_train2.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "no_pre_train2.fit(X_train, y_train, epochs=115, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_no_pre_train2 = no_pre_train2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_no_pre_train2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b396d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_no_pre_train2 = no_pre_train2.predict(test_kaggle)\n",
    "y_pred_no_pre_train2 = np.around(y_pred_no_pre_train2, decimals=0).ravel()\n",
    "\n",
    "create_submission(y_pred_no_pre_train2, \"NN_no_pre_train2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "302fcfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_acc = [accuracy_no_pre_train2, accuracy_encoder_clasificacion2]\n",
    "labels = [\"Red sin pre-entrenar\", \"Red pre-entrenanda\\n con autoencoder\"]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "barlist = plt.bar(labels, results_acc, width=0.25, align=\"center\")\n",
    "barlist[0].set_color(\"r\")\n",
    "plt.xlim(-0.45, len(labels) -1 + 0.45)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
