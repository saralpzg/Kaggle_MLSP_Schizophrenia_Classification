{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f259b",
   "metadata": {},
   "source": [
    "Este notebook recoge los resultados de una serie de métodos para tratar de mejorar el entrenamiento de una red neuronal o poder dar uso al conjunto de datos no etiquetado.\n",
    "\n",
    "Las pruebas se van a realizar sobre la configuración de red óptima encontrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40286d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de optimización de hiperparámetros\n",
    "import optuna\n",
    "\n",
    "# Modelo\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing, regularizers\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4b44",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3014e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231ee89",
   "metadata": {},
   "source": [
    "###  Modelo óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516cc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# En primer lugar, hay que adaptar los datos\n",
    "NUM_CLASSES = 2\n",
    "y_train_softmax = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_softmax = np_utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e4bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 149ms/step - loss: 0.6864 - acc: 0.5686 - val_loss: 0.6845 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5365 - acc: 0.7451 - val_loss: 0.7852 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4486 - acc: 0.7647 - val_loss: 0.7125 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2293 - acc: 0.9804 - val_loss: 0.7357 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0894 - acc: 1.0000 - val_loss: 0.9013 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.9585 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9094 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5813 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_opt = models.Sequential()\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_opt.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_opt.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_original = model_opt.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_original * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebafe",
   "metadata": {},
   "source": [
    "# Función de pérdida personalizada\n",
    "\n",
    "Se trata de tener en cuenta los datos con peores resultados en el backpropagation, es decir, asignar un mayor peso a esos datos, definiendo propiamente una función de loss personalizada.\n",
    "\n",
    "Fuente: https://stackoverflow.com/questions/48720197/weight-samples-if-incorrect-guessed-in-binary-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad055d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, tp_weight=0.5, tn_weight=0.5, fp_weight=1, fn_weight=1):\n",
    "    '''\n",
    "    Función de pérdida personalizada para el optimizador de una red neuronal.\n",
    "    El método recibe las predicciones y el valor real de la clasificación, así como los pesos que se desea asignar a los\n",
    "    clasificaciones tanto erróneas como acertadas.\n",
    "    '''\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_classes = tf.keras.backend.greater_equal(y_pred, 0.5)\n",
    "    y_pred_classes_float = tf.keras.backend.cast(y_pred_classes, tf.keras.backend.floatx())\n",
    "    y_true_float = tf.keras.backend.cast(y_true, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get misclassified examples\n",
    "    wrongly_classified = tf.keras.backend.not_equal(y_true_float, y_pred_classes_float)\n",
    "    wrongly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "    wrongly_classified_float2 = tf.gather(wrongly_classified_float, [0], axis=1)\n",
    "        \n",
    "    # Get correctly classified examples\n",
    "    correctly_classified = tf.keras.backend.equal(y_true_float, y_pred_classes_float)\n",
    "    correctly_classified_float = tf.keras.backend.cast(correctly_classified, tf.keras.backend.floatx())\n",
    "    correctly_classified_float2 = tf.gather(correctly_classified_float, [0], axis=1)\n",
    "    \n",
    "    # Get tp, fp, tn, fn\n",
    "    tp = correctly_classified_float * y_true_float\n",
    "    tn = correctly_classified_float * (1 - y_true_float)\n",
    "    fp = wrongly_classified_float * y_true_float\n",
    "    fn = wrongly_classified_float * (1 - y_true_float)\n",
    "\n",
    "    # Get weights\n",
    "    weight = tp_weight * tp + fp_weight * fp + tn_weight * tn + fn_weight * fn\n",
    "    weight2 = tf.gather(weight, [0], axis=1)\n",
    "    weight3 = tf.math.reduce_sum(weight2, axis=1)\n",
    "    \n",
    "    loss = tf.keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weight3\n",
    "        \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73f781",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que los anteriores ``tp``, ``fn``, ... son tensores (vectores de ``tensorflow``) y por tanto, el resultado guardado en ``weight`` es igualmente otro tensor, no un único valor numérico.\n",
    "\n",
    "_NOTA: se entienden los positivos como aquellas muestras correspondientes con una etiqueta \"1\" (enfermos) y por tanto como negativos los registros con etiqueta \"0\" (grupo de control)._\n",
    "\n",
    "Vamos a probar cómo funciona la función personalizada sobre los modelos de redes neuronales que peores resultados han obtenido para comprobar si de este modo podemos mejorar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb9f5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 0.5316 - acc: 0.5686 - val_loss: 0.4673 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3406 - acc: 0.9020 - val_loss: 0.5205 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2365 - acc: 0.9804 - val_loss: 0.5716 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1206 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3440 - acc: 0.7843 - val_loss: 1.0695 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1280 - acc: 0.9412 - val_loss: 0.7024 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.7442 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.7961 - val_acc: 0.5882\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.8185 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4409 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_CL = models.Sequential()\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_CL.compile(loss=custom_loss, optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_CL.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_CL = model_CL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracyCL * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a69621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_CL = model_CL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_CL, \"NN_CustomLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16fe8",
   "metadata": {},
   "source": [
    "# Sample-weight\n",
    "\n",
    "Una alternativa a lo anterior, sería usar un vector de pesos propio y pasarlo a la función ``fit`` del modelo, a través del parámetro ``sample_weight``, en lugar de definir una función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "065bb881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 170ms/step - loss: 0.3475 - acc: 0.5686 - val_loss: 0.3414 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2470 - acc: 0.5098 - val_loss: 0.2745 - val_acc: 0.4118\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3152 - acc: 0.6471 - val_loss: 0.2306 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2156 - acc: 0.7059 - val_loss: 0.2567 - val_acc: 0.5294\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1106 - acc: 0.9608 - val_loss: 0.3408 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.3547 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.3733 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.3857 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3920 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3987 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4053 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4121 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4256 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 9.4525e-04 - acc: 1.0000 - val_loss: 0.4327 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 7.3577e-04 - acc: 1.0000 - val_loss: 0.4398 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 5.7974e-04 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 4.6185e-04 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.7296e-04 - acc: 1.0000 - val_loss: 0.4581 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.0341e-04 - acc: 1.0000 - val_loss: 0.4635 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 2.4874e-04 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.0503e-04 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.6953e-04 - acc: 1.0000 - val_loss: 0.4818 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.4058e-04 - acc: 1.0000 - val_loss: 0.4879 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.1711e-04 - acc: 1.0000 - val_loss: 0.4939 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.7745e-05 - acc: 1.0000 - val_loss: 0.4996 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 8.1903e-05 - acc: 1.0000 - val_loss: 0.5055 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 6.8878e-05 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 5.8192e-05 - acc: 1.0000 - val_loss: 0.5162 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.9310e-05 - acc: 1.0000 - val_loss: 0.5215 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.1931e-05 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.5722e-05 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.0528e-05 - acc: 1.0000 - val_loss: 0.5370 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.6139e-05 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.2429e-05 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.9267e-05 - acc: 1.0000 - val_loss: 0.5518 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.6596e-05 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.4319e-05 - acc: 1.0000 - val_loss: 0.5605 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.2385e-05 - acc: 1.0000 - val_loss: 0.5645 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.0727e-05 - acc: 1.0000 - val_loss: 0.5684 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.3051e-06 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 8.0796e-06 - acc: 1.0000 - val_loss: 0.5762 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 7.0269e-06 - acc: 1.0000 - val_loss: 0.5802 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 6.1198e-06 - acc: 1.0000 - val_loss: 0.5840 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 5.3400e-06 - acc: 1.0000 - val_loss: 0.5880 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.6716e-06 - acc: 1.0000 - val_loss: 0.5918 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.0940e-06 - acc: 1.0000 - val_loss: 0.5958 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 3.5943e-06 - acc: 1.0000 - val_loss: 0.5993 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.1564e-06 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.7761e-06 - acc: 1.0000 - val_loss: 0.6066 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 2.4437e-06 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.1534e-06 - acc: 1.0000 - val_loss: 0.6142 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.8983e-06 - acc: 1.0000 - val_loss: 0.6177 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.6762e-06 - acc: 1.0000 - val_loss: 0.6213 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.4832e-06 - acc: 1.0000 - val_loss: 0.6243 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.3140e-06 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.1662e-06 - acc: 1.0000 - val_loss: 0.6308 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0358e-06 - acc: 1.0000 - val_loss: 0.6341 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.2125e-07 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 8.2014e-07 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 7.3192e-07 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 6.5402e-07 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 5.8538e-07 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 5.2465e-07 - acc: 1.0000 - val_loss: 0.6509 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 4.7095e-07 - acc: 1.0000 - val_loss: 0.6534 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 4.2346e-07 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 72ms/step - loss: 3.8142e-07 - acc: 1.0000 - val_loss: 0.6581 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.4407e-07 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.1094e-07 - acc: 1.0000 - val_loss: 0.6625 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.8134e-07 - acc: 1.0000 - val_loss: 0.6647 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.5497e-07 - acc: 1.0000 - val_loss: 0.6667 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.3148e-07 - acc: 1.0000 - val_loss: 0.6686 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.1037e-07 - acc: 1.0000 - val_loss: 0.6706 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.9153e-07 - acc: 1.0000 - val_loss: 0.6725 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.7461e-07 - acc: 1.0000 - val_loss: 0.6743 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5947e-07 - acc: 1.0000 - val_loss: 0.6761 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.4580e-07 - acc: 1.0000 - val_loss: 0.6780 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.3357e-07 - acc: 1.0000 - val_loss: 0.6797 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.2249e-07 - acc: 1.0000 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.1253e-07 - acc: 1.0000 - val_loss: 0.6833 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.0349e-07 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.5368e-08 - acc: 1.0000 - val_loss: 0.6867 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 8.7987e-08 - acc: 1.0000 - val_loss: 0.6886 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 8.1333e-08 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 7.5265e-08 - acc: 1.0000 - val_loss: 0.6919 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 6.9781e-08 - acc: 1.0000 - val_loss: 0.6934 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.4738e-08 - acc: 1.0000 - val_loss: 0.6950 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.0196e-08 - acc: 1.0000 - val_loss: 0.6966 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 5.6040e-08 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 5.2261e-08 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 4.8824e-08 - acc: 1.0000 - val_loss: 0.7013 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.5657e-08 - acc: 1.0000 - val_loss: 0.7028 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 4.2732e-08 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.0080e-08 - acc: 1.0000 - val_loss: 0.7058 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.7669e-08 - acc: 1.0000 - val_loss: 0.7073 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5415e-08 - acc: 1.0000 - val_loss: 0.7087 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.3342e-08 - acc: 1.0000 - val_loss: 0.7102 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.1413e-08 - acc: 1.0000 - val_loss: 0.7116 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.9671e-08 - acc: 1.0000 - val_loss: 0.7130 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.5105 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_SW = models.Sequential()\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_SW.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "args = y_train_softmax.astype(bool)\n",
    "y_pred = model_SW.predict(X_train)\n",
    "samples = 1 - y_pred[args]\n",
    "for epoch in range(100):\n",
    "    model_SW.fit(X_train, y_train_softmax, epochs=1, validation_split=0.25, sample_weight=samples)\n",
    "    y_pred = model_SW.predict(X_train)\n",
    "    samples = 1 - y_pred[args]\n",
    "    \n",
    "# Precisión en partición de test\n",
    "loss, accuracy_SW = model_SW.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_SW * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0965d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_SW = model_SW.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_SW, \"NN_SampleWeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570984a",
   "metadata": {},
   "source": [
    "# Pseudo-labeling\n",
    "\n",
    "Finalmente, esta técnica va a permitir aprovechar el conjunto de datos no etiquetado (método de aprendizaje semi-automático).\n",
    "\n",
    "Fuente: https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04feba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_epoch(epoch, val, start, stop):\n",
    "    if epoch < start:\n",
    "        alpha = 0\n",
    "    elif epoch < stop:\n",
    "        alpha = ((epoch-start) / (stop-start)) * val\n",
    "    else:\n",
    "        alpha = val\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba7172a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step - loss: 1.3332 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.25\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4707 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5044 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.75\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5247 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 1.00\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha_values = [0.25, 0.5, 0.75, 1]\n",
    "iters = 100\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for i in range(iters):\n",
    "        pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "        alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "        samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "        model_PL.fit(X_train, y_train_softmax, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3114c",
   "metadata": {},
   "source": [
    "Vamos a pintar la evolución del valor del peso $\\alpha$ (función $\\alpha(t)$) elegido a lo largo de 100 iteraciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee4838bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "alpha_per_epoch = []\n",
    "for i in range(iters):\n",
    "    alpha_per_epoch.append(alpha_epoch(i+1, alpha, start, stop))\n",
    "    \n",
    "# Pintamos el vector\n",
    "plt.plot(alpha_per_epoch)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(r\"$\\alpha(t)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda9ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step - loss: 1.3332 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.50\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "alpha=0.5\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for i in range(iters):\n",
    "    pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "    alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "    samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "    model_PL.fit(X_train, y_train_softmax, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e13294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_PL = model_PL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_PL, \"NN_PseudoLabels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a0af6",
   "metadata": {},
   "source": [
    "# Comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e6fd758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_acc = [accuracy_original, accuracy_CL, accuracy_SW, accuracy_PL]\n",
    "xaxis = [\"Accuracy en red neuronal \\n óptima (original)\", \"Accuracy con función de\\n pérdida personalizada\", \n",
    "         \"Accuracy con sample-weight\", \"Accuracy con pseudo-labels\"]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "barlist = plt.bar(xaxis, results_acc, width=0.2)\n",
    "barlist[0].set_color(\"r\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(bottom=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
