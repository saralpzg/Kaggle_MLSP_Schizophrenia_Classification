{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter tunning libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ca4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"../data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"../data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on=\"Id\", right_on=\"Id\")\n",
    "data = pd.merge(left=train_labels, right=train, left_on=\"Id\", right_on=\"Id\")\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74697a",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0f9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aae0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"../data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"../data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test_kaggle = pd.merge(left=testFNC, right=testSBM, left_on=\"Id\", right_on=\"Id\")\n",
    "test_kaggle.drop(\"Id\", inplace=True, axis=1)\n",
    "test_kaggle.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1404/1404 [==============================] - 7s 4ms/step - loss: 0.0931 - val_loss: 0.0859\n",
      "Epoch 2/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.0840 - val_loss: 0.0849\n",
      "Epoch 3/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.0835 - val_loss: 0.0847\n",
      "Epoch 4/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0833 - val_loss: 0.0841\n",
      "Epoch 5/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0831 - val_loss: 0.0839\n",
      "Epoch 6/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0831 - val_loss: 0.0841\n",
      "Epoch 7/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0830 - val_loss: 0.0839\n",
      "Epoch 8/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0829 - val_loss: 0.0836\n",
      "Epoch 9/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0829 - val_loss: 0.0839\n",
      "Epoch 10/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0828 - val_loss: 0.0836\n",
      "Epoch 11/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0828 - val_loss: 0.0834\n",
      "Epoch 12/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0828 - val_loss: 0.0832\n",
      "Epoch 13/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0827 - val_loss: 0.0835\n",
      "Epoch 14/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0827 - val_loss: 0.0836\n",
      "Epoch 15/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0827 - val_loss: 0.0836\n",
      "Epoch 16/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0826 - val_loss: 0.0838\n",
      "Epoch 17/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0826 - val_loss: 0.0834\n",
      "Epoch 18/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0826 - val_loss: 0.0835\n",
      "Epoch 19/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0826 - val_loss: 0.0832\n",
      "Epoch 20/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0825 - val_loss: 0.0832\n",
      "Epoch 21/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0825 - val_loss: 0.0834\n",
      "Epoch 22/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0825 - val_loss: 0.0830\n",
      "Epoch 23/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0825 - val_loss: 0.0836\n",
      "Epoch 24/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0825 - val_loss: 0.0829\n",
      "Epoch 25/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.0825 - val_loss: 0.0830\n",
      "Epoch 26/100\n",
      "1404/1404 [==============================] - 5s 4ms/step - loss: 0.0825 - val_loss: 0.0831\n",
      "Epoch 27/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 28/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 29/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 30/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0833\n",
      "Epoch 31/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0833\n",
      "Epoch 32/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 33/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 34/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 35/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 36/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 37/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 38/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0828\n",
      "Epoch 39/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0832\n",
      "Epoch 40/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 41/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 42/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 43/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 44/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 45/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 46/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 47/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 48/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 49/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 50/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0832\n",
      "Epoch 51/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 52/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 53/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 54/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0832\n",
      "Epoch 55/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 56/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 57/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 58/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 59/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 60/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 61/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 62/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 63/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 64/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 65/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 66/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 67/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 68/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 69/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 70/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 71/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 72/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 73/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 74/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 75/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 76/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 77/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 78/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 79/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 81/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 82/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 83/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 84/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 85/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 86/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 87/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 88/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 89/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 90/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 91/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0830\n",
      "Epoch 92/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 93/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 94/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 95/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0825\n",
      "Epoch 96/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 97/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 98/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 99/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 100/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0822 - val_loss: 0.0827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b08265ba60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"linear\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"linear\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(410)(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder.fit(test_kaggle, test_kaggle, epochs=100, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c774fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como:\n",
    "\n",
    "* MSE = $\\frac{1}{n} \\sum_{i=1}^{n} (Y_{true}^{i} - Y_{pred}^{i})^{2}$\n",
    "* MAPE = $\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_{true}^{i} - Y_{pred}^{i}}{Y_{true}^{i}} \\right|$\n",
    "\n",
    "donde $Y_{true}$ es el valor real, $Y_{pred}$ el valor de la predicción y $n$ el número de predicciones.\n",
    "\n",
    "La anterior fórmula realiza el cálculo para dos \"listas de valores\", por ejemplo, podemos calcular el error MAPE por muestra o por feature. Para el valor del error de la predicción total, se debe promediar el error obtenido para todas las filas/columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a20e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(268.11365, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train (etiquetados)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "print(\"MAPE:\", mape(X_train, X_train_pred))\n",
    "\n",
    "\n",
    "# print(\"Mean squared error in X_train data prediction:\", mse(X_train, X_train_pred).numpy())\n",
    "# sum(abs((test.to_numpy()[:, 0] - test_pred[:, 0]) / test.to_numpy()[:, 0])) * (100 / len(test_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b675b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.1136737291194"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = X_train_pred.shape\n",
    "\n",
    "mape_list = np.empty((410))\n",
    "\n",
    "# El bucle calcula el MAPE por cada columna\n",
    "for i in range(dims[1]):\n",
    "    mape_list[i] = sum(abs((X_train.iloc[:, i] - X_train_pred[:, i]) / X_train.iloc[:, i]))\n",
    "    \n",
    "# 1/N ~ 1/(longitud de cada columna) = 1/(nº de filas)\n",
    "mape_list.mean() * (100 / dims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "348d0ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.1136737291194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobación: el valor debe ser el mismo por filas o por columnas (por samples o por variables)\n",
    "dims = X_train_pred.shape\n",
    "\n",
    "mape_list_row = np.empty((68))\n",
    "\n",
    "# El bucle calcula el MAPE por cada fila\n",
    "for i in range(dims[0]):\n",
    "    mape_list_row[i] = sum(abs((X_train.iloc[i, :] - X_train_pred[i, :]) / X_train.iloc[i, :]))\n",
    "    \n",
    "# 1/N ~ 1/(longitud de cada fila) = 1/(nº de columnas)\n",
    "mape_list_row.mean() * (100 / dims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(411.17978, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "\n",
    "test_kaggle_pred = autoencoder.predict(test_kaggle)\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9829f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(395.5444, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados) REDUCIDO A LAS 68 PRIMERAS SAMPLES\n",
    "\n",
    "test_kaggle_reduc = test_kaggle.iloc[:68, :]\n",
    "\n",
    "y_pred_kaggle_reduc = autoencoder.predict(test_kaggle_reduc)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle_reduc, y_pred_kaggle_reduc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0d348",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{EL ERROR ES MAYOR EN EL CJTO. DE KAGGLE, INCLUSO EL REDUCIDO}}$\n",
    "\n",
    "Al sumar menos registros (68 primeros de ``test_kaggle``), se suman cantidades errores comparables, aún así es mayor el error en los datos con los que se ha entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c38d1",
   "metadata": {},
   "source": [
    "Un error de MAPE superior al 100% indica un error muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4813a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_encoder = encoder # loading the previously saved model.\n",
    "\n",
    "new_encoder = models.Sequential()\n",
    "new_encoder.add(prev_encoder)\n",
    "new_encoder.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 18s 367ms/step - loss: 0.6492 - acc: 0.6667 - val_loss: 0.8262 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6411 - acc: 0.6667 - val_loss: 0.8238 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6352 - acc: 0.6667 - val_loss: 0.8217 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6305 - acc: 0.7059 - val_loss: 0.8198 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6268 - acc: 0.7059 - val_loss: 0.8183 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6231 - acc: 0.7059 - val_loss: 0.8168 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6198 - acc: 0.7059 - val_loss: 0.8151 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6167 - acc: 0.7059 - val_loss: 0.8133 - val_acc: 0.5882\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6133 - acc: 0.7059 - val_loss: 0.8115 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6106 - acc: 0.7255 - val_loss: 0.8102 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6077 - acc: 0.7255 - val_loss: 0.8091 - val_acc: 0.5882\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6048 - acc: 0.7255 - val_loss: 0.8080 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6024 - acc: 0.7255 - val_loss: 0.8071 - val_acc: 0.5882\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5995 - acc: 0.7255 - val_loss: 0.8055 - val_acc: 0.5882\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5970 - acc: 0.7255 - val_loss: 0.8038 - val_acc: 0.5882\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5943 - acc: 0.7255 - val_loss: 0.8026 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5921 - acc: 0.7255 - val_loss: 0.8016 - val_acc: 0.5882\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5894 - acc: 0.7255 - val_loss: 0.8001 - val_acc: 0.5882\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5870 - acc: 0.7255 - val_loss: 0.7990 - val_acc: 0.5882\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5848 - acc: 0.7255 - val_loss: 0.7974 - val_acc: 0.5882\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5822 - acc: 0.7255 - val_loss: 0.7959 - val_acc: 0.5882\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5798 - acc: 0.7451 - val_loss: 0.7951 - val_acc: 0.5882\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5777 - acc: 0.7451 - val_loss: 0.7937 - val_acc: 0.5882\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5750 - acc: 0.7451 - val_loss: 0.7925 - val_acc: 0.5882\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5729 - acc: 0.7451 - val_loss: 0.7916 - val_acc: 0.5882\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5704 - acc: 0.7451 - val_loss: 0.7906 - val_acc: 0.5882\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5679 - acc: 0.7451 - val_loss: 0.7894 - val_acc: 0.5882\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5652 - acc: 0.7451 - val_loss: 0.7883 - val_acc: 0.5882\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5627 - acc: 0.7451 - val_loss: 0.7870 - val_acc: 0.5882\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5604 - acc: 0.7451 - val_loss: 0.7862 - val_acc: 0.5882\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5583 - acc: 0.7451 - val_loss: 0.7850 - val_acc: 0.5882\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5560 - acc: 0.7451 - val_loss: 0.7842 - val_acc: 0.5882\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 255ms/step - loss: 0.5535 - acc: 0.7451 - val_loss: 0.7833 - val_acc: 0.5294\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5515 - acc: 0.7451 - val_loss: 0.7822 - val_acc: 0.5294\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5493 - acc: 0.7255 - val_loss: 0.7812 - val_acc: 0.5294\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5472 - acc: 0.7451 - val_loss: 0.7803 - val_acc: 0.5294\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5450 - acc: 0.7451 - val_loss: 0.7791 - val_acc: 0.5294\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5430 - acc: 0.7451 - val_loss: 0.7783 - val_acc: 0.5294\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5407 - acc: 0.7255 - val_loss: 0.7774 - val_acc: 0.4706\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5383 - acc: 0.7255 - val_loss: 0.7765 - val_acc: 0.4706\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5363 - acc: 0.7255 - val_loss: 0.7758 - val_acc: 0.4706\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5343 - acc: 0.7255 - val_loss: 0.7749 - val_acc: 0.4706\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5323 - acc: 0.7255 - val_loss: 0.7739 - val_acc: 0.4706\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5303 - acc: 0.7451 - val_loss: 0.7737 - val_acc: 0.4706\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5284 - acc: 0.7451 - val_loss: 0.7730 - val_acc: 0.4706\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5261 - acc: 0.7451 - val_loss: 0.7719 - val_acc: 0.4706\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5241 - acc: 0.7647 - val_loss: 0.7708 - val_acc: 0.4706\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5219 - acc: 0.7647 - val_loss: 0.7697 - val_acc: 0.4706\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5200 - acc: 0.7647 - val_loss: 0.7683 - val_acc: 0.4706\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5182 - acc: 0.7647 - val_loss: 0.7675 - val_acc: 0.4706\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5163 - acc: 0.7647 - val_loss: 0.7660 - val_acc: 0.4706\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5144 - acc: 0.7647 - val_loss: 0.7654 - val_acc: 0.4706\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5124 - acc: 0.7647 - val_loss: 0.7649 - val_acc: 0.4706\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5105 - acc: 0.7647 - val_loss: 0.7637 - val_acc: 0.4706\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5089 - acc: 0.7647 - val_loss: 0.7635 - val_acc: 0.4706\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5068 - acc: 0.7647 - val_loss: 0.7622 - val_acc: 0.4706\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5049 - acc: 0.7647 - val_loss: 0.7613 - val_acc: 0.4706\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5030 - acc: 0.7647 - val_loss: 0.7604 - val_acc: 0.4706\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5011 - acc: 0.7647 - val_loss: 0.7591 - val_acc: 0.4706\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4996 - acc: 0.7647 - val_loss: 0.7580 - val_acc: 0.4706\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4975 - acc: 0.8039 - val_loss: 0.7573 - val_acc: 0.4118\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4956 - acc: 0.8039 - val_loss: 0.7564 - val_acc: 0.4118\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4940 - acc: 0.8039 - val_loss: 0.7556 - val_acc: 0.4118\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4921 - acc: 0.8039 - val_loss: 0.7548 - val_acc: 0.4118\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4905 - acc: 0.8235 - val_loss: 0.7544 - val_acc: 0.4118\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4889 - acc: 0.8235 - val_loss: 0.7541 - val_acc: 0.4118\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4873 - acc: 0.8235 - val_loss: 0.7529 - val_acc: 0.4118\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4854 - acc: 0.8235 - val_loss: 0.7519 - val_acc: 0.4118\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4837 - acc: 0.8235 - val_loss: 0.7513 - val_acc: 0.4118\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4822 - acc: 0.8235 - val_loss: 0.7497 - val_acc: 0.4118\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4807 - acc: 0.8235 - val_loss: 0.7493 - val_acc: 0.4118\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4790 - acc: 0.8235 - val_loss: 0.7484 - val_acc: 0.4118\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4775 - acc: 0.8235 - val_loss: 0.7473 - val_acc: 0.4118\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4757 - acc: 0.8235 - val_loss: 0.7471 - val_acc: 0.4118\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4742 - acc: 0.8235 - val_loss: 0.7455 - val_acc: 0.4118\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4726 - acc: 0.8235 - val_loss: 0.7446 - val_acc: 0.4118\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4709 - acc: 0.8235 - val_loss: 0.7443 - val_acc: 0.4118\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4693 - acc: 0.8235 - val_loss: 0.7436 - val_acc: 0.4118\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4680 - acc: 0.8235 - val_loss: 0.7434 - val_acc: 0.4118\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4667 - acc: 0.8235 - val_loss: 0.7430 - val_acc: 0.4118\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4648 - acc: 0.8235 - val_loss: 0.7428 - val_acc: 0.4118\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4634 - acc: 0.8235 - val_loss: 0.7420 - val_acc: 0.4118\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4621 - acc: 0.8235 - val_loss: 0.7415 - val_acc: 0.4118\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4608 - acc: 0.8235 - val_loss: 0.7415 - val_acc: 0.4118\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4589 - acc: 0.8235 - val_loss: 0.7413 - val_acc: 0.4118\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4577 - acc: 0.8235 - val_loss: 0.7410 - val_acc: 0.4118\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4564 - acc: 0.8235 - val_loss: 0.7402 - val_acc: 0.4118\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4549 - acc: 0.8235 - val_loss: 0.7399 - val_acc: 0.4118\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4536 - acc: 0.8235 - val_loss: 0.7391 - val_acc: 0.4118\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4522 - acc: 0.8235 - val_loss: 0.7387 - val_acc: 0.4118\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4507 - acc: 0.8235 - val_loss: 0.7384 - val_acc: 0.4118\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4493 - acc: 0.8235 - val_loss: 0.7378 - val_acc: 0.4118\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4484 - acc: 0.8235 - val_loss: 0.7374 - val_acc: 0.4118\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4465 - acc: 0.8235 - val_loss: 0.7371 - val_acc: 0.4118\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4452 - acc: 0.8235 - val_loss: 0.7365 - val_acc: 0.4118\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4439 - acc: 0.8235 - val_loss: 0.7353 - val_acc: 0.4118\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4426 - acc: 0.8235 - val_loss: 0.7345 - val_acc: 0.4118\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4414 - acc: 0.8235 - val_loss: 0.7340 - val_acc: 0.4118\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4400 - acc: 0.8235 - val_loss: 0.7331 - val_acc: 0.4118\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4386 - acc: 0.8235 - val_loss: 0.7323 - val_acc: 0.4118\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5242 - acc: 0.7222\n",
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1913fc",
   "metadata": {},
   "source": [
    "Tras unas pocas iteraciones, descongelamos todas las capas y hacemos unas pocas épocas más entrenando y actualizando los pesos para el modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f79e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.4514 - acc: 0.8235 - val_loss: 0.7365 - val_acc: 0.5294\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3222 - acc: 0.9020 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1790 - acc: 0.9608 - val_loss: 0.7514 - val_acc: 0.6471\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0955 - acc: 0.9804 - val_loss: 0.8395 - val_acc: 0.6471\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0626 - acc: 0.9804 - val_loss: 1.0120 - val_acc: 0.8235\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0582 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.6471\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.7721 - val_acc: 0.6471\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.9838 - val_acc: 0.7059\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.9407 - val_acc: 0.6471\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9743 - val_acc: 0.6471\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.0043 - val_acc: 0.6471\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.6471\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.0612 - val_acc: 0.6471\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.6471\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0967 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7808 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1258",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, test_id=testFNC[\"Id\"]):\n",
    "    '''\n",
    "    Función para generar un csv con las predicciones de un modelo para participar en la competición de Kaggle\n",
    "    '''\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape) # Comprobación del tamaño, debe ser: (119748, 2)\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    parent_path = current_path.parent\n",
    "    submissionDF.to_csv(f\"{parent_path}\\submissions\\MLSP_submission_AE_{current_time}.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
