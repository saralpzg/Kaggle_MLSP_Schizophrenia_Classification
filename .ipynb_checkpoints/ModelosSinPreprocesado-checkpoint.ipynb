{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on='Id', right_on='Id')\n",
    "data = pd.merge(left=train_labels, right=train, left_on='Id', right_on='Id')\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test = pd.merge(left=testFNC, right=testSBM, left_on='Id', right_on='Id')\n",
    "test.drop(\"Id\", inplace=True, axis=1)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 750}\n",
      "Modelo óptimo: RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=750,\n",
      "                       random_state=0)\n",
      "Accuracy: 83.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_RF = RandomForestClassifier(random_state=0)\n",
    "param_grid_RF = {\n",
    "    \"n_estimators\": [100, 250, 500, 750, 1000],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [5, 10, 15, 20, None]\n",
    "}\n",
    "grid_search_RF = GridSearchCV(estimator=model_RF, param_grid=param_grid_RF, cv=4)\n",
    "# cv = 4 porque así: el conjunto de validation tiene un 0.25 del tamaño de train y: 0.25 * 0.8 = 0.2\n",
    "#                    el conjunto de train tiene un 0.75 del tamaño de train y: 0.75 * 0.8 = 0.6\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "print(\"Parámetros óptimos:\", grid_search_RF.best_params_)\n",
    "print(\"Modelo óptimo:\", grid_search_RF.best_estimator_)\n",
    "model_RF_opt = grid_search_RF.best_estimator_\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_RF = model_RF_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_RF)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Predicción en test para kaggle\n",
    "y_pred_kaggle_RF = model_RF_opt.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'eval_metric': 'logloss', 'learning_rate': 0.5, 'max_depth': 10, 'n_estimators': 100, 'use_label_encoder': False}\n",
      "Modelo óptimo: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.5, max_delta_step=0,\n",
      "              max_depth=10, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "Accuracy: 77.78%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_XGB = XGBClassifier()\n",
    "param_grid_XGB = {\n",
    "    \"booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "    \"learning_rate\": [0.001, 0.05, 0.1, 0.5],\n",
    "    \"min_split_loss\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [5, 10, 15, 20, 0] # 0 = ninguna restricción\n",
    "}\n",
    "grid_search_XGB = GridSearchCV(estimator=model_XGB, param_grid=param_grid_XGB, cv=4)\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "print(\"Parámetros óptimos:\", grid_search_XGB.best_params_)\n",
    "print(\"Modelo óptimo:\", grid_search_XGB.best_estimator_)\n",
    "model_XGB_opt = grid_search_XGB.best_estimator_\n",
    "# model_XGB.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_XGB = model_XGB_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_XGB)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Predicción en test para kaggle\n",
    "y_pred_kaggle_XGB = model_XGB_opt.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales\n",
    "\n",
    "### Fully-connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar como cross-validar nº de capas y nº de neuronas.\n",
    "\n",
    "Usar función MLPClassifier (sklearn), debería salir por lo menos parecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 408ms/step - loss: 0.6808 - acc: 0.4902 - val_loss: 0.6432 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5095 - acc: 0.9216 - val_loss: 0.6690 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4107 - acc: 0.8235 - val_loss: 0.6035 - val_acc: 0.7647\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2556 - acc: 1.0000 - val_loss: 0.5873 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1555 - acc: 1.0000 - val_loss: 0.7041 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0969 - acc: 1.0000 - val_loss: 0.6286 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0552 - acc: 1.0000 - val_loss: 0.6955 - val_acc: 0.7647\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.7111 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.8330 - val_acc: 0.8235\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.8062 - val_acc: 0.7647\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.7911 - val_acc: 0.7059\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.8540 - val_acc: 0.7059\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.8782 - val_acc: 0.7059\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.9156 - val_acc: 0.7059\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9013 - val_acc: 0.7059\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.9892 - val_acc: 0.7647\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9743 - val_acc: 0.7059\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.0152 - val_acc: 0.7647\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9874 - val_acc: 0.7059\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.7059\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.7059\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.4807e-04 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.7059\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.9421e-04 - acc: 1.0000 - val_loss: 1.1160 - val_acc: 0.7647\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.6717e-04 - acc: 1.0000 - val_loss: 1.1215 - val_acc: 0.7059\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.5871e-04 - acc: 1.0000 - val_loss: 1.1255 - val_acc: 0.7059\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.7759e-04 - acc: 1.0000 - val_loss: 1.1529 - val_acc: 0.7059\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.0657e-04 - acc: 1.0000 - val_loss: 1.1800 - val_acc: 0.7059\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.4837e-04 - acc: 1.0000 - val_loss: 1.1935 - val_acc: 0.7059\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.9831e-04 - acc: 1.0000 - val_loss: 1.2107 - val_acc: 0.7059\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.5839e-04 - acc: 1.0000 - val_loss: 1.2382 - val_acc: 0.7059\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.2389e-04 - acc: 1.0000 - val_loss: 1.2586 - val_acc: 0.7059\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8998e-04 - acc: 1.0000 - val_loss: 1.2659 - val_acc: 0.7059\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6275e-04 - acc: 1.0000 - val_loss: 1.2861 - val_acc: 0.7059\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.4137e-04 - acc: 1.0000 - val_loss: 1.2904 - val_acc: 0.7059\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.2258e-04 - acc: 1.0000 - val_loss: 1.3178 - val_acc: 0.7059\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0763e-04 - acc: 1.0000 - val_loss: 1.3047 - val_acc: 0.7059\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.3316e-05 - acc: 1.0000 - val_loss: 1.3492 - val_acc: 0.7059\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.1002e-05 - acc: 1.0000 - val_loss: 1.3570 - val_acc: 0.7059\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.1785e-05 - acc: 1.0000 - val_loss: 1.3602 - val_acc: 0.7059\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.2085e-05 - acc: 1.0000 - val_loss: 1.3871 - val_acc: 0.7059\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.4237e-05 - acc: 1.0000 - val_loss: 1.4212 - val_acc: 0.7059\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.7422e-05 - acc: 1.0000 - val_loss: 1.4310 - val_acc: 0.7059\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.0827e-05 - acc: 1.0000 - val_loss: 1.4398 - val_acc: 0.7059\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.5966e-05 - acc: 1.0000 - val_loss: 1.4379 - val_acc: 0.7059\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.1368e-05 - acc: 1.0000 - val_loss: 1.4780 - val_acc: 0.7059\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.7485e-05 - acc: 1.0000 - val_loss: 1.4778 - val_acc: 0.7059\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4101e-05 - acc: 1.0000 - val_loss: 1.5148 - val_acc: 0.7059\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.1036e-05 - acc: 1.0000 - val_loss: 1.5270 - val_acc: 0.7059\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.8252e-05 - acc: 1.0000 - val_loss: 1.5364 - val_acc: 0.7059\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.5954e-05 - acc: 1.0000 - val_loss: 1.5447 - val_acc: 0.7059\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.4324e-05 - acc: 1.0000 - val_loss: 1.5443 - val_acc: 0.7059\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2500e-05 - acc: 1.0000 - val_loss: 1.5791 - val_acc: 0.7059\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0842e-05 - acc: 1.0000 - val_loss: 1.5914 - val_acc: 0.7059\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 9.5534e-06 - acc: 1.0000 - val_loss: 1.5978 - val_acc: 0.7059\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.3967e-06 - acc: 1.0000 - val_loss: 1.6188 - val_acc: 0.7059\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.4448e-06 - acc: 1.0000 - val_loss: 1.6265 - val_acc: 0.7059\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.5455e-06 - acc: 1.0000 - val_loss: 1.6493 - val_acc: 0.7059\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.7847e-06 - acc: 1.0000 - val_loss: 1.6637 - val_acc: 0.7059\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.1279e-06 - acc: 1.0000 - val_loss: 1.6761 - val_acc: 0.7059\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.5155e-06 - acc: 1.0000 - val_loss: 1.6921 - val_acc: 0.7059\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.0051e-06 - acc: 1.0000 - val_loss: 1.6960 - val_acc: 0.7059\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.5458e-06 - acc: 1.0000 - val_loss: 1.7100 - val_acc: 0.7059\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.1323e-06 - acc: 1.0000 - val_loss: 1.7359 - val_acc: 0.7059\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.7722e-06 - acc: 1.0000 - val_loss: 1.7397 - val_acc: 0.7059\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.4820e-06 - acc: 1.0000 - val_loss: 1.7491 - val_acc: 0.7059\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.1645e-06 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 0.7059\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.9162e-06 - acc: 1.0000 - val_loss: 1.7893 - val_acc: 0.7059\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.7039e-06 - acc: 1.0000 - val_loss: 1.7908 - val_acc: 0.7059\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5099e-06 - acc: 1.0000 - val_loss: 1.8059 - val_acc: 0.7059\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.3428e-06 - acc: 1.0000 - val_loss: 1.8155 - val_acc: 0.7059\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2094e-06 - acc: 1.0000 - val_loss: 1.8173 - val_acc: 0.7059\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0776e-06 - acc: 1.0000 - val_loss: 1.8536 - val_acc: 0.7059\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.4684e-07 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 0.7059\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.4777e-07 - acc: 1.0000 - val_loss: 1.8634 - val_acc: 0.7059\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.5058e-07 - acc: 1.0000 - val_loss: 1.8877 - val_acc: 0.7059\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.6641e-07 - acc: 1.0000 - val_loss: 1.8966 - val_acc: 0.7059\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.9579e-07 - acc: 1.0000 - val_loss: 1.9157 - val_acc: 0.7059\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.3455e-07 - acc: 1.0000 - val_loss: 1.9195 - val_acc: 0.7059\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.8039e-07 - acc: 1.0000 - val_loss: 1.9473 - val_acc: 0.7059\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.2958e-07 - acc: 1.0000 - val_loss: 1.9519 - val_acc: 0.7059\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.8871e-07 - acc: 1.0000 - val_loss: 1.9610 - val_acc: 0.7059\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.4882e-07 - acc: 1.0000 - val_loss: 1.9766 - val_acc: 0.7059\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2001e-07 - acc: 1.0000 - val_loss: 1.9720 - val_acc: 0.7059\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8441e-07 - acc: 1.0000 - val_loss: 1.9906 - val_acc: 0.7059\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.5363e-07 - acc: 1.0000 - val_loss: 2.0087 - val_acc: 0.7059\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.2846e-07 - acc: 1.0000 - val_loss: 2.0250 - val_acc: 0.7059\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.0884e-07 - acc: 1.0000 - val_loss: 2.0248 - val_acc: 0.7059\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.8816e-07 - acc: 1.0000 - val_loss: 2.0495 - val_acc: 0.7059\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6994e-07 - acc: 1.0000 - val_loss: 2.0571 - val_acc: 0.7059\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.5482e-07 - acc: 1.0000 - val_loss: 2.0707 - val_acc: 0.7059\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.4165e-07 - acc: 1.0000 - val_loss: 2.0866 - val_acc: 0.7059\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2862e-07 - acc: 1.0000 - val_loss: 2.0868 - val_acc: 0.7059\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1699e-07 - acc: 1.0000 - val_loss: 2.0980 - val_acc: 0.7059\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0678e-07 - acc: 1.0000 - val_loss: 2.1117 - val_acc: 0.7059\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 9.7989e-08 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.7059\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.9869e-08 - acc: 1.0000 - val_loss: 2.1369 - val_acc: 0.7059\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.3508e-08 - acc: 1.0000 - val_loss: 2.1524 - val_acc: 0.7059\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.5842e-08 - acc: 1.0000 - val_loss: 2.1546 - val_acc: 0.7059\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.9208e-08 - acc: 1.0000 - val_loss: 2.1544 - val_acc: 0.7059\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.3206e-08 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3248 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC1 = models.Sequential()\n",
    "modelFC1.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC1.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC1.add(layers.Dense(100, activation=\"relu\"))\n",
    "modelFC1.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC1.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "modelFC1.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "# Se utilizan 100 épocas ya que como se irá viendo, incluso con estas épocas la red sobreajusta.\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC1.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# # Predicción en test para kaggle\n",
    "# y_pred_kaggle_FC1 = modelFC1.predict(test)\n",
    "# y_pred_kaggle_FC1[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras probar una configuración cualquiera en el bloque anterior, vemos que tenemos un gran problema de sobreajuste (100% en el conjunto de train y diferencia de un 30% aproximadamente con la precisión en el conjunto de validación). Vamos a introducir unos términos de regularización para evitar esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 176ms/step - loss: 2.0341 - acc: 0.5098 - val_loss: 1.9356 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.8778 - acc: 0.6275 - val_loss: 1.8620 - val_acc: 0.7647\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.8085 - acc: 0.6471 - val_loss: 1.8049 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.7317 - acc: 0.6667 - val_loss: 1.7663 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6955 - acc: 0.6863 - val_loss: 1.7382 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.5640 - acc: 0.8431 - val_loss: 1.7126 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.5509 - acc: 0.8235 - val_loss: 1.6727 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.5890 - acc: 0.6667\n",
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC2 = models.Sequential()\n",
    "modelFC2.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC2.add(layers.Dropout(0.5))\n",
    "modelFC2.add(layers.Dense(200, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "modelFC2.add(layers.Dropout(0.5))\n",
    "modelFC2.add(layers.Dense(100, activation=\"relu\"))\n",
    "modelFC2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC2.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "# Detenemos el entrenamiento si hay más de 5 épocas en las que no hay una ganancia en el conjunto de validación del 1%\n",
    "modelFC2.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, sobre la red anterior, buscaremos el ajuste de los parámetros.\n",
    "\n",
    "Al compilar el modelo, la función de pérdida (``binary_crossentropy``) es la adecuada porque estamos en un problema de clasificación binaria e igualmente para la métrica (``acc``), pero podemos probar varias opciones para el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Entrenando para el optimizador: adadelta -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.0219 - acc: 0.5556\n",
      "----- Entrenando para el optimizador: adagrad -----\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0501 - acc: 0.3889\n",
      "----- Entrenando para el optimizador: adam -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6028 - acc: 0.5556\n",
      "----- Entrenando para el optimizador: adamax -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.8739 - acc: 0.4444\n",
      "----- Entrenando para el optimizador: rmsprop -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4074 - acc: 0.7778\n",
      "----- Entrenando para el optimizador: sgd -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9864 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adadelta': 0.5555555820465088,\n",
       " 'adagrad': 0.3888888955116272,\n",
       " 'adam': 0.5555555820465088,\n",
       " 'adamax': 0.4444444477558136,\n",
       " 'rmsprop': 0.7777777910232544,\n",
       " 'sgd': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEST OPTIMIZER FUNCTION\n",
    "optimizers_range = [\"adadelta\", \"adagrad\", \"adam\", \"adamax\", \"rmsprop\", \"sgd\"]\n",
    "acc = {}\n",
    "\n",
    "for optimizer in optimizers_range:\n",
    "    print(f\"----- Entrenando para el optimizador: {optimizer} -----\")\n",
    "    # Definir y entrenar el modelo\n",
    "    modelFC2 = models.Sequential()\n",
    "    modelFC2.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "    modelFC2.add(layers.Dropout(0.5))\n",
    "    modelFC2.add(layers.Dense(200, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "    modelFC2.add(layers.Dropout(0.5))\n",
    "    modelFC2.add(layers.Dense(100, activation=\"relu\"))\n",
    "    modelFC2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    modelFC2.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "    es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "    modelFC2.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es], verbose=0)\n",
    "    # verbose=0 oculta el informe del proceso en cada época\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy = modelFC2.evaluate(X_test, y_test)\n",
    "    acc[optimizer] = accuracy\n",
    "    \n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vista de que el mayor accuracy se tiene para el optimizador ``RMSprop``, vamos a optimizar los parámetros del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Entrenando el optimizador RMSprop con un lr=0.0001 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0338 - acc: 0.3889\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.0001 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.9832 - acc: 0.3889\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.0001 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9626 - acc: 0.4444\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.001 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5417 - acc: 0.6111\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.001 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.5551 - acc: 0.7222\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.001 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4580 - acc: 0.7222\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.01 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7889 - acc: 0.7778\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.01 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9195 - acc: 0.7778\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.01 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9762 - acc: 0.7778\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.05 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4201 - acc: 0.7778\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.05 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3279 - acc: 0.4444\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.05 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.9976 - acc: 0.6667\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.1 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.5016 - acc: 0.5556\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.1 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.6634 - acc: 0.6111\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.1 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.5124 - acc: 0.5000\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.5 y un momentum=0 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3122.3616 - acc: 0.5556\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.5 y un momentum=0.001 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5618.5869 - acc: 0.7222\n",
      "----- Entrenando el optimizador RMSprop con un lr=0.5 y un momentum=0.1 -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3086.8550 - acc: 0.3333\n",
      "Valor óptimo de los hiperparámetros: lr=0.01; momentum=0\n",
      "Se logra un accuracy de 77.78%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# BEST RMSPROP HYPERPARAMETERS\n",
    "lr_range = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5]\n",
    "moment_range = [0, 0.001, 0.1]\n",
    "acc = 0\n",
    "\n",
    "for lr in lr_range:\n",
    "    for moment in moment_range:\n",
    "        print(f\"----- Entrenando el optimizador RMSprop con un lr={lr} y un momentum={moment} -----\")\n",
    "        # Definir y entrenar el modelo\n",
    "        modelFC2 = models.Sequential()\n",
    "        modelFC2.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "        modelFC2.add(layers.Dropout(0.5))\n",
    "        modelFC2.add(layers.Dense(200, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "        modelFC2.add(layers.Dropout(0.5))\n",
    "        modelFC2.add(layers.Dense(100, activation=\"relu\"))\n",
    "        modelFC2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr, momentum=moment)\n",
    "        modelFC2.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "        es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "        modelFC2.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es], verbose=0)\n",
    "\n",
    "        # Precisión en partición de test\n",
    "        loss, accuracy = modelFC2.evaluate(X_test, y_test)\n",
    "        if accuracy > acc:\n",
    "            best_lr = lr\n",
    "            best_momentum = moment\n",
    "            acc = accuracy\n",
    "    \n",
    "print(f\"Valor óptimo de los hiperparámetros: lr={best_lr}; momentum={best_momentum}\")\n",
    "print(\"Se logra un accuracy de {:0.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ajustar algunos parámetros de la función ``fit``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Entrenando el modelo con un tamaño de batch de 5         y un porcentaje de datos de validación de 15.0% -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7452 - acc: 0.6667\n",
      "----- Entrenando el modelo con un tamaño de batch de 5         y un porcentaje de datos de validación de 20.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1207 - acc: 0.6111\n",
      "----- Entrenando el modelo con un tamaño de batch de 5         y un porcentaje de datos de validación de 25.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8624 - acc: 0.8333\n",
      "----- Entrenando el modelo con un tamaño de batch de 5         y un porcentaje de datos de validación de 30.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6088 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 5         y un porcentaje de datos de validación de 40.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6625 - acc: 0.6111\n",
      "----- Entrenando el modelo con un tamaño de batch de 10         y un porcentaje de datos de validación de 15.0% -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9655 - acc: 0.7222\n",
      "----- Entrenando el modelo con un tamaño de batch de 10         y un porcentaje de datos de validación de 20.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8606 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 10         y un porcentaje de datos de validación de 25.0% -----\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1894 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 10         y un porcentaje de datos de validación de 30.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9865 - acc: 0.7222\n",
      "----- Entrenando el modelo con un tamaño de batch de 10         y un porcentaje de datos de validación de 40.0% -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6853 - acc: 0.6667\n",
      "----- Entrenando el modelo con un tamaño de batch de 15         y un porcentaje de datos de validación de 15.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9858 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 15         y un porcentaje de datos de validación de 20.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2781 - acc: 0.6667\n",
      "----- Entrenando el modelo con un tamaño de batch de 15         y un porcentaje de datos de validación de 25.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1886 - acc: 0.7222\n",
      "----- Entrenando el modelo con un tamaño de batch de 15         y un porcentaje de datos de validación de 30.0% -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1143 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 15         y un porcentaje de datos de validación de 40.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4090 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 20         y un porcentaje de datos de validación de 15.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7491 - acc: 0.8333\n",
      "----- Entrenando el modelo con un tamaño de batch de 20         y un porcentaje de datos de validación de 20.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9694 - acc: 0.6667\n",
      "----- Entrenando el modelo con un tamaño de batch de 20         y un porcentaje de datos de validación de 25.0% -----\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8543 - acc: 0.7778\n",
      "----- Entrenando el modelo con un tamaño de batch de 20         y un porcentaje de datos de validación de 30.0% -----\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9343 - acc: 0.7222\n",
      "----- Entrenando el modelo con un tamaño de batch de 20         y un porcentaje de datos de validación de 40.0% -----\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6672 - acc: 0.6111\n",
      "Valor óptimo de los hiperparámetros: batch_size=5; validation_split=0.25\n",
      "Se logra un accuracy de 83.33%\n"
     ]
    }
   ],
   "source": [
    "# BEST FIT FUNCTION HYPERPARAMETERS\n",
    "batch_size_range = [5, 10, 15, 20] # Al hacer la partición de test, nos quedan 68 entradas de datos\n",
    "val_split_range = [0.15, 0.2, 0.25, 0.3, 0.4]\n",
    "acc = 0\n",
    "\n",
    "for batch_size in batch_size_range:\n",
    "    for val_split in val_split_range:\n",
    "        print(f\"----- Entrenando el modelo con un tamaño de batch de {batch_size} \\\n",
    "        y un porcentaje de datos de validación de {val_split*100}% -----\")\n",
    "        # Definir y entrenar el modelo\n",
    "        modelFC2 = models.Sequential()\n",
    "        modelFC2.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "        modelFC2.add(layers.Dropout(0.5))\n",
    "        modelFC2.add(layers.Dense(200, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "        modelFC2.add(layers.Dropout(0.5))\n",
    "        modelFC2.add(layers.Dense(100, activation=\"relu\"))\n",
    "        modelFC2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr, momentum=best_momentum)\n",
    "        modelFC2.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "        es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "        modelFC2.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_split=val_split, callbacks=[es], verbose=0)\n",
    "\n",
    "        # Precisión en partición de test\n",
    "        loss, accuracy = modelFC2.evaluate(X_test, y_test)\n",
    "        if accuracy > acc:\n",
    "            best_batch_size = batch_size\n",
    "            best_val_split = val_split\n",
    "            acc = accuracy\n",
    "    \n",
    "print(f\"Valor óptimo de los hiperparámetros: batch_size={best_batch_size}; validation_split={best_val_split}\")\n",
    "print(\"Se logra un accuracy de {:0.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 18ms/step - loss: 1.8068 - acc: 0.4902 - val_loss: 1.1421 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0872 - acc: 0.4902 - val_loss: 0.8710 - val_acc: 0.7059\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7586 - acc: 0.7451 - val_loss: 1.0913 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3600 - acc: 0.9412 - val_loss: 5.2507 - val_acc: 0.5294\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2411 - acc: 0.8039 - val_loss: 0.8056 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6949 - acc: 0.8431 - val_loss: 0.9574 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6054 - acc: 0.8431 - val_loss: 1.0039 - val_acc: 0.7647\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3323 - acc: 0.9412 - val_loss: 2.9974 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6001 - acc: 0.8431 - val_loss: 1.2739 - val_acc: 0.7647\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3239 - acc: 0.9608 - val_loss: 1.2952 - val_acc: 0.7647\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1896 - acc: 0.9608 - val_loss: 0.9052 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0983 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC2 = models.Sequential()\n",
    "modelFC2.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC2.add(layers.Dropout(0.5))\n",
    "modelFC2.add(layers.Dense(200, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "modelFC2.add(layers.Dropout(0.5))\n",
    "modelFC2.add(layers.Dense(100, activation=\"relu\"))\n",
    "modelFC2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr, momentum=best_momentum)\n",
    "modelFC2.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "modelFC2.fit(X_train, y_train, epochs=100, batch_size=best_batch_size, validation_split=best_val_split, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sigue habiendo una gran diferencia entre la precisión en el conjunto de validación y en el de entrenamiento, vamos a tratar de reducirla cambiando la topología de la red.\n",
    "\n",
    "$\\color{red}{\\text{¿Está mal hecho que busque la mejor estructura de la red después del ajuste de los parámetros?}}$\n",
    "Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 30ms/step - loss: 12.0956 - acc: 0.4902 - val_loss: 4.5607 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.7107 - acc: 0.5294 - val_loss: 2.8128 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5648 - acc: 0.5490 - val_loss: 2.3293 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2777 - acc: 0.5294 - val_loss: 2.2114 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1971 - acc: 0.5490 - val_loss: 2.2162 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2517 - acc: 0.5490 - val_loss: 2.1974 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2413 - acc: 0.4444\n",
      "Accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC3 = models.Sequential()\n",
    "modelFC3.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC3.add(layers.Dropout(0.5))\n",
    "modelFC3.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1\"))\n",
    "modelFC3.add(layers.Dropout(0.5))\n",
    "modelFC3.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1\"))\n",
    "modelFC3.add(layers.Dropout(0.5))\n",
    "modelFC3.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1\"))\n",
    "modelFC3.add(layers.Dropout(0.5))\n",
    "modelFC3.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr, momentum=best_momentum)\n",
    "modelFC3.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "modelFC3.fit(X_train, y_train, epochs=100, batch_size=best_batch_size, validation_split=best_val_split, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC3.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 22ms/step - loss: 13.5708 - acc: 0.5098 - val_loss: 5.0148 - val_acc: 0.8235\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.0705 - acc: 0.5490 - val_loss: 3.1077 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8113 - acc: 0.6078 - val_loss: 2.5500 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.4462 - acc: 0.4902 - val_loss: 2.3053 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2729 - acc: 0.4510 - val_loss: 2.2208 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2082 - acc: 0.5294 - val_loss: 2.2032 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2075 - acc: 0.4444\n",
      "Accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC4 = models.Sequential()\n",
    "modelFC4.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC4.add(layers.Dropout(0.5))\n",
    "modelFC4.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1_l2\"))\n",
    "modelFC4.add(layers.Dropout(0.5))\n",
    "modelFC4.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1_l2\"))\n",
    "modelFC4.add(layers.Dropout(0.5))\n",
    "modelFC4.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l1_l2\"))\n",
    "modelFC4.add(layers.Dropout(0.5))\n",
    "modelFC4.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr, momentum=best_momentum)\n",
    "modelFC4.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "modelFC4.fit(X_train, y_train, epochs=100, batch_size=best_batch_size, validation_split=best_val_split, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC4.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 21ms/step - loss: 2.8688 - acc: 0.5098 - val_loss: 1.6835 - val_acc: 0.4118\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3549 - acc: 0.7059 - val_loss: 2.1897 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9615 - acc: 0.5490 - val_loss: 1.2474 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2266 - acc: 0.5882 - val_loss: 1.0481 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8608 - acc: 0.7647 - val_loss: 0.9755 - val_acc: 0.8235\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8653 - acc: 0.7647 - val_loss: 1.1740 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8634 - acc: 0.7255 - val_loss: 0.7729 - val_acc: 0.8235\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4374 - acc: 0.9216 - val_loss: 0.8469 - val_acc: 0.8235\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3717 - acc: 0.9216 - val_loss: 1.4813 - val_acc: 0.7647\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6584 - acc: 0.8235 - val_loss: 1.3721 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9456 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "modelFC5 = models.Sequential()\n",
    "modelFC5.add(layers.Dense(100, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC5.add(layers.Dropout(0.5))\n",
    "modelFC5.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "modelFC5.add(layers.Dropout(0.5))\n",
    "modelFC5.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "modelFC5.add(layers.Dropout(0.5))\n",
    "modelFC5.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "modelFC5.add(layers.Dropout(0.5))\n",
    "modelFC5.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr, momentum=best_momentum)\n",
    "modelFC5.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "modelFC5.fit(X_train, y_train, epochs=100, batch_size=best_batch_size, validation_split=best_val_split, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC5.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos probado diferentes topologías para la red hasta encontrar una que ha permitido obtener unos mejores resultados. La regularización que mejor funciona, tanto para no limitar excesivamente el entrenamiento de la red como para acortar las diferencias en el accuracy en train y validation, ha sido la regularización ``L2``. Tras esto, la diferencia en accuracy en las últimas etapas se ha reducido hasta llegar al 6%.\n",
    "\n",
    "$\\color{red}{\\text{Los resultados aún así oscilan bastante si se re-ejecuta el código.}}$\n",
    "$\\color{red}{\\text{El número de capas y de neuronas un poco a ciegas...}}$\n",
    "\n",
    "### CNN\n",
    "\n",
    "$\\color{red}{\\text{¿Tiene sentido usar CNNs?}}$\n",
    "\n",
    "Parece desaconsejado en clasificación binaria (datos numéricos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "Idea: el encoder puede ayudar a \"recrear\" etiquetas de los datos de test proporcionados en kaggle, del que no se dispone de la clasificación verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 22ms/step - loss: 3.1657 - val_loss: 2.6599\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3727 - val_loss: 2.0970\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8879 - val_loss: 1.7399\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5418 - val_loss: 1.4367\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2421 - val_loss: 1.1721\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9688 - val_loss: 0.9426\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6911 - val_loss: 0.7068\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3887 - val_loss: 0.4628\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.2104\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -0.3819 - val_loss: -0.0954\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -0.8302 - val_loss: -0.4102\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1.3896 - val_loss: -0.8423\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2.0860 - val_loss: -1.3052\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -2.8670 - val_loss: -1.7941\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -3.7221 - val_loss: -2.4079\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -4.7549 - val_loss: -3.0250\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: -5.8903 - val_loss: -3.7336\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -7.2902 - val_loss: -4.7497\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -8.8646 - val_loss: -5.8225\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -10.6714 - val_loss: -6.7548\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -12.6146 - val_loss: -8.5708\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -15.1245 - val_loss: -10.1534\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -17.8828 - val_loss: -11.7639\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -20.8270 - val_loss: -14.0773\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -24.1867 - val_loss: -16.6993\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -28.1417 - val_loss: -19.3334\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -32.2272 - val_loss: -22.2336\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -37.0489 - val_loss: -25.3769\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -42.4121 - val_loss: -29.0502\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -48.0773 - val_loss: -33.8964\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -54.4721 - val_loss: -38.4849\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -61.3606 - val_loss: -43.8882\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -69.0186 - val_loss: -49.2167\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -76.6427 - val_loss: -53.9284\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -85.4701 - val_loss: -61.1887\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -95.1500 - val_loss: -67.2925\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -104.6004 - val_loss: -71.8584\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -115.1255 - val_loss: -81.4222\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -126.6445 - val_loss: -89.8439\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -139.2264 - val_loss: -98.9871\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -153.1842 - val_loss: -109.1312\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -168.1530 - val_loss: -119.5797\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -183.5855 - val_loss: -130.1838\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -198.7359 - val_loss: -141.7104\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -214.7307 - val_loss: -151.3353\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -233.1146 - val_loss: -165.9074\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -253.5430 - val_loss: -180.1378\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -274.8517 - val_loss: -192.9592\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -295.7891 - val_loss: -208.6990\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -318.8365 - val_loss: -226.3385\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -343.5214 - val_loss: -244.0741\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -369.2435 - val_loss: -261.8312\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -394.8616 - val_loss: -278.0468\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -422.0153 - val_loss: -300.4911\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -454.1409 - val_loss: -322.5715\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -486.5106 - val_loss: -342.1908\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -519.0249 - val_loss: -367.0091\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -552.5768 - val_loss: -389.5369\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -589.3503 - val_loss: -416.1909\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -627.7996 - val_loss: -439.5958\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -667.2795 - val_loss: -472.2532\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -706.9515 - val_loss: -500.3789\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -751.3480 - val_loss: -529.2538\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -798.3332 - val_loss: -562.3885\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -847.4025 - val_loss: -600.7933\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -898.2846 - val_loss: -635.4885\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -947.1087 - val_loss: -669.0955\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1003.7301 - val_loss: -716.3845\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1063.7966 - val_loss: -754.1011\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1123.1179 - val_loss: -791.9887\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1184.0597 - val_loss: -834.0953\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1248.0219 - val_loss: -886.1262\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1317.1112 - val_loss: -934.9164\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1387.7094 - val_loss: -992.8698\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1464.8575 - val_loss: -1038.0214\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -1535.9362 - val_loss: -1093.0135\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -1613.4602 - val_loss: -1141.5145\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -1689.3093 - val_loss: -1200.5983\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -1768.4078 - val_loss: -1256.9601\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -1848.6809 - val_loss: -1310.6597\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -1937.5166 - val_loss: -1380.8213\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2035.6844 - val_loss: -1455.9443\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2135.1401 - val_loss: -1503.1053\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2229.1001 - val_loss: -1598.7231\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -2339.9270 - val_loss: -1663.9844\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2447.2498 - val_loss: -1749.5339\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -2545.4419 - val_loss: -1819.2302\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2661.6296 - val_loss: -1900.4404\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2782.9180 - val_loss: -1979.6967\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -2905.8591 - val_loss: -2069.1677\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -3034.9053 - val_loss: -2171.4365\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -3169.2163 - val_loss: -2268.3494\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: -3309.5483 - val_loss: -2363.5962\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -3450.2017 - val_loss: -2459.3606\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -3580.8870 - val_loss: -2557.0835\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -3727.8743 - val_loss: -2667.2319\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -3887.7991 - val_loss: -2764.0503\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: -4044.0662 - val_loss: -2884.3120\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: -4199.7900 - val_loss: -2989.4771\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: -4374.9458 - val_loss: -3119.3298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bdd6fdab80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(1, activation=\"sigmoid\")(encoded)\n",
    "# Capas red decoder\n",
    "# IDEA: red simétrica\n",
    "decoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(410, activation=\"sigmoid\")(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder (lo que nos sirve para generar las etiquetas)\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "best_batch_size= 5\n",
    "best_val_split = 0.25\n",
    "\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "autoencoder.fit(X_train, X_train, epochs=100, batch_size=best_batch_size, validation_split=best_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De los 7380 elementos que tiene la matriz de datos de test, hay 6151 que no se han estimado de manera exacta (83.35%).\n",
      "De los 27880 elementos que tiene la matriz de datos de train, hay 23589 que no se han estimado de manera exacta (84.61%).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Precisión en partición de test\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "X_test_matrix = np.array(X_test)\n",
    "diff = X_test_matrix - X_test_pred\n",
    "dims = diff.shape\n",
    "total_elem = dims[0]*dims[1]\n",
    "elem_no_estim = (np.abs(diff) > 0.1).sum()\n",
    "print(\"De los {} elementos que tiene la matriz de datos de test, hay {} que no se han estimado de manera exacta ({:0.2f}%).\"\\\n",
    "      .format(total_elem, elem_no_estim, elem_no_estim*100/total_elem))\n",
    "# Hemos dado un margen de error en la precisión del [-0.1, 0.1]. \n",
    "# Igual es incluso demasiado elevado (10% en los datos de correlaciones)? SBM!!\n",
    "# Métricas: mape, mse\n",
    "\n",
    "\n",
    "# Precisión en partición de train\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "X_train_matrix = np.array(X_train)\n",
    "diff = X_train_matrix - X_train_pred\n",
    "dims = diff.shape\n",
    "total_elem = dims[0]*dims[1]\n",
    "elem_no_estim = (np.abs(diff) > 0.1).sum()\n",
    "print(\"De los {} elementos que tiene la matriz de datos de train, hay {} que no se han estimado de manera exacta ({:0.2f}%).\"\\\n",
    "      .format(total_elem, elem_no_estim, elem_no_estim*100/total_elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder\n",
    "\n",
    "El objetivo es que la red encoder aprenda la distribución generadora de los datos.\n",
    "Esto permitirá generar nuevas muestras.\n",
    "\n",
    "In VAE, we optimize two loss functions: reconstruction loss and KL-divergence loss. We will learn about them in detail in the next section. For now, remember that the reconstruction loss ensures that the images generated by the decoder are similar to the input or the ones in the dataset. While the KL-divergence measures the divergence between a pair of probability distributions, in this case, the pair of distributions being the latent vector Z (sampled from $Z_{\\mu}$ and $Z_{\\sigma}$) and unit normal distribution $\\mathcal{N}(0, 1)$. KL-divergence ensures that the latent-variables are close to the standard normal distribution.\n",
    "\n",
    "Partimos de la misma red encoder, sólo que esta vez la última capa se descompone ahora en dos capas (Dense), generando concretamente los vectores mean y log_variance (latent-variables, las cuales se espera que sigan una distribución normal).\n",
    "\n",
    "Referencia: https://learnopencv.com/variational-autoencoder-in-tensorflow/#network-fmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{REVISAR ENTERO}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(encoded)\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "mean = layers.Dense(2, name=\"mean\")(encoded)\n",
    "log_var = layers.Dense(2, name=\"log_var\")(encoded)\n",
    "\n",
    "# Red de sampleo\n",
    "def sampling_reparameterization(distribution_params):\n",
    "    mean, log_var = distribution_params\n",
    "    epsilon = backend.random_normal(shape=backend.shape(mean), mean=0., stddev=1.)\n",
    "    z = mean + backend.exp(log_var / 2) * epsilon\n",
    "    return z\n",
    "sample = layers.Lambda(sampling_reparameterization)([mean, log_var])\n",
    "# A Lambda layer comes in handy when you want to pass a tensor to a custom function that isn’t already included in tensorflow\n",
    "\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(410, activation=\"sigmoid\")(sample)\n",
    "\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, (mean, log_var), name=\"Encoder\")\n",
    "# Sampler \n",
    "sampler = models.Model([mean,log_var], sample,  name=\"Encoder_2\")\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De los 7380 elementos que tiene la matriz de datos de test, hay 6308 que no se han estimado de manera exacta (85.47%).\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "X_test_matrix = np.array(X_test)\n",
    "diff = X_test_matrix - X_test_pred\n",
    "dims = diff.shape\n",
    "total_elem = dims[0]*dims[1]\n",
    "elem_no_estim = (diff < -0.1).sum() + (diff > 0.1).sum()\n",
    "print(\"De los {} elementos que tiene la matriz de datos de test, hay {} que no se han estimado de manera exacta ({:0.2f}%).\"\\\n",
    "      .format(total_elem, elem_no_estim, elem_no_estim*100/total_elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, method, test_id=testFNC[\"Id\"]):\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape)\n",
    "#     print(submissionDF.head(10))\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    submissionDF.to_csv(f\"{current_path}\\submissions\\MLSP_submission_{method}_{current_time}.csv\", header=True, index=False)\n",
    "    \n",
    "create_submission(y_pred_kaggle_XGB, \"XGBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
