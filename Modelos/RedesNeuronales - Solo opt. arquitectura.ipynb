{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f259b",
   "metadata": {},
   "source": [
    "# Optimización de un modelo de red neuronal (fully-connected)\n",
    "\n",
    "Este notebook recoge los resultados de la búsqueda del mejor modelo de clasificación mediante una red neuronal densa o fully-connected, ya que el uso de redes convolucionales no parece adecuado para un problema de datos tabulares.\n",
    "\n",
    "Para buscar el mejor modelo posible, se tratará de buscar los mejores hiperparámetros para el número de capas ocultas de la red, su anchura (número de neuronas), posible introducción de términos de regularización, optimizadores, ...\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40286d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de optimización de hiperparámetros\n",
    "import optuna\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4b44",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3014e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2d87e",
   "metadata": {},
   "source": [
    "# Prueba: efectividad preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d8b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396c67f",
   "metadata": {},
   "source": [
    "### Sin aplicar preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54b5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_no_preprocess(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    Se va a utilizar para estudiar si las redes tienen un mejor funcionamiento si se escalan sus datos\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c647e90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:45,421]\u001b[0m A new study created in memory with name: no-name-3d95d5a7-91fe-488e-986d-3640dbe63d36\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:50,428]\u001b[0m Trial 0 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:51,840]\u001b[0m Trial 1 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 150, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:54,238]\u001b[0m Trial 2 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 150, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:55,693]\u001b[0m Trial 3 finished with value: 0.4444444477558136 and parameters: {'n_layers': 2, 'n_units': 50, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:01:57,284]\u001b[0m Trial 4 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:00,441]\u001b[0m Trial 5 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 50, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:01,962]\u001b[0m Trial 6 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:04,091]\u001b[0m Trial 7 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:06,203]\u001b[0m Trial 8 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 150, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:07,800]\u001b[0m Trial 9 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 100, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:11,103]\u001b[0m Trial 10 finished with value: 0.4444444477558136 and parameters: {'n_layers': 2, 'n_units': 250, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:13,497]\u001b[0m Trial 11 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 250, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:15,396]\u001b[0m Trial 12 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 100, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:17,770]\u001b[0m Trial 13 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 150, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:19,429]\u001b[0m Trial 14 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objeto \"study\" y buscamos la optimización de la función objetivo.\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study_no_preprocess = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_no_preprocess.optimize(objective_no_preprocess, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de02825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[0.4444444477558136], datetime_start=datetime.datetime(2022, 7, 3, 13, 1, 45, 422178), datetime_complete=datetime.datetime(2022, 7, 3, 13, 1, 50, 428365), params={'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'}, distributions={'n_layers': IntUniformDistribution(high=5, low=2, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=50), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_no_preprocess.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f94a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 366ms/step - loss: 0.6823 - acc: 0.5686 - val_loss: 0.6734 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4921 - acc: 0.8431 - val_loss: 0.9328 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4662 - acc: 0.7255 - val_loss: 0.6614 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1876 - acc: 1.0000 - val_loss: 0.6741 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0767 - acc: 1.0000 - val_loss: 0.8513 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.7580 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.7779 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.8797 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.8762 - val_acc: 0.7059\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9029 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6104 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_no_preprocess = models.Sequential()\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna_no_preprocess.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_no_preprocess.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_no_preprocess.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ac73f",
   "metadata": {},
   "source": [
    "### Escalando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ece1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocess = StandardScaler()\n",
    "\n",
    "X_train_processed = preprocess.fit_transform(X_train)\n",
    "X_test_processed = preprocess.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b231074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_preprocess(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    Se va a utilizar para estudiar si las redes tienen un mejor funcionamiento si se escalan sus datos\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train_processed, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test_processed, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe9c336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:23,399]\u001b[0m A new study created in memory with name: no-name-d70c8b1d-c071-4372-bbfd-92d874a2d735\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:25,392]\u001b[0m Trial 0 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:26,868]\u001b[0m Trial 1 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 150, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:28,724]\u001b[0m Trial 2 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 150, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:30,079]\u001b[0m Trial 3 finished with value: 0.4444444477558136 and parameters: {'n_layers': 2, 'n_units': 50, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:31,466]\u001b[0m Trial 4 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:33,404]\u001b[0m Trial 5 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 50, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:34,858]\u001b[0m Trial 6 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:35,950]\u001b[0m Trial 7 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:37,223]\u001b[0m Trial 8 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 150, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:39,242]\u001b[0m Trial 9 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 100, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:40,502]\u001b[0m Trial 10 finished with value: 0.4444444477558136 and parameters: {'n_layers': 2, 'n_units': 250, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:42,299]\u001b[0m Trial 11 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 250, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:43,410]\u001b[0m Trial 12 finished with value: 0.4444444477558136 and parameters: {'n_layers': 5, 'n_units': 100, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:44,882]\u001b[0m Trial 13 finished with value: 0.4444444477558136 and parameters: {'n_layers': 3, 'n_units': 150, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-03 13:02:46,491]\u001b[0m Trial 14 finished with value: 0.4444444477558136 and parameters: {'n_layers': 4, 'n_units': 200, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.4444444477558136.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objeto \"study\" y buscamos la optimización de la función objetivo.\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study_preprocess = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_preprocess.optimize(objective_preprocess, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc5382a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[0.4444444477558136], datetime_start=datetime.datetime(2022, 7, 3, 13, 2, 23, 399571), datetime_complete=datetime.datetime(2022, 7, 3, 13, 2, 25, 391131), params={'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'}, distributions={'n_layers': IntUniformDistribution(high=5, low=2, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=50), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_preprocess.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab75eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 239ms/step - loss: 0.6690 - acc: 0.5686 - val_loss: 0.8292 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3761 - acc: 0.8627 - val_loss: 0.7716 - val_acc: 0.7059\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1697 - acc: 0.9412 - val_loss: 0.6257 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0484 - acc: 1.0000 - val_loss: 0.6701 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.6927 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.7118 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.7372 - val_acc: 0.8235\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.7692 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6386 - acc: 0.6111\n",
      "Accuracy: 61.11%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_no_preprocess = models.Sequential()\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_no_preprocess.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna_no_preprocess.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_no_preprocess.fit(X_train_processed, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_no_preprocess.evaluate(X_test_processed, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c920bc",
   "metadata": {},
   "source": [
    "Lo anterior muestra que el uso de los datos escalados puede hacer que, aún habiéndose escogido la misma infraestructura de red en el proceso de optimización, la misma red entrenada sobre los datos escalados pierde en accuracy frente a la entrenada con los datos sin aplicar ningún tipo de preprocesado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61745a4",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb208b",
   "metadata": {},
   "source": [
    "Para redes neuronales, compararemos los resultados obtenidos construyendo redes a partir de librerías distintas.\n",
    "\n",
    "**Comenzamos con ``MLPClassifier`` de ``sklearn`` y búsqueda de hiperparámetros con ``GridSearchCV``:**\n",
    "\n",
    "Documentación: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b4b39",
   "metadata": {},
   "source": [
    "El método debe recibir arquitecturas de red pre-definidas, por lo que probaremos topologías variadas en cuanto a profundidad, ancho y número de capas.\n",
    "\n",
    "_NOTA: el parámetro ``learning_rate`` sólo se aplica cuando el solver que se esté utilizando sea el \"sgd\", el cual toma el valor constamte por defecto. Podría resultar de especial utilidad cuando toma el valor \"adaptive\", en ese caso mantiene el valor del learning_rate constante mientras la curva de pérdida siga decreciendo, en el momento en que haya dos épocas consecutivas en las que no decrece un mínimo el valor de loss, el learning_rate se divide entre 5._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d163caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_MLPC = MLPClassifier(max_iter=1000, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\"], # solver \"lbfgs\" no permite hacer los plots más abajo\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"] # únicamente válido junto con el solver \"sgd\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2733f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv_results_MLPC = train_GridSearchCV(model_MLPC, param_grid_MLPC, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d305580c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'invscaling',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'adam'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_GridSearchCV(cv_results_MLPC[\"mean_test_score\"])\n",
    "models_same_acc_GridSearchCV(cv_results_MLPC, top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fdd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_MLPC_opt = MLPClassifier(activation=\"relu\", hidden_layer_sizes=(100, 250, 250, 100, 1), solver=\"adam\",\n",
    "                               max_iter=1000, random_state=0)\n",
    "model_MLPC_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_MLPC = model_MLPC_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_MLPC)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b89f6b",
   "metadata": {},
   "source": [
    "_COMPROBACIÓN: la función de activación elegida sólo afecta a las capas ocultas y no a la capa de clasificación en la salida._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8455bf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logistic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLPC_opt.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6075466",
   "metadata": {},
   "source": [
    "Las anteriores celdas de código muestran un \"warning\" indicando que el método de computación alcanza el número máximo de iteraciones (épocas de entrenamiento) sin haber llegado a converger.\n",
    "\n",
    "El criterio para considerar que ha habido convergencia está definido en la documentación como: la reducción del valor de la función de loss es inferior a 1e-4 por un número de etapas determinado. \n",
    "\n",
    "La anterior condición por tanto no se está cumpliendo en nuestro entrenamiento, así que vamos a pintar la evolución de la función de pérdida para determinar si estamos cortando el entrenamiento demasiado pronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a417d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(model_MLPC_opt.loss_curve_)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3d32f",
   "metadata": {},
   "source": [
    "La anterior curva de loss parece tener una tendencia aún claramente descendiente cuando es cortada en la época número 1000. Se va a repetir el entrenamiento incrementando este valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c41d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_MLPC2 = MLPClassifier(max_iter=1500, random_state=0)\n",
    "cv_results_MLPC2 = train_GridSearchCV(model_MLPC2, param_grid_MLPC, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1316b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'identity',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'identity',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'invscaling',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'identity',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'sgd'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'invscaling',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'sgd'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'invscaling',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'tanh',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'invscaling',\n",
       "  'solver': 'adam'},\n",
       " {'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'solver': 'adam'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_GridSearchCV(cv_results_MLPC2[\"mean_test_score\"])\n",
    "models_same_acc_GridSearchCV(cv_results_MLPC2, top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1039b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "model_MLPC_opt2 = MLPClassifier(activation=\"identity\", hidden_layer_sizes=(200, 200, 100, 50, 1), solver=\"adam\",\n",
    "                                max_iter=1500, random_state=0)\n",
    "model_MLPC_opt2.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_MLPC2 = model_MLPC_opt2.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_MLPC2)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9b4ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_MLPC_opt2.loss_curve_)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af707e",
   "metadata": {},
   "source": [
    "Al aumentar el número de épocas, los resultados varían y el modelo que se retorna ahora como óptimo necesita solamente 25 épocas para converger.\n",
    "\n",
    "Sin embargo, sorprende que este último modelo que ha resultado generar predicciones con un mejor accuracy y que no sobrepasan el primer tope de 1000 iteraciones no haya sido el elegido en la primera búsqueda. Vamos a comparar las precisiones que han alcanzado cada uno de los modelos en el entrenamiento en cada una de las repeticiones de cross-validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = {'activation': 'relu',\n",
    "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
    "  'learning_rate': 'constant',\n",
    "  'solver': 'adam'}\n",
    "\n",
    "result2 = {'activation': 'identity',\n",
    "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
    "  'learning_rate': 'constant',\n",
    "  'solver': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9170b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(cv_results_MLPC[\"params\"]):\n",
    "    if item == result1:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "for i, item in enumerate(cv_results_MLPC[\"params\"]):\n",
    "    if item == result2:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "171e0701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELO OPT (1000 iter) - Accuracy en primer entrenamiento: 0.6764705882352942\n",
      "MODELO OPT (1500 iter) - Accuracy en primer entrenamiento: 0.676470588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"MODELO OPT (1000 iter) - Accuracy en primer entrenamiento:\", cv_results_MLPC[\"mean_test_score\"][91])\n",
    "print(\"MODELO OPT (1500 iter) - Accuracy en primer entrenamiento:\", cv_results_MLPC[\"mean_test_score\"][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5397be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(cv_results_MLPC2[\"params\"]):\n",
    "    if item == result1:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "for i, item in enumerate(cv_results_MLPC2[\"params\"]):\n",
    "    if item == result2:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21cca501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELO OPT (1000 iter) - Accuracy en segundo entrenamiento: 0.6617647058823529\n",
      "MODELO OPT (1500 iter) - Accuracy en segundo entrenamiento: 0.676470588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"MODELO OPT (1000 iter) - Accuracy en segundo entrenamiento:\", cv_results_MLPC2[\"mean_test_score\"][91])\n",
    "print(\"MODELO OPT (1500 iter) - Accuracy en segundo entrenamiento:\", cv_results_MLPC2[\"mean_test_score\"][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d4a85",
   "metadata": {},
   "source": [
    "Se observa que en la primera prueba de ``GridSearchCV`` ambos modelos tienen una accuracy prácticamente idéntica durante el entrenamiento, aunque al no terminar de converger el primero (visto con las 1000 iteraciones), finalmente su accuracy tiene un decimal más y por ser en definitiva mayor, es el único modelo escogido. Sin embargo, al aumentar el margen de iteraciones, mientras que el segundo modelo (1500 iteraciones) no cambia su accuracy, el primero sigue entrenando y el valor final de precisión del entrenamiento en este segundo caso decrece.\n",
    "\n",
    "Por tanto, el óptimo hasta este momento y de acuerdo a las pruebas anteriores es el modelo obtenido en la segunda prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14ede2",
   "metadata": {},
   "source": [
    "La librería ``sklearn`` permite implementar un tercer optimizador, lbfgs, que no se ha incluído en las pruebas anteriores ya que no permite dibujar la curva de loss para hacer las anteriores comprobaciones. Por tanto, vamos ahora a probar y comparar los resultados manualmente incluyendo este optimizador.\n",
    "\n",
    "El solver ``sgd`` no se ha elegido en ninguna de las anteriores pruebas. Como en esta ocasión queremos comprobar si el solver ``lbfgs`` incrementa el accuracy frente a los resultados anteriores, prescindiremos del solver ``sgd`` y la optimización del tipo de learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f906f2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC3 = MLPClassifier(max_iter=1000, random_state=0)\n",
    "param_grid_MLPC3 = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"adam\", \"lbfgs\"]\n",
    "}\n",
    "cv_results_MLPC3 = train_GridSearchCV(model_MLPC3, param_grid_MLPC3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e26327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'relu',\n",
       "  'hidden_layer_sizes': (100, 250, 250, 100, 1),\n",
       "  'solver': 'adam'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_GridSearchCV(cv_results_MLPC3[\"mean_test_score\"])\n",
    "models_same_acc_GridSearchCV(cv_results_MLPC3, top_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f027b",
   "metadata": {},
   "source": [
    "Si volvemos a utilizar el máximo de 1000 iteraciones, el óptimo es el mismo obtenido en el primer caso. Vamos a repetir el intento ahora con 1500 épocas de máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e7f5dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC4 = MLPClassifier(max_iter=1500, random_state=0)\n",
    "cv_results_MLPC4 = train_GridSearchCV(model_MLPC4, param_grid_MLPC3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5d8de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'identity',\n",
       "  'hidden_layer_sizes': (200, 200, 100, 50, 1),\n",
       "  'solver': 'adam'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_GridSearchCV(cv_results_MLPC4[\"mean_test_score\"])\n",
    "models_same_acc_GridSearchCV(cv_results_MLPC4, top_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df63b0",
   "metadata": {},
   "source": [
    "Nuevamente, se obtiene el mismo resultado que en el caso anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b5a47",
   "metadata": {},
   "source": [
    "**Usando la librería \"keras\"**\n",
    "\n",
    "Ahora utilizaremos la librería de ``keras``, por su mayor flexibilidad para intentar mejorar los resultados de la red neuronal.\n",
    "\n",
    "Comenzaremos repitiendo la búsqueda de hiperparámetros, ya que la propia librería de ``keras`` dispone de integración con otras que nos permitirán hacer una búsqueda algo más exhaustiva por ejemplo en cuanto al número de capas y neuronas en estas. Concretamente, vamos a utilizar ``optuna``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be2b93",
   "metadata": {},
   "source": [
    "Documentación:\n",
    "* https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html\n",
    "* https://optuna.org/\n",
    "\n",
    "Para reducir el coste computacional tomaremos de base resultados como la función de activación óptima: \"relu\", que hemos podido obtener con ``GridSearchCV``. Como por el contrario usando ``sklearn`` no hemos podido utilizar el optimizador RMSProp, vamos a probarlo también con ``optuna`` + ``keras`` para ver si mejora nuestros resultados.\n",
    "\n",
    "Búsqueda mediante la librería ``optuna`` probando 2 métodos de búsqueda de hiperparámetros:\n",
    "\n",
    "* **GridSampler:** equivalente a la anterior búsqueda de grid de sklearn. Lo usaremos para que los resultados sean comparables.\n",
    "* **TPE:** algoritmo para hacer una \"búsqueda inteligente\" de hiperparámetros. Debería ahorrar intentos de combinaciones haciendo una selección inteligente de las pruebas. En nuestro caso le permitiremos probar un 10% del número de combinaciones posibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1709fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveNN_Grid(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo GridSampler.\n",
    "    En este caso se trata de maximizar el accuracy para una red neuronal con activación sigmoide\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2fd41f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# Prueba con GridSampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "search_space = {\"n_layers\": range(2, 6), \n",
    "                \"n_units\": range(50, 300, 50),\n",
    "                \"optimizer\": [\"RMSprop\", \"SGD\", \"Adam\"]\n",
    "               }\n",
    "sampler = optuna.samplers.GridSampler(search_space)\n",
    "study_Grid = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_Grid.optimize(objectiveNN_Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5691260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_layers': 4, 'n_units': 50, 'optimizer': 'SGD'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_OptunaSearchCV(study_Grid.get_trials())\n",
    "models_same_acc_OptunaSearchCV(study_Grid.get_trials(), top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d211270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.6682 - acc: 0.5490 - val_loss: 0.6626 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6665 - acc: 0.5490 - val_loss: 0.6624 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6647 - acc: 0.5490 - val_loss: 0.6620 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6630 - acc: 0.5490 - val_loss: 0.6617 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6613 - acc: 0.5490 - val_loss: 0.6614 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6596 - acc: 0.5490 - val_loss: 0.6613 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6984 - acc: 0.4444\n",
      "Accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_Grid = models.Sequential()\n",
    "modelFC_optuna_Grid.add(layers.Dense(50, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_Grid.add(layers.Dense(50, activation=\"relu\"))\n",
    "modelFC_optuna_Grid.add(layers.Dense(50, activation=\"relu\"))\n",
    "modelFC_optuna_Grid.add(layers.Dense(50, activation=\"relu\"))\n",
    "modelFC_optuna_Grid.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna_Grid.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_Grid.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_Grid.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb2cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveNN_TPE(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    En este caso se trata de maximizar el accuracy para una red neuronal con activación sigmoide\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6846d5f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objeto \"study\" y buscamos la optimización de la función objetivo.\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_TPE.optimize(objectiveNN_TPE, n_trials=6)\n",
    "# n_trials = (4 x 5 x 3) * 0.1 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9563b3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'},\n",
       " {'n_layers': 4, 'n_units': 150, 'optimizer': 'SGD'},\n",
       " {'n_layers': 5, 'n_units': 150, 'optimizer': 'SGD'},\n",
       " {'n_layers': 2, 'n_units': 50, 'optimizer': 'Adam'},\n",
       " {'n_layers': 5, 'n_units': 200, 'optimizer': 'SGD'},\n",
       " {'n_layers': 4, 'n_units': 50, 'optimizer': 'RMSprop'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_OptunaSearchCV(study_TPE.get_trials())\n",
    "models_same_acc_OptunaSearchCV(study_TPE.get_trials(), top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d14b938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 0.6836 - acc: 0.5490 - val_loss: 0.6775 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6823 - acc: 0.5490 - val_loss: 0.6768 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6808 - acc: 0.5490 - val_loss: 0.6761 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6794 - acc: 0.5490 - val_loss: 0.6754 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6781 - acc: 0.5490 - val_loss: 0.6746 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6767 - acc: 0.5490 - val_loss: 0.6739 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6916 - acc: 0.4444\n",
      "Accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_TPE = models.Sequential()\n",
    "modelFC_optuna_TPE.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_TPE.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_TPE.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_TPE.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_TPE.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_TPE.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna_TPE.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_TPE.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_TPE.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8da59",
   "metadata": {},
   "source": [
    "Veamos si podemos obtener mejores resultados cambiando la última capa con activación sigmoide por una activación softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443bedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# En primer lugar, hay que adaptar los datos\n",
    "NUM_CLASSES = 2\n",
    "y_train_softmax = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_softmax = np_utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b767209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveSoftmax_Grid(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo GridSampler.\n",
    "    En este caso se trata de maximizar el accuracy para una red neuronal con activación softmax\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train_softmax, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b70e2c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 2.0830 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5392 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5812 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7217 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4972 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7064 - accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7097 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5149 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6739 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6523 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7195 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6943 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7581 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6736 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6536 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5716 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5735 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7213 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4889 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5612 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7102 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6397 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7042 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5437 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5638 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6378 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7550 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7109 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7141 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7202 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5131 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5949 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5619 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6763 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5677 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6791 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6527 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7150 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7514 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7531 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9518 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6257 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5256 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5713 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7287 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8276 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.7163 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7187 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6622 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3878 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7076 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4896 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5843 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5860 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6348 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6956 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8789 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4565 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5114 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1638 - accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# Prueba con GridSampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "search_space = {\"n_layers\": range(2, 6), \n",
    "                \"n_units\": range(50, 300, 50),\n",
    "                \"optimizer\": [\"RMSprop\", \"SGD\", \"Adam\"]\n",
    "               }\n",
    "sampler = optuna.samplers.GridSampler(search_space)\n",
    "studySoftmax_Grid = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "studySoftmax_Grid.optimize(objectiveSoftmax_Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64f63924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_layers': 5, 'n_units': 150, 'optimizer': 'RMSprop'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_OptunaSearchCV(studySoftmax_Grid.get_trials())\n",
    "models_same_acc_OptunaSearchCV(studySoftmax_Grid.get_trials(), top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5e3b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 412ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_Grid_softmax = models.Sequential()\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna_Grid_softmax.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_optuna_Grid_softmax.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_Grid_softmax.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_Grid_softmax.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d41c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveSoftmax_TPE(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    En este caso se trata de maximizar el accuracy para una red neuronal con activación softmax\n",
    "    '''\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train_softmax, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c552f735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5105 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6960 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7070 - accuracy: 0.2778\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6543 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6874 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.6445 - accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "studySoftmax_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "studySoftmax_TPE.optimize(objectiveSoftmax_TPE, n_trials=6)\n",
    "# n_trials = (4 x 5 x 3) * 0.1 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4cfd980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_layers': 4, 'n_units': 200, 'optimizer': 'RMSprop'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acc = top_acc_OptunaSearchCV(studySoftmax_TPE.get_trials())\n",
    "models_same_acc_OptunaSearchCV(studySoftmax_TPE.get_trials(), top_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19d7dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.6864 - acc: 0.5686 - val_loss: 0.6845 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5365 - acc: 0.7451 - val_loss: 0.7852 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4486 - acc: 0.7647 - val_loss: 0.7125 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2293 - acc: 0.9804 - val_loss: 0.7357 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0894 - acc: 1.0000 - val_loss: 0.9013 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.9585 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9094 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5813 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna_TPE_softmax = models.Sequential()\n",
    "modelFC_optuna_TPE_softmax.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna_TPE_softmax.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_TPE_softmax.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_TPE_softmax.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna_TPE_softmax.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_optuna_TPE_softmax.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna_TPE_softmax.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna_TPE_softmax.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa31a6",
   "metadata": {},
   "source": [
    "Obtenemos un mayor accuracy utilizando la activación softmax. Sin embargo, parece que la red tiene un problema de sobreajuste. Vamos a tratar de reducir esta diferencia probando distintos tipos de regularización.\n",
    "\n",
    "Lo probamos sobre la red \"más grande\" (más capas), que es la obtenida en la búsqueda con el sampler ``GridSampler`` y la activación softmax en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddc9d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 273ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy : 88.89% ----- Regularización: None \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 465ms/step - loss: 94.5943 - acc: 0.5686 - val_loss: 88.3086 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 87.2959 - acc: 0.5490 - val_loss: 83.1615 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 82.3585 - acc: 0.5490 - val_loss: 78.9901 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 78.2947 - acc: 0.5490 - val_loss: 75.3455 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 74.7184 - acc: 0.5490 - val_loss: 72.0417 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 71.4616 - acc: 0.5490 - val_loss: 68.9792 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 68.9915 - acc: 0.4444\n",
      "Accuracy : 44.44% ----- Regularización: l1 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 8.6900 - acc: 0.5686 - val_loss: 7.9392 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 7.8063 - acc: 0.5686 - val_loss: 7.3718 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 7.2342 - acc: 0.6471 - val_loss: 6.9339 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.7587 - acc: 0.8235 - val_loss: 6.6261 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6.3901 - acc: 0.7451 - val_loss: 6.3522 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.9875 - acc: 0.8824 - val_loss: 6.0613 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.6850 - acc: 0.9216 - val_loss: 6.0565 - val_acc: 0.5294\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.5425 - acc: 0.8627 - val_loss: 5.6746 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.1227 - acc: 1.0000 - val_loss: 5.4277 - val_acc: 0.7059\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.8737 - acc: 1.0000 - val_loss: 5.2603 - val_acc: 0.7059\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.6510 - acc: 1.0000 - val_loss: 5.1002 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.9892 - acc: 0.7778\n",
      "Accuracy : 77.78% ----- Regularización: l2 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 359ms/step - loss: 102.5319 - acc: 0.3922 - val_loss: 95.4868 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 94.3453 - acc: 0.5294 - val_loss: 89.7512 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 88.8530 - acc: 0.5490 - val_loss: 85.1273 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 84.3539 - acc: 0.5686 - val_loss: 81.1005 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 80.4065 - acc: 0.5294 - val_loss: 77.4611 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 76.8219 - acc: 0.5490 - val_loss: 74.0979 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 74.1095 - acc: 0.4444\n",
      "Accuracy : 44.44% ----- Regularización: l1_l2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "regularizer_types = [None, \"l1\", \"l2\", \"l1_l2\"]\n",
    "\n",
    "for regularizer in regularizer_types:\n",
    "    modelFC_optuna_Grid_softmax = models.Sequential()\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizer, input_shape=(410,)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    modelFC_optuna_Grid_softmax.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna_Grid_softmax.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy = modelFC_optuna_Grid_softmax.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy : {:0.2f}% ----- Regularización: {} \\n\".format(accuracy * 100, regularizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82ad14",
   "metadata": {},
   "source": [
    "Parece que la regularización hace decrecer la precisión de la red. \n",
    "\n",
    "Vamos a probar si podemos mejorar el resultado cambiando el valor del parámetro de la regularización L2 (término de penalización)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2b6bf11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 265ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy : 88.89% ----- Penalización = 0 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 631ms/step - loss: 0.7727 - acc: 0.5490 - val_loss: 0.7409 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6769 - acc: 0.6667 - val_loss: 0.7503 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5574 - acc: 0.7647 - val_loss: 0.7863 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3975 - acc: 0.8824 - val_loss: 0.7172 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2095 - acc: 1.0000 - val_loss: 0.9658 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1453 - acc: 1.0000 - val_loss: 0.9572 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1032 - acc: 1.0000 - val_loss: 0.5617 - val_acc: 0.7059\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0961 - acc: 1.0000 - val_loss: 1.0233 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0844 - acc: 1.0000 - val_loss: 0.9673 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6711 - acc: 0.7778\n",
      "Accuracy : 77.78% ----- Penalización = 0.0001 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 422ms/step - loss: 1.4964 - acc: 0.5686 - val_loss: 1.4407 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.3660 - acc: 0.6471 - val_loss: 1.4670 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.2345 - acc: 0.7255 - val_loss: 1.5349 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.0761 - acc: 0.9216 - val_loss: 1.5068 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.9045 - acc: 0.9608 - val_loss: 1.7188 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.7957 - acc: 1.0000 - val_loss: 1.4471 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7378 - acc: 1.0000 - val_loss: 1.3777 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7031 - acc: 1.0000 - val_loss: 1.5503 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6803 - acc: 1.0000 - val_loss: 1.4961 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4007 - acc: 0.6667\n",
      "Accuracy : 66.67% ----- Penalización = 0.001 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 238ms/step - loss: 8.7034 - acc: 0.4314 - val_loss: 7.9881 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 7.8392 - acc: 0.6275 - val_loss: 7.4552 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 7.2793 - acc: 0.6863 - val_loss: 7.0449 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 6.7802 - acc: 0.8824 - val_loss: 6.7581 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 6.4508 - acc: 0.7843 - val_loss: 6.5944 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 6.0827 - acc: 0.8824 - val_loss: 6.2641 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5.7297 - acc: 0.9804 - val_loss: 6.1497 - val_acc: 0.5294\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.4855 - acc: 0.9608 - val_loss: 5.9213 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.1882 - acc: 1.0000 - val_loss: 5.7051 - val_acc: 0.7647\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.9559 - acc: 1.0000 - val_loss: 5.5202 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.7451 - acc: 1.0000 - val_loss: 5.3496 - val_acc: 0.7647\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.5431 - acc: 1.0000 - val_loss: 5.2588 - val_acc: 0.6471\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.3510 - acc: 1.0000 - val_loss: 4.9707 - val_acc: 0.7059\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.1576 - acc: 1.0000 - val_loss: 4.8382 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.6847 - acc: 0.7222\n",
      "Accuracy : 72.22% ----- Penalización = 0.01 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 80.3163 - acc: 0.5098 - val_loss: 72.6355 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 71.4401 - acc: 0.5686 - val_loss: 66.7183 - val_acc: 0.5294\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 65.8093 - acc: 0.5490 - val_loss: 62.1135 - val_acc: 0.5294\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 61.3539 - acc: 0.5686 - val_loss: 58.2200 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 57.5547 - acc: 0.5490 - val_loss: 54.7874 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 54.1881 - acc: 0.5490 - val_loss: 51.6854 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 51.6964 - acc: 0.4444\n",
      "Accuracy : 44.44% ----- Penalización = 0.1 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 360ms/step - loss: 799.6153 - acc: 0.4510 - val_loss: 722.5627 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 710.6449 - acc: 0.5098 - val_loss: 663.2262 - val_acc: 0.7059\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 654.2073 - acc: 0.5882 - val_loss: 617.0561 - val_acc: 0.7647\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 609.5286 - acc: 0.6471 - val_loss: 578.0114 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 571.4247 - acc: 0.6078 - val_loss: 543.5861 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 537.6614 - acc: 0.5490 - val_loss: 512.4725 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 507.0507 - acc: 0.5490 - val_loss: 483.8954 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 478.8687 - acc: 0.5490 - val_loss: 457.3521 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 457.3635 - acc: 0.4444\n",
      "Accuracy : 44.44% ----- Penalización = 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "l2_rates = [0, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for rate in l2_rates:\n",
    "    modelFC_optuna_Grid_softmax = models.Sequential()\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizers.L2(rate),\n",
    "                                                 input_shape=(410,)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizers.L2(rate)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizers.L2(rate)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizers.L2(rate)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", kernel_regularizer=regularizers.L2(rate)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    modelFC_optuna_Grid_softmax.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna_Grid_softmax.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy = modelFC_optuna_Grid_softmax.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy : {:0.2f}% ----- Penalización = {} \\n\".format(accuracy * 100, rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eea713",
   "metadata": {},
   "source": [
    "Parece que no hay ningún valor del parámetro ``kernel_regularizer`` que no empeore los resultados en precisión de la red.\n",
    "\n",
    "Vamos a probar ahora el control del sobreajuste introduciendo capas con una cierta tasa de dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eda79a99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy : 88.89% ----- Tasa de dropout = 0 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 151ms/step - loss: 0.6913 - acc: 0.6078 - val_loss: 0.6564 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6370 - acc: 0.6667 - val_loss: 0.6331 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5177 - acc: 0.8039 - val_loss: 0.6616 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4352 - acc: 0.7647 - val_loss: 0.9156 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4560 - acc: 0.8039 - val_loss: 0.6223 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2111 - acc: 0.9608 - val_loss: 0.6989 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1230 - acc: 0.9804 - val_loss: 0.5816 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0704 - acc: 1.0000 - val_loss: 0.7672 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.5764 - val_acc: 0.7059\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.8334 - val_acc: 0.7647\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.8151 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6752 - acc: 0.7778\n",
      "Accuracy : 77.78% ----- Tasa de dropout = 0.1 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 145ms/step - loss: 0.6955 - acc: 0.5294 - val_loss: 0.6753 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6676 - acc: 0.5882 - val_loss: 0.6610 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6268 - acc: 0.5686 - val_loss: 0.6498 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5803 - acc: 0.6275 - val_loss: 0.6369 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4783 - acc: 0.8039 - val_loss: 0.7288 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4523 - acc: 0.8039 - val_loss: 0.7355 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3402 - acc: 0.8824 - val_loss: 0.8894 - val_acc: 0.4706\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3438 - acc: 0.8431 - val_loss: 0.8227 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1566 - acc: 1.0000 - val_loss: 0.6199 - val_acc: 0.7059\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1155 - acc: 0.9608 - val_loss: 0.7158 - val_acc: 0.7059\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0771 - acc: 1.0000 - val_loss: 0.8846 - val_acc: 0.7647\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0523 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.7647\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.8854 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8410 - acc: 0.7778\n",
      "Accuracy : 77.78% ----- Tasa de dropout = 0.2 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 567ms/step - loss: 0.6695 - acc: 0.6275 - val_loss: 0.6771 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6877 - acc: 0.5490 - val_loss: 0.6738 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6239 - acc: 0.6863 - val_loss: 0.6678 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6351 - acc: 0.6471 - val_loss: 0.6771 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6205 - acc: 0.7255 - val_loss: 0.7029 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5090 - acc: 0.7647 - val_loss: 0.6695 - val_acc: 0.4706\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6197 - acc: 0.7222\n",
      "Accuracy : 72.22% ----- Tasa de dropout = 0.3 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.7618 - acc: 0.3725 - val_loss: 0.6799 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6941 - acc: 0.6275 - val_loss: 0.6735 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6869 - acc: 0.5098 - val_loss: 0.6713 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7000 - acc: 0.4902 - val_loss: 0.6723 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7005 - acc: 0.5686 - val_loss: 0.6608 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6484 - acc: 0.6078 - val_loss: 0.6509 - val_acc: 0.5882\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6740 - acc: 0.6078 - val_loss: 0.6550 - val_acc: 0.7059\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6024 - acc: 0.7255 - val_loss: 0.6406 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6135 - acc: 0.6667 - val_loss: 0.6305 - val_acc: 0.7647\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5881 - acc: 0.7255 - val_loss: 0.6157 - val_acc: 0.8235\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6040 - acc: 0.7255 - val_loss: 0.6020 - val_acc: 0.8235\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5329 - acc: 0.7255 - val_loss: 0.5945 - val_acc: 0.8235\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5225 - acc: 0.7451 - val_loss: 0.5822 - val_acc: 0.7647\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4902 - acc: 0.8039 - val_loss: 0.5779 - val_acc: 0.7059\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4238 - acc: 0.8431 - val_loss: 0.5770 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5219 - acc: 0.7778\n",
      "Accuracy : 77.78% ----- Tasa de dropout = 0.4 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 308ms/step - loss: 0.7128 - acc: 0.5098 - val_loss: 0.6857 - val_acc: 0.7647\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7781 - acc: 0.5294 - val_loss: 0.6903 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7302 - acc: 0.5294 - val_loss: 0.6870 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.7330 - acc: 0.5686 - val_loss: 0.6893 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7251 - acc: 0.4706 - val_loss: 0.6877 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.7403 - acc: 0.4902 - val_loss: 0.6863 - val_acc: 0.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6926 - acc: 0.4444\n",
      "Accuracy : 44.44% ----- Tasa de dropout = 0.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    modelFC_optuna_Grid_softmax = models.Sequential()\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dropout(rate))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dropout(rate))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dropout(rate))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dropout(rate))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dropout(rate))\n",
    "    modelFC_optuna_Grid_softmax.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    modelFC_optuna_Grid_softmax.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna_Grid_softmax.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy = modelFC_optuna_Grid_softmax.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy : {:0.2f}% ----- Tasa de dropout = {} \\n\".format(accuracy * 100, rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebafe",
   "metadata": {},
   "source": [
    "### Función de pérdida personalizada\n",
    "\n",
    "Otra posibilidad es tratar de tener en cuenta los datos con peores resultados en el backpropagation, es decir, asignar un mayor peso a esos datos. Esta es la idea del método \"AdaBoost\" que sin embargo no está actualmente implementado en keras. Una alternativa es definir propiamente otra función de loss personalizada.\n",
    "\n",
    "Fuente: https://stackoverflow.com/questions/48720197/weight-samples-if-incorrect-guessed-in-binary-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad055d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, tp_weight=0.5, tn_weight=0.5, fp_weight=1, fn_weight=1):\n",
    "    '''\n",
    "    Función de pérdida personalizada para el optimizador de una red neuronal.\n",
    "    El método recibe las predicciones y el valor real de la clasificación, así como los pesos que se desea asignar a los\n",
    "    clasificaciones tanto erróneas como acertadas.\n",
    "    '''\n",
    "#     print(\"NEW ITER ------\")\n",
    "#     print(\"Y_TRUE:\", tf.print(y_true))\n",
    "#     print(\"Y_PRED:\", tf.print(y_pred))\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_classes = tf.keras.backend.greater_equal(y_pred, 0.5)\n",
    "    y_pred_classes_float = tf.keras.backend.cast(y_pred_classes, tf.keras.backend.floatx())\n",
    "    y_true_float = tf.keras.backend.cast(y_true, tf.keras.backend.floatx())\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "#     print(\"1:\", tf.print(y_pred_classes_float))\n",
    "#     print(\"2:\", tf.print(y_true_float))\n",
    "\n",
    "    # Get misclassified examples\n",
    "    wrongly_classified = tf.keras.backend.not_equal(y_true_float, y_pred_classes_float)\n",
    "    wrongly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "    wrongly_classified_float2 = tf.gather(wrongly_classified_float, [0], axis=1)\n",
    "    \n",
    "#     print(\"3:\", tf.print(wrongly_classified_float2))\n",
    "    \n",
    "    # Get correctly classified examples\n",
    "    correctly_classified = tf.keras.backend.equal(y_true_float, y_pred_classes_float)\n",
    "    correctly_classified_float = tf.keras.backend.cast(correctly_classified, tf.keras.backend.floatx())\n",
    "    correctly_classified_float2 = tf.gather(correctly_classified_float, [0], axis=1)\n",
    "    \n",
    "#     print(\"4:\", tf.print(correctly_classified_float2))\n",
    "\n",
    "    # Get tp, fp, tn, fn\n",
    "    tp = correctly_classified_float * y_true_float\n",
    "    tn = correctly_classified_float * (1 - y_true_float)\n",
    "    fp = wrongly_classified_float * y_true_float\n",
    "    fn = wrongly_classified_float * (1 - y_true_float)\n",
    "\n",
    "    # Get weights\n",
    "    weight = tp_weight * tp + fp_weight * fp + tn_weight * tn + fn_weight * fn\n",
    "    weight2 = tf.gather(weight, [0], axis=1)\n",
    "    weight3 = tf.math.reduce_sum(weight2, axis=1)\n",
    "#     print(\"TN_W\", tn)\n",
    "#     tf.print(tp)\n",
    "#     tf.print(tp2)\n",
    "#     tf.print(weight3)\n",
    "    \n",
    "    loss = tf.keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "#     tf.print(loss)\n",
    "    weighted_loss = loss * weight3\n",
    "#     tf.print(weighted_loss)\n",
    "    \n",
    "#     weighted_loss2 = tf.gather(weighted_loss, [0], axis=1)\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73f781",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que los anteriores ``tp``, ``fn``, ... son tensores (vectores de ``tensorflow``) y por tanto, el resultado guardado en ``weight`` es igualmente otro tensor, no un único valor numérico.\n",
    "\n",
    "_NOTA: se entienden los positivos como aquellas muestras correspondientes con una etiqueta \"1\" (enfermos) y por tanto como negativos los registros con etiqueta \"0\" (grupo de control)._\n",
    "\n",
    "Vamos a probar cómo funciona la función personalizada sobre los modelos de redes neuronales que peores resultados han obtenido para comprobar si de este modo podemos mejorar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9bb9f5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.5306 - acc: 0.5098 - val_loss: 0.4859 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4283 - acc: 0.7059 - val_loss: 0.4530 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3334 - acc: 0.8235 - val_loss: 0.4740 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2332 - acc: 0.9020 - val_loss: 0.4568 - val_acc: 0.7059\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1204 - acc: 0.9804 - val_loss: 0.4746 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0468 - acc: 1.0000 - val_loss: 0.3448 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2940 - val_acc: 0.8235\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.4290 - val_acc: 0.8235\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.4033 - val_acc: 0.7647\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.4145 - val_acc: 0.7647\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4270 - val_acc: 0.7647\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4722 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5148 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_CL = models.Sequential()\n",
    "modelFC_CL.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_CL.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_CL.compile(loss=custom_loss, optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_CL.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_CL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16fe8",
   "metadata": {},
   "source": [
    "### Sample-weight\n",
    "\n",
    "Una alternativa a lo anterior, sería usar un vector de pesos propio y pasarlo a la función ``fit`` del modelo, a través del parámetro ``sample_weight``, en lugar de definir una función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "065bb881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 170ms/step - loss: 0.3418 - acc: 0.5882 - val_loss: 0.3174 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2318 - acc: 0.7255 - val_loss: 0.2629 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1168 - acc: 0.9020 - val_loss: 0.2408 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0314 - acc: 1.0000 - val_loss: 0.2301 - val_acc: 0.8235\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.1631 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.1534 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0171 - acc: 0.9804 - val_loss: 0.0695 - val_acc: 0.5294\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 6.9570e-04 - acc: 1.0000 - val_loss: 0.1776 - val_acc: 0.8235\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.9242e-04 - acc: 1.0000 - val_loss: 0.1657 - val_acc: 0.8235\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.1914e-04 - acc: 1.0000 - val_loss: 0.1625 - val_acc: 0.8824\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.6941e-04 - acc: 1.0000 - val_loss: 0.1614 - val_acc: 0.8824\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.3213e-04 - acc: 1.0000 - val_loss: 0.1616 - val_acc: 0.8824\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.0263e-04 - acc: 1.0000 - val_loss: 0.1626 - val_acc: 0.8824\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.7837e-04 - acc: 1.0000 - val_loss: 0.1640 - val_acc: 0.8824\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.5763e-04 - acc: 1.0000 - val_loss: 0.1658 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.3951e-04 - acc: 1.0000 - val_loss: 0.1678 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.2198e-04 - acc: 1.0000 - val_loss: 0.1702 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.0704e-04 - acc: 1.0000 - val_loss: 0.1726 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.4490e-05 - acc: 1.0000 - val_loss: 0.1751 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 8.3502e-05 - acc: 1.0000 - val_loss: 0.1774 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 7.4014e-05 - acc: 1.0000 - val_loss: 0.1797 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 6.5577e-05 - acc: 1.0000 - val_loss: 0.1821 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.8259e-05 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 5.1701e-05 - acc: 1.0000 - val_loss: 0.1871 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.5930e-05 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.0814e-05 - acc: 1.0000 - val_loss: 0.1920 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.6248e-05 - acc: 1.0000 - val_loss: 0.1945 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.2222e-05 - acc: 1.0000 - val_loss: 0.1970 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.8600e-05 - acc: 1.0000 - val_loss: 0.1996 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.5364e-05 - acc: 1.0000 - val_loss: 0.2020 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.2497e-05 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.9926e-05 - acc: 1.0000 - val_loss: 0.2069 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.7646e-05 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.5631e-05 - acc: 1.0000 - val_loss: 0.2115 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.3827e-05 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.2214e-05 - acc: 1.0000 - val_loss: 0.2166 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.0797e-05 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.5555e-06 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 8.4535e-06 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 7.4818e-06 - acc: 1.0000 - val_loss: 0.2269 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.6251e-06 - acc: 1.0000 - val_loss: 0.2294 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.8649e-06 - acc: 1.0000 - val_loss: 0.2319 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.1860e-06 - acc: 1.0000 - val_loss: 0.2346 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.5859e-06 - acc: 1.0000 - val_loss: 0.2371 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.0577e-06 - acc: 1.0000 - val_loss: 0.2397 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.5893e-06 - acc: 1.0000 - val_loss: 0.2425 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.1776e-06 - acc: 1.0000 - val_loss: 0.2453 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.8149e-06 - acc: 1.0000 - val_loss: 0.2482 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.4956e-06 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.2142e-06 - acc: 1.0000 - val_loss: 0.2536 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.9652e-06 - acc: 1.0000 - val_loss: 0.2564 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.7452e-06 - acc: 1.0000 - val_loss: 0.2593 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.5513e-06 - acc: 1.0000 - val_loss: 0.2620 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.3805e-06 - acc: 1.0000 - val_loss: 0.2648 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.2288e-06 - acc: 1.0000 - val_loss: 0.2677 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0948e-06 - acc: 1.0000 - val_loss: 0.2705 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.7596e-07 - acc: 1.0000 - val_loss: 0.2734 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 8.7077e-07 - acc: 1.0000 - val_loss: 0.2763 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 7.7780e-07 - acc: 1.0000 - val_loss: 0.2792 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 6.9515e-07 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 6.2163e-07 - acc: 1.0000 - val_loss: 0.2851 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.5604e-07 - acc: 1.0000 - val_loss: 0.2881 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 4.9785e-07 - acc: 1.0000 - val_loss: 0.2909 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.4638e-07 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 4.0049e-07 - acc: 1.0000 - val_loss: 0.2967 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.5970e-07 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 3.2349e-07 - acc: 1.0000 - val_loss: 0.3022 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 2.9107e-07 - acc: 1.0000 - val_loss: 0.3050 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.6225e-07 - acc: 1.0000 - val_loss: 0.3077 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.3647e-07 - acc: 1.0000 - val_loss: 0.3104 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.1347e-07 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.9299e-07 - acc: 1.0000 - val_loss: 0.3158 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.7461e-07 - acc: 1.0000 - val_loss: 0.3184 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.5822e-07 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.4349e-07 - acc: 1.0000 - val_loss: 0.3235 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.3034e-07 - acc: 1.0000 - val_loss: 0.3261 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1843e-07 - acc: 1.0000 - val_loss: 0.3285 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.0785e-07 - acc: 1.0000 - val_loss: 0.3311 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.8266e-08 - acc: 1.0000 - val_loss: 0.3335 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 8.9734e-08 - acc: 1.0000 - val_loss: 0.3359 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 8.1984e-08 - acc: 1.0000 - val_loss: 0.3383 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 7.5081e-08 - acc: 1.0000 - val_loss: 0.3406 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 6.8810e-08 - acc: 1.0000 - val_loss: 0.3429 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 6.3179e-08 - acc: 1.0000 - val_loss: 0.3452 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.8093e-08 - acc: 1.0000 - val_loss: 0.3473 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.3460e-08 - acc: 1.0000 - val_loss: 0.3495 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 4.9298e-08 - acc: 1.0000 - val_loss: 0.3516 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 4.5485e-08 - acc: 1.0000 - val_loss: 0.3536 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.2053e-08 - acc: 1.0000 - val_loss: 0.3555 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.8970e-08 - acc: 1.0000 - val_loss: 0.3574 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.6137e-08 - acc: 1.0000 - val_loss: 0.3592 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.3568e-08 - acc: 1.0000 - val_loss: 0.3610 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.1245e-08 - acc: 1.0000 - val_loss: 0.3628 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.9094e-08 - acc: 1.0000 - val_loss: 0.3645 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.7173e-08 - acc: 1.0000 - val_loss: 0.3662 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.5407e-08 - acc: 1.0000 - val_loss: 0.3678 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.3780e-08 - acc: 1.0000 - val_loss: 0.3694 - val_acc: 0.7647\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.2272e-08 - acc: 1.0000 - val_loss: 0.3709 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.5757 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_SW = models.Sequential()\n",
    "modelFC_SW.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_SW.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_SW.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "args = y_train_softmax.astype(bool)\n",
    "y_pred = modelFC_SW.predict(X_train)\n",
    "samples = 1 - y_pred[args]\n",
    "for epoch in range(100):\n",
    "    modelFC_SW.fit(X_train, y_train_softmax, epochs=1, validation_split=0.25, sample_weight=samples)\n",
    "    y_pred = modelFC_SW.predict(X_train)\n",
    "    samples = 1 - y_pred[args]\n",
    "    \n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_SW.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570984a",
   "metadata": {},
   "source": [
    "### Pseudo-labeling\n",
    "\n",
    "Fuente: https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04feba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_epoch(epoch, val, start, stop):\n",
    "    if epoch < start:\n",
    "        alpha = 0\n",
    "    elif epoch < stop:\n",
    "        alpha = ((epoch-start) / (stop-start)) * val\n",
    "    else:\n",
    "        alpha = val\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7172a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step - loss: 1.4723 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.25\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6284 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6685 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.75\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6924 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 1.00\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha_values = [0.25, 0.5, 0.75, 1]\n",
    "iters = 100\n",
    "\n",
    "modelFC = models.Sequential()\n",
    "modelFC.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for i in range(iters):\n",
    "        pseudolabels = modelFC.predict(test_reduc).squeeze()\n",
    "        alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "        samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "        modelFC.fit(X_train, y_train_softmax, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy_pl = modelFC.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_pl * 100, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3114c",
   "metadata": {},
   "source": [
    "Vamos a pintar la evolución del valor del peso $\\alpha$ (función $\\alpha(t)$) elegido a lo largo de 100 iteraciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4838bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha_values = 1\n",
    "iters = 100\n",
    "\n",
    "alpha_per_epoch = []\n",
    "for i in range(iters):\n",
    "    alpha_per_epoch.append(alpha_epoch(i+1, alpha, start, stop))\n",
    "    \n",
    "# Pintamos el vector\n",
    "plt.plot(alpha_per_epoch)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(r\"$\\alpha(t)$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
