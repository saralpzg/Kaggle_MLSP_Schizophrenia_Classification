{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f259b",
   "metadata": {},
   "source": [
    "Este notebook recoge los resultados de una serie de métodos para tratar de mejorar el entrenamiento de una red neuronal o poder dar uso al conjunto de datos no etiquetado.\n",
    "\n",
    "Las pruebas se van a realizar sobre la configuración de red óptima encontrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40286d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de optimización de hiperparámetros\n",
    "import optuna\n",
    "\n",
    "# Modelo\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing, regularizers\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4b44",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3014e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231ee89",
   "metadata": {},
   "source": [
    "###  Modelo óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516cc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# En primer lugar, hay que adaptar los datos\n",
    "NUM_CLASSES = 2\n",
    "y_train_softmax = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_softmax = np_utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e4bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 359ms/step - loss: 0.6864 - acc: 0.5686 - val_loss: 0.6845 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5365 - acc: 0.7451 - val_loss: 0.7852 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4486 - acc: 0.7647 - val_loss: 0.7125 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2293 - acc: 0.9804 - val_loss: 0.7357 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0894 - acc: 1.0000 - val_loss: 0.9013 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.9585 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9094 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5813 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_opt = models.Sequential()\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_opt.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_opt.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_original = model_opt.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_original * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebafe",
   "metadata": {},
   "source": [
    "# Función de pérdida personalizada\n",
    "\n",
    "Se trata de tener en cuenta los datos con peores resultados en el backpropagation, es decir, asignar un mayor peso a esos datos, definiendo propiamente una función de loss personalizada.\n",
    "\n",
    "Fuente: https://stackoverflow.com/questions/48720197/weight-samples-if-incorrect-guessed-in-binary-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad055d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, tp_weight=0.5, tn_weight=0.5, fp_weight=1, fn_weight=1):\n",
    "    '''\n",
    "    Función de pérdida personalizada para el optimizador de una red neuronal.\n",
    "    El método recibe las predicciones y el valor real de la clasificación, así como los pesos que se desea asignar a los\n",
    "    clasificaciones tanto erróneas como acertadas.\n",
    "    '''\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_classes = tf.keras.backend.greater_equal(y_pred, 0.5)\n",
    "    y_pred_classes_float = tf.keras.backend.cast(y_pred_classes, tf.keras.backend.floatx())\n",
    "    y_true_float = tf.keras.backend.cast(y_true, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get misclassified examples\n",
    "    wrongly_classified = tf.keras.backend.not_equal(y_true_float, y_pred_classes_float)\n",
    "    wrongly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "    wrongly_classified_float2 = tf.gather(wrongly_classified_float, [0], axis=1)\n",
    "        \n",
    "    # Get correctly classified examples\n",
    "    correctly_classified = tf.keras.backend.equal(y_true_float, y_pred_classes_float)\n",
    "    correctly_classified_float = tf.keras.backend.cast(correctly_classified, tf.keras.backend.floatx())\n",
    "    correctly_classified_float2 = tf.gather(correctly_classified_float, [0], axis=1)\n",
    "    \n",
    "    # Get tp, fp, tn, fn\n",
    "    tp = correctly_classified_float * y_true_float\n",
    "    tn = correctly_classified_float * (1 - y_true_float)\n",
    "    fp = wrongly_classified_float * y_true_float\n",
    "    fn = wrongly_classified_float * (1 - y_true_float)\n",
    "\n",
    "    # Get weights\n",
    "    weight = tp_weight * tp + fp_weight * fp + tn_weight * tn + fn_weight * fn\n",
    "    weight2 = tf.gather(weight, [0], axis=1)\n",
    "    weight3 = tf.math.reduce_sum(weight2, axis=1)\n",
    "    \n",
    "    loss = tf.keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weight3\n",
    "        \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73f781",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que los anteriores ``tp``, ``fn``, ... son tensores (vectores de ``tensorflow``) y por tanto, el resultado guardado en ``weight`` es igualmente otro tensor, no un único valor numérico.\n",
    "\n",
    "_NOTA: se entienden los positivos como aquellas muestras correspondientes con una etiqueta \"1\" (enfermos) y por tanto como negativos los registros con etiqueta \"0\" (grupo de control)._\n",
    "\n",
    "Vamos a probar cómo funciona la función personalizada sobre los modelos de redes neuronales que peores resultados han obtenido para comprobar si de este modo podemos mejorar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb9f5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 164ms/step - loss: 0.5316 - acc: 0.5686 - val_loss: 0.4673 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3406 - acc: 0.9020 - val_loss: 0.5205 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2365 - acc: 0.9804 - val_loss: 0.5716 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1206 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3440 - acc: 0.7843 - val_loss: 1.0695 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1280 - acc: 0.9412 - val_loss: 0.7024 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.7442 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.7961 - val_acc: 0.5882\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.8185 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4409 - acc: 0.8333\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracyCL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Precisión en partición de test\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss, accuracy_CL \u001b[38;5;241m=\u001b[39m model_CL\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test_softmax)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{:0.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43maccuracyCL\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracyCL' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_CL = models.Sequential()\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_CL.compile(loss=custom_loss, optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_CL.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_CL = model_CL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracyCL * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a69621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_CL = model_CL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_CL, \"NN_CustomLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16fe8",
   "metadata": {},
   "source": [
    "# Sample-weight\n",
    "\n",
    "Una alternativa a lo anterior, sería usar un vector de pesos propio y pasarlo a la función ``fit`` del modelo, a través del parámetro ``sample_weight``, en lugar de definir una función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065bb881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 249ms/step - loss: 0.3475 - acc: 0.5686 - val_loss: 0.3414 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2470 - acc: 0.5098 - val_loss: 0.2745 - val_acc: 0.4118\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3152 - acc: 0.6471 - val_loss: 0.2306 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2156 - acc: 0.7059 - val_loss: 0.2567 - val_acc: 0.5294\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1106 - acc: 0.9608 - val_loss: 0.3408 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.3547 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.3733 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.3857 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3920 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3987 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4053 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4121 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4256 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.4525e-04 - acc: 1.0000 - val_loss: 0.4327 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 7.3577e-04 - acc: 1.0000 - val_loss: 0.4398 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.7974e-04 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 4.6185e-04 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.7296e-04 - acc: 1.0000 - val_loss: 0.4581 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.0341e-04 - acc: 1.0000 - val_loss: 0.4635 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.4874e-04 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0503e-04 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.6953e-04 - acc: 1.0000 - val_loss: 0.4818 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.4058e-04 - acc: 1.0000 - val_loss: 0.4879 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.1711e-04 - acc: 1.0000 - val_loss: 0.4939 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.7745e-05 - acc: 1.0000 - val_loss: 0.4996 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 8.1903e-05 - acc: 1.0000 - val_loss: 0.5055 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 6.8878e-05 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 5.8192e-05 - acc: 1.0000 - val_loss: 0.5162 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 4.9310e-05 - acc: 1.0000 - val_loss: 0.5215 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.1931e-05 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 3.5722e-05 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 3.0528e-05 - acc: 1.0000 - val_loss: 0.5370 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.6139e-05 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.2429e-05 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.9267e-05 - acc: 1.0000 - val_loss: 0.5518 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 1.6596e-05 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.4319e-05 - acc: 1.0000 - val_loss: 0.5605 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.2385e-05 - acc: 1.0000 - val_loss: 0.5645 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0727e-05 - acc: 1.0000 - val_loss: 0.5684 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 9.3051e-06 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 8.0796e-06 - acc: 1.0000 - val_loss: 0.5762 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 7.0269e-06 - acc: 1.0000 - val_loss: 0.5802 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 6.1198e-06 - acc: 1.0000 - val_loss: 0.5840 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 5.3400e-06 - acc: 1.0000 - val_loss: 0.5880 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.6716e-06 - acc: 1.0000 - val_loss: 0.5918 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.0940e-06 - acc: 1.0000 - val_loss: 0.5958 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.5943e-06 - acc: 1.0000 - val_loss: 0.5993 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 3.1564e-06 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 2.7761e-06 - acc: 1.0000 - val_loss: 0.6066 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.4437e-06 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.1534e-06 - acc: 1.0000 - val_loss: 0.6142 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.8983e-06 - acc: 1.0000 - val_loss: 0.6177 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.6762e-06 - acc: 1.0000 - val_loss: 0.6213 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.4832e-06 - acc: 1.0000 - val_loss: 0.6243 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.3140e-06 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1662e-06 - acc: 1.0000 - val_loss: 0.6308 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.0358e-06 - acc: 1.0000 - val_loss: 0.6341 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 9.2125e-07 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 8.2014e-07 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 7.3192e-07 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.5402e-07 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.8538e-07 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.2465e-07 - acc: 1.0000 - val_loss: 0.6509 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.7095e-07 - acc: 1.0000 - val_loss: 0.6534 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.2346e-07 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 69ms/step - loss: 3.8142e-07 - acc: 1.0000 - val_loss: 0.6581 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 3.4407e-07 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 3.1094e-07 - acc: 1.0000 - val_loss: 0.6625 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.8134e-07 - acc: 1.0000 - val_loss: 0.6647 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.5497e-07 - acc: 1.0000 - val_loss: 0.6667 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.3148e-07 - acc: 1.0000 - val_loss: 0.6686 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.1037e-07 - acc: 1.0000 - val_loss: 0.6706 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.9153e-07 - acc: 1.0000 - val_loss: 0.6725 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.7461e-07 - acc: 1.0000 - val_loss: 0.6743 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 1.5947e-07 - acc: 1.0000 - val_loss: 0.6761 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 1.4580e-07 - acc: 1.0000 - val_loss: 0.6780 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.3357e-07 - acc: 1.0000 - val_loss: 0.6797 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.2249e-07 - acc: 1.0000 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1253e-07 - acc: 1.0000 - val_loss: 0.6833 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0349e-07 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 9.5368e-08 - acc: 1.0000 - val_loss: 0.6867 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 8.7987e-08 - acc: 1.0000 - val_loss: 0.6886 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.1333e-08 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 7.5265e-08 - acc: 1.0000 - val_loss: 0.6919 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 6.9781e-08 - acc: 1.0000 - val_loss: 0.6934 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.4738e-08 - acc: 1.0000 - val_loss: 0.6950 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 6.0196e-08 - acc: 1.0000 - val_loss: 0.6966 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.6040e-08 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.2261e-08 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.8824e-08 - acc: 1.0000 - val_loss: 0.7013 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.5657e-08 - acc: 1.0000 - val_loss: 0.7028 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.2732e-08 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.0080e-08 - acc: 1.0000 - val_loss: 0.7058 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.7669e-08 - acc: 1.0000 - val_loss: 0.7073 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.5415e-08 - acc: 1.0000 - val_loss: 0.7087 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.3342e-08 - acc: 1.0000 - val_loss: 0.7102 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.1413e-08 - acc: 1.0000 - val_loss: 0.7116 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.9671e-08 - acc: 1.0000 - val_loss: 0.7130 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5105 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_SW = models.Sequential()\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_SW.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "args = y_train_softmax.astype(bool)\n",
    "y_pred = model_SW.predict(X_train)\n",
    "samples = 1 - y_pred[args]\n",
    "for epoch in range(100):\n",
    "    model_SW.fit(X_train, y_train_softmax, epochs=1, validation_split=0.25, sample_weight=samples)\n",
    "    y_pred = model_SW.predict(X_train)\n",
    "    samples = 1 - y_pred[args]\n",
    "    \n",
    "# Precisión en partición de test\n",
    "loss, accuracy_SW = model_SW.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_SW * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0965d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_SW = model_SW.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_SW, \"NN_SampleWeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570984a",
   "metadata": {},
   "source": [
    "# Pseudo-labeling\n",
    "\n",
    "Finalmente, esta técnica va a permitir aprovechar el conjunto de datos no etiquetado (método de aprendizaje semi-automático).\n",
    "\n",
    "Fuente: https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04feba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_epoch(epoch, val, start, stop):\n",
    "    if epoch < start:\n",
    "        alpha = 0\n",
    "    elif epoch < stop:\n",
    "        alpha = ((epoch-start) / (stop-start)) * val\n",
    "    else:\n",
    "        alpha = val\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba7172a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step - loss: 1.3332 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.25\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4707 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5044 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.75\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5247 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 1.00\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha_values = [0.25, 0.5, 0.75, 1]\n",
    "iters = 100\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for i in range(iters):\n",
    "        pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "        alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "        samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "        model_PL.fit(X_train, y_train_softmax, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3114c",
   "metadata": {},
   "source": [
    "Vamos a pintar la evolución del valor del peso $\\alpha$ (función $\\alpha(t)$) elegido a lo largo de 100 iteraciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4838bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha = 0.5\n",
    "iters = 100\n",
    "\n",
    "alpha_per_epoch = []\n",
    "for i in range(iters):\n",
    "    alpha_per_epoch.append(alpha_epoch(i+1, alpha, start, stop))\n",
    "    \n",
    "# Pintamos el vector\n",
    "plt.plot(alpha_per_epoch)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(r\"$\\alpha(t)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda9ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step - loss: 1.3332 - acc: 0.8889\n",
      "Accuracy: 88.89% ------- alpha = 0.50\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "\n",
    "alpha=0.5\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for i in range(iters):\n",
    "    pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "    alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "    samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "    model_PL.fit(X_train, y_train_softmax, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e13294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_PL = model_PL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_PL, \"NN_PseudoLabels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a0af6",
   "metadata": {},
   "source": [
    "# Comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6fd758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_acc = [accuracy_original, accuracy_CL, accuracy_SW, accuracy_PL]\n",
    "xaxis = [\"Accuracy en red neuronal \\n óptima (original)\", \"Accuracy con función de\\n pérdida personalizada\", \n",
    "         \"Accuracy con sample-weight\", \"Accuracy con pseudo-labels\"]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "barlist = plt.bar(xaxis, results_acc, width=0.2)\n",
    "barlist[0].set_color(\"r\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylim(bottom=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
