{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f259b",
   "metadata": {},
   "source": [
    "# Optimización de un modelo de red neuronal (fully-connected)\n",
    "\n",
    "Este notebook recoge los resultados de la búsqueda del mejor modelo de clasificación mediante una red neuronal densa o fully-connected, ya que el uso de redes convolucionales no parece adecuado para un problema de datos tabulares.\n",
    "\n",
    "Para buscar el mejor modelo posible, se tratará de buscar los mejores hiperparámetros para el número de capas ocultas de la red, su anchura (número de neuronas), posible introducción de términos de regularización, optimizadores, ...\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40286d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter tunning libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Accuracy function\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f89a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"../data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"../data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on='Id', right_on='Id')\n",
    "data = pd.merge(left=train_labels, right=train, left_on='Id', right_on='Id')\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4b44",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3014e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e85fa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"../data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"../data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test = pd.merge(left=testFNC, right=testSBM, left_on='Id', right_on='Id')\n",
    "test.drop(\"Id\", inplace=True, axis=1)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61745a4",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb208b",
   "metadata": {},
   "source": [
    "Para redes neuronales, compararemos los resultados obtenidos construyendo redes a partir de librerías distintas.\n",
    "\n",
    "**Comenzamos con ``MLPClassifier`` de ``sklearn`` y búsqueda de hiperparámetros con ``GridSearchCV``:**\n",
    "\n",
    "Documentación: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675c7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59a970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, param_grid):\n",
    "    '''Función para realizar el entrenamiento y la búsqueda de hiperparámetros'''\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=4)\n",
    "    # cv = 4 porque así: el conjunto de validation tiene un 0.25 del tamaño de train y: 0.25 * 0.8 = 0.2 ~ 20% datos\n",
    "    #                    el conjunto de train tiene un 0.75 del tamaño de train y: 0.75 * 0.8 = 0.6 ~60% datos\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Parámetros óptimos:\", grid_search.best_params_)\n",
    "    print(\"Modelo óptimo:\", grid_search.best_estimator_)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b4b39",
   "metadata": {},
   "source": [
    "El método debe recibir arquitecturas de red pre-definidas, por lo que probaremos topologías variadas en cuanto a profundidad, ancho y número de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d163caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'relu', 'hidden_layer_sizes': (100, 250, 250, 100, 1), 'solver': 'adam'}\n",
      "Modelo óptimo: MLPClassifier(hidden_layer_sizes=(100, 250, 250, 100, 1), max_iter=1000,\n",
      "              random_state=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1000, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], # Tiene sentido probar identity y relu?\n",
    "    \"solver\": [\"sgd\", \"adam\"] # solver \"lbfgs\" no permite hacer los plots más abajo\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fdd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_MLPC_opt = MLPClassifier(activation=\"relu\", hidden_layer_sizes=(100, 250, 250, 100, 1), solver=\"adam\",\n",
    "                               validation_fraction=0.25, max_iter=1000, random_state=0)\n",
    "model_MLPC_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_MLPC = model_MLPC_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_MLPC)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6075466",
   "metadata": {},
   "source": [
    "Las anteriores celdas de código muestran un \"warning\" indicando que el método de computación alcanza el número máximo de iteraciones (épocas de entrenamiento) sin haber llegado a converger.\n",
    "\n",
    "El criterio para considerar que ha habido convergencia está definido en la documentación como: la reducción del valor de la función de loss es inferior a 1e-4 por un número de etapas determinado. \n",
    "\n",
    "La anterior condición por tanto no se está cumpliendo en nuestro entrenamiento, así que vamos a pintar la evolución de la función de pérdida para determinar si estamos cortando el entrenamiento demasiado pronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a417d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(model_MLPC_opt.loss_curve_)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3d32f",
   "metadata": {},
   "source": [
    "La anterior curva de loss parece tener una tendencia aún claramente descendiente cuando es cortada en la época número 1000. Se va a repetir el entrenamiento incrementando este valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c41d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'identity', 'hidden_layer_sizes': (200, 200, 100, 50, 1), 'solver': 'adam'}\n",
      "Modelo óptimo: MLPClassifier(activation='identity', hidden_layer_sizes=(200, 200, 100, 50, 1),\n",
      "              max_iter=1500, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1500, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], # Tiene sentido probar identity y relu?\n",
    "    \"solver\": [\"sgd\", \"adam\"]\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1039b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "model_MLPC_opt = MLPClassifier(activation=\"identity\", hidden_layer_sizes=(200, 200, 100, 50, 1), solver=\"adam\",\n",
    "                               validation_fraction=0.25, max_iter=1500, random_state=0)\n",
    "model_MLPC_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_MLPC = model_MLPC_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_MLPC)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c9b4ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_MLPC_opt.loss_curve_)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af707e",
   "metadata": {},
   "source": [
    "Al aumentar el número de épocas, los resultados varían y el modelo que se retorna ahora como óptimo necesita solamente 25 épocas para converger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14ede2",
   "metadata": {},
   "source": [
    "La librería ``sklearn`` permite implementar un tercer optimizador, lbfgs, que no se ha incluído en las pruebas anteriores ya que no permite dibujar la curva de loss para hacer las anteriores comprobaciones. Por tanto, vamos ahora a probar y comparar los resultados manualmente incluyendo este optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f906f2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'relu', 'hidden_layer_sizes': (100, 250, 250, 100, 1), 'solver': 'adam'}\n",
      "Modelo óptimo: MLPClassifier(hidden_layer_sizes=(100, 250, 250, 100, 1), max_iter=1000,\n",
      "              random_state=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1000, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], # Tiene sentido probar identity y relu?\n",
    "    \"solver\": [\"sgd\", \"adam\", \"lbfgs\"]\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f027b",
   "metadata": {},
   "source": [
    "Si volvemos a utilizar el máximo de 1000 iteraciones, el óptimo es el mismo obtenido en el primer caso. Vamos a repetir el intento ahora con 1500 épocas de máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83e7f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'tanh', 'hidden_layer_sizes': (200, 200, 100, 50, 1), 'solver': 'lbfgs'}\n",
      "Modelo óptimo: MLPClassifier(activation='tanh', hidden_layer_sizes=(200, 200, 100, 50, 1),\n",
      "              max_iter=1500, random_state=0, solver='lbfgs')\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1500, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\", \"lbfgs\"]\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df63b0",
   "metadata": {},
   "source": [
    "Al aumentar las iteraciones, ahora el modelo óptimo sí toma \"lbfgs\" como algoritmo de optimización. Por tanto, debemos asegurarnos de que el modelo previo entrenado sobre 1500 épocas sea mejor que este otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e40fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "model_MLPC_opt = MLPClassifier(activation=\"tanh\", hidden_layer_sizes=(200, 200, 100, 50, 1), solver='lbfgs',\n",
    "                               validation_fraction=0.25, max_iter=1500, random_state=0)\n",
    "model_MLPC_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_MLPC = model_MLPC_opt.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_MLPC)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c605c9",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{CÓMO COMPRUEBO CUAL ES MEJOR?}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d91c8",
   "metadata": {},
   "source": [
    "Por último, vamos a introducir un nuevo parámetro a considerar en las pruebas de optimización:  ``learning_rate=\"adaptive\"``. L idea es mantener el valor del learning_rate constante mientras la curva de pérdida siga decreciendo, en el momento en que haya dos éocas consecutivas en las que no decrece un mínimo el valos de loss, el learning_rate se divide entre 5. Este parámetro sólo actúa cuando el solver es ``sgd``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7fbae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'relu', 'hidden_layer_sizes': (100, 250, 250, 100, 1), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "Modelo óptimo: MLPClassifier(hidden_layer_sizes=(100, 250, 250, 100, 1),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1000, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\", \"lbfgs\"],\n",
    "    \"learning_rate\": [\"adaptive\"]\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "562e37b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saral\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros óptimos: {'activation': 'tanh', 'hidden_layer_sizes': (200, 200, 100, 50, 1), 'learning_rate': 'adaptive', 'solver': 'lbfgs'}\n",
      "Modelo óptimo: MLPClassifier(activation='tanh', hidden_layer_sizes=(200, 200, 100, 50, 1),\n",
      "              learning_rate='adaptive', max_iter=1500, random_state=0,\n",
      "              solver='lbfgs')\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "model_MLPC = MLPClassifier(max_iter=1500, random_state=0)\n",
    "param_grid_MLPC = {\n",
    "    \"hidden_layer_sizes\": [(100, 200, 100, 1), (100, 100, 100, 100, 1), (200, 200, 100, 50, 1), (100, 250, 250, 100, 1)],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\", \"lbfgs\"],\n",
    "    \"learning_rate\": [\"adaptive\"]\n",
    "}\n",
    "model_MLPC_opt = train_model(model_MLPC, param_grid_MLPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ef953",
   "metadata": {},
   "source": [
    "Aún introduciendo este cambio, los óptimos son los mismos que los que se han obtenido anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b5a47",
   "metadata": {},
   "source": [
    "**Usando la librería \"keras\"**\n",
    "\n",
    "Ahora utilizaremos la librería de ``keras``, por su mayor flexibilidad para intentar mejorar los resultados de la red neuronal.\n",
    "\n",
    "Comenzaremos repitiendo la búsqueda de hiperparámetros, ya que la propia librería de ``keras`` dispone de integración con otras que nos permitirán hacer una búsqueda algo más exhaustiva por ejemplo en cuanto al número de capas y neuronas en estas. Concretamente, vamos a utilizar ``optuna``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c04e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be2b93",
   "metadata": {},
   "source": [
    "Documentación:\n",
    "* https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html\n",
    "* https://optuna.org/\n",
    "\n",
    "Para reducir el coste computacional tomaremos de base resultados como la función de activación óptima: \"relu\", que hemos podido obtener con ``GridSearchCV``. Como por el contrario usando ``sklearn`` no hemos podido utilizar el optimizador RMSProp, vamos a probarlo también con ``optuna`` + ``keras`` para ver si mejora nuestros resultados.\n",
    "\n",
    "Búsqueda mediante la librería ``optuna`` probando 2 métodos de búsqueda de hiperparámetros:\n",
    "\n",
    "* **GridSampler:** equivalente a la anterior búsqueda de grid de sklearn. Lo usaremos para que los resultados sean comparables.\n",
    "* **TPE:** algoritmo para hacer una \"búsqueda inteligente\" de hiperparámetros. Debería ahorrar intentos de combinaciones haciendo una selección inteligente de las pruebas. En nuestro caso le permitiremos probar un 10% del número de combinaciones posibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1709fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El objetivo es definir una función que será optimizada. En este caso, nos interesa maximizar el accuracy.\n",
    "def objectiveNN_Grid(trial):\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.5)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "        modelFC_optuna.add(layers.Dropout(rate=dropout))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea2fd41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# Prueba con GridSampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "search_space = {\"n_layers\": range(2, 6), \n",
    "                \"n_units\": range(50, 300, 50),\n",
    "                \"dropout\": np.arange(0, 0.6, 0.1),\n",
    "                \"optimizer\": [\"RMSprop\", \"SGD\", \"Adam\"]\n",
    "               }\n",
    "sampler = optuna.samplers.GridSampler(search_space)\n",
    "study_Grid = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_Grid.optimize(objectiveNN_Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2243d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=277, values=[0.6666666865348816], datetime_start=datetime.datetime(2022, 6, 12, 17, 37, 54, 881045), datetime_complete=datetime.datetime(2022, 6, 12, 17, 37, 55, 862629), params={'n_layers': 3, 'n_units': 50, 'dropout': 0.1, 'optimizer': 'SGD'}, distributions={'n_layers': IntUniformDistribution(high=5, low=1, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=1), 'dropout': UniformDistribution(high=0.5, low=0.0), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={'search_space': OrderedDict([('dropout', [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5]), ('n_layers', [2, 3, 4, 5]), ('n_units', [50, 100, 150, 200, 250]), ('optimizer', ['Adam', 'RMSprop', 'SGD'])]), 'grid_id': 77}, intermediate_values={}, trial_id=277, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_Grid.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d211270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 212ms/step - loss: 0.7053 - acc: 0.5294 - val_loss: 0.6789 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6629 - acc: 0.6471 - val_loss: 0.6771 - val_acc: 0.7059\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6776 - acc: 0.5686 - val_loss: 0.6756 - val_acc: 0.7647\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6843 - acc: 0.5686 - val_loss: 0.6741 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6676 - acc: 0.6471 - val_loss: 0.6731 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7049 - acc: 0.4706 - val_loss: 0.6720 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6754 - acc: 0.5882 - val_loss: 0.6716 - val_acc: 0.7059\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6886 - acc: 0.5490 - val_loss: 0.6705 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6743 - acc: 0.6111\n",
      "Accuracy: 61.11%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna = models.Sequential()\n",
    "modelFC_optuna.add(layers.Dense(50, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna.add(layers.Dropout(0.1))\n",
    "modelFC_optuna.add(layers.Dense(50, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.1))\n",
    "modelFC_optuna.add(layers.Dense(50, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.1))\n",
    "modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna.compile(loss=\"binary_crossentropy\", optimizer=\"SGD\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "# modelFC_optuna_Adam.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb2cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El objetivo es definir una función que será optimizada. En este caso, nos interesa maximizar el accuracy.\n",
    "def objectiveNN_TPE(trial):\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    # Se utiliza el objeto \"trial\" para asignar las posibilidades a los hiperparámetros.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.5, step=0.1)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "        modelFC_optuna.add(layers.Dropout(rate=dropout))\n",
    "    modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6846d5f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objeto \"study\" y buscamos la optimización de la función objetivo.\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_TPE.optimize(objectiveNN_TPE, n_trials=36)\n",
    "# n_trials = (4 x 5 x 6 x 3) * 0.1 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c88551f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[0.4444444477558136], datetime_start=datetime.datetime(2022, 6, 12, 17, 42, 44, 509225), datetime_complete=datetime.datetime(2022, 6, 12, 17, 42, 45, 603461), params={'n_layers': 4, 'n_units': 200, 'dropout': 0.30000000000000004, 'optimizer': 'Adam'}, distributions={'n_layers': IntUniformDistribution(high=5, low=2, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=50), 'dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_TPE.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2db5ef",
   "metadata": {},
   "source": [
    "La red con los parámetros optimizados es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d14b938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 140ms/step - loss: 0.6907 - acc: 0.5686 - val_loss: 0.7020 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6908 - acc: 0.5490 - val_loss: 0.6967 - val_acc: 0.4118\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6219 - acc: 0.7255 - val_loss: 0.6908 - val_acc: 0.3529\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6492 - acc: 0.6471 - val_loss: 0.6842 - val_acc: 0.5294\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6124 - acc: 0.6275 - val_loss: 0.6722 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5017 - acc: 0.8235 - val_loss: 0.6597 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5774 - acc: 0.6667 - val_loss: 0.6501 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4641 - acc: 0.8627 - val_loss: 0.6373 - val_acc: 0.5294\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3752 - acc: 0.9216 - val_loss: 0.6285 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3086 - acc: 0.9608 - val_loss: 0.6302 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3575 - acc: 0.9216 - val_loss: 0.6420 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5175 - acc: 0.7778\n",
      "Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna = models.Sequential()\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna.add(layers.Dropout(0.3))\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.3))\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.3))\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.3))\n",
    "modelFC_optuna.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_optuna.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "# modelFC_optuna_Adam.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ba6ef",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{NO PUEDO PROBAR EL OPTUNASEARCHCV. YA HAGO CROSS-VALIDACIÓN CON EL validation_split}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8da59",
   "metadata": {},
   "source": [
    "Veamos si podemos obtener mejores resultados cambiando la última capa con activación sigmoide por una activación softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443bedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# En primer lugar, hay que adaptar los datos\n",
    "NUM_CLASSES = 2\n",
    "y_train_softmax = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_softmax = np_utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b767209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveSoftmax_Grid(trial):\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.5)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "        modelFC_optuna.add(layers.Dropout(rate=dropout))\n",
    "    modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    optimizers = trial.suggest_vcategorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train_softmax, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b70e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6547 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6449 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6111 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5904 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5599 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6706 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6895 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4799 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5490 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6689 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5502 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7562 - accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5363 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5776 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4996 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6074 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7019 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7200 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6738 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5408 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5706 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7120 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5463 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7139 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6348 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5336 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6299 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5307 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6162 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7036 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7769 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6990 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7014 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4851 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6771 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4479 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5930 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5248 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6738 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7208 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5008 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6985 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6546 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5350 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6066 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6931 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6752 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7103 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5665 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7647 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6713 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6732 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4674 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6109 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6379 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7421 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6998 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6914 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6942 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5642 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6736 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5879 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5032 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7053 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6896 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7041 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6241 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7011 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6643 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5807 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6519 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6948 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6273 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5372 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6734 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5873 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6888 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5800 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4946 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7096 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6953 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6521 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7230 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6884 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6655 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6789 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5914 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7522 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6524 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6274 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0000 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5678 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5553 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6192 - accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6938 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6166 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5253 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7160 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6839 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6028 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6803 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4411 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6909 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6737 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6908 - accuracy: 0.4444\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 0.6647 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7266 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7135 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6780 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7449 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6501 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7137 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6812 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7614 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6678 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6565 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6449 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6771 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5407 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6423 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7070 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6911 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4822 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5770 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5683 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2550 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6742 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6834 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7244 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7437 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5098 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6067 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6760 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4480 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6260 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5507 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7294 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6981 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6199 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5043 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7117 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6318 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6916 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6861 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7075 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6901 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6832 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5766 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6921 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5305 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6043 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7747 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7480 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6606 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6831 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6471 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5716 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5383 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6590 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9097 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5986 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6624 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5656 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7611 - accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6737 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6598 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6367 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6086 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5832 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6674 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6804 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7015 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6882 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6858 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6613 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6662 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6673 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6802 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6741 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5088 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5567 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6767 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6168 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5850 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6603 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7134 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6547 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6971 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5554 - accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6469 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6854 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5985 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5441 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5688 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5083 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7471 - accuracy: 0.2778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6560 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6716 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5557 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6734 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5212 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6369 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6272 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6969 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5507 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7112 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6947 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6897 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7016 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6904 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5488 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6939 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4995 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5603 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6605 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7056 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6225 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6880 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7502 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5864 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5804 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7390 - accuracy: 0.2222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6106 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8871 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4506 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6835 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7032 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6077 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5469 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6597 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6525 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5837 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8344 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6397 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7050 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4795 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4960 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6802 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6748 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7056 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8577 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6864 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6024 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6718 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5426 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4850 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5403 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5978 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5900 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6714 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7239 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6750 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6905 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6979 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6554 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4914 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5726 - accuracy: 0.8889\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6949 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6291 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6904 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5757 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5844 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6748 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8415 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7150 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6451 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6259 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7149 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6324 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6734 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5524 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7082 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6227 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7010 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7023 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7162 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6915 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5673 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5562 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6932 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6933 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7175 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5151 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4921 - accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5673 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5416 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6905 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7359 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7170 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6775 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9539 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6738 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6734 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7950 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7006 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5886 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6906 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6441 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4977 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6955 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5392 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4565 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5433 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6802 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6995 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6025 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6586 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6237 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4726 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7048 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6890 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5180 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6718 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5663 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6863 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6810 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6992 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6242 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6910 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5574 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6310 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6655 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6991 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6036 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6181 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6901 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5510 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6619 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6077 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6918 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6181 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5011 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6757 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6191 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7205 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7239 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6974 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4688 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7393 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6581 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5781 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9020 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7014 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8230 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5881 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6411 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6205 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4931 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5381 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6975 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6379 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6664 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6048 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5997 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5154 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7303 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6631 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5209 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5624 - accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "# Prueba con GridSampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "search_space = {\"n_layers\": range(2, 6), \n",
    "                \"n_units\": range(50, 300, 50),\n",
    "                \"dropout\": np.arange(0, 0.6, 0.1),\n",
    "                \"optimizer\": [\"RMSprop\", \"SGD\", \"Adam\"]\n",
    "               }\n",
    "sampler = optuna.samplers.GridSampler(search_space)\n",
    "studySoftmax_Grid = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "studySoftmax_Grid.optimize(objectiveSoftmax_Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "871e5374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=23, values=[0.8888888955116272], datetime_start=datetime.datetime(2022, 6, 15, 0, 26, 35, 76894), datetime_complete=datetime.datetime(2022, 6, 15, 0, 26, 36, 329205), params={'n_layers': 5, 'n_units': 150, 'dropout': 0.0, 'optimizer': 'RMSprop'}, distributions={'n_layers': IntUniformDistribution(high=5, low=1, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=1), 'dropout': UniformDistribution(high=0.5, low=0.0), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={'search_space': OrderedDict([('dropout', [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5]), ('n_layers', [2, 3, 4, 5]), ('n_units', [50, 100, 150, 200, 250]), ('optimizer', ['Adam', 'RMSprop', 'SGD'])]), 'grid_id': 52}, intermediate_values={}, trial_id=23, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studySoftmax_Grid.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5e3b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 159ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna = models.Sequential()\n",
    "modelFC_optuna.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_optuna.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91d41c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveSoftmax_TPE(trial):\n",
    "    modelFC_optuna = models.Sequential()\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5, 1)\n",
    "    num_hidden = trial.suggest_int(\"n_units\", 50, 250, 50)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.5, step=0.1)\n",
    "    for i in range(n_layers):\n",
    "        modelFC_optuna.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "        modelFC_optuna.add(layers.Dropout(rate=dropout))\n",
    "    modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\", \"Adam\"])\n",
    "    modelFC_optuna.compile(loss=\"categorical_crossentropy\", optimizer=optimizers, metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train_softmax, callbacks=[es], epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c552f735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6743 - accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6219 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7090 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4179 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6114 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5312 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7134 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7316 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7613 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7968 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6850 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5796 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6177 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6047 - accuracy: 0.6111\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6190 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4985 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7152 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7328 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7005 - accuracy: 0.4444\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6794 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9709 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7363 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5167 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5548 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5743 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6667 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5751 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4878 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7231 - accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5305 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6848 - accuracy: 0.7222\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6969 - accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0900 - accuracy: 0.8333\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5353 - accuracy: 0.7778\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4007 - accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "studySoftmax_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "studySoftmax_TPE.optimize(objectiveSoftmax_TPE, n_trials=36)\n",
    "# n_trials = (4 x 5 x 6 x 3) * 0.1 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdf62f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=5, values=[0.8333333134651184], datetime_start=datetime.datetime(2022, 6, 15, 0, 37, 43, 537182), datetime_complete=datetime.datetime(2022, 6, 15, 0, 37, 45, 28320), params={'n_layers': 3, 'n_units': 200, 'dropout': 0.2, 'optimizer': 'Adam'}, distributions={'n_layers': IntUniformDistribution(high=5, low=2, step=1), 'n_units': IntUniformDistribution(high=250, low=50, step=50), 'dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1), 'optimizer': CategoricalDistribution(choices=('RMSprop', 'SGD', 'Adam'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=5, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studySoftmax_TPE.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0ce7c",
   "metadata": {},
   "source": [
    "La red con los parámetros optimizados es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19d7dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 170ms/step - loss: 0.7059 - acc: 0.5294 - val_loss: 0.6895 - val_acc: 0.5294\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6473 - acc: 0.7059 - val_loss: 0.6883 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6034 - acc: 0.6863 - val_loss: 0.6828 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5779 - acc: 0.7451 - val_loss: 0.6710 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5034 - acc: 0.8235 - val_loss: 0.6616 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4033 - acc: 0.9608 - val_loss: 0.6583 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3817 - acc: 0.9020 - val_loss: 0.6608 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3050 - acc: 0.9804 - val_loss: 0.6612 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5301 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_optuna = models.Sequential()\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_optuna.add(layers.Dropout(0.2))\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.2))\n",
    "modelFC_optuna.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_optuna.add(layers.Dropout(0.2))\n",
    "modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "modelFC_optuna.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "modelFC_optuna.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa31a6",
   "metadata": {},
   "source": [
    "Obtenemos un mayor accuracy utilizando la activación softmax. Sin embargo, parece que ahora la red tiene un problema de sobreajuste, ya que por ejemplo en la última época hay una diferencia entre la precisión en train y en validación del 30% aproximadamente. Incluso, desde la 6ª etapa, la red clasifica los datos en train con una precisión del 100%. Vamos a tratar de reducir esta diferencia probando distintos tipos de regularización. $\\color{red}{\\text{REVISAR}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddc9d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 182ms/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5491 - acc: 0.7255 - val_loss: 0.5449 - val_acc: 0.8235\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2874 - acc: 0.9608 - val_loss: 0.4808 - val_acc: 0.8235\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2832 - acc: 0.8824 - val_loss: 1.4154 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4606 - acc: 0.8431 - val_loss: 0.4541 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.7059\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4612 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4476 - acc: 0.8889\n",
      "Accuracy : 88.89% ----- Regularización: None \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 189ms/step - loss: 0.6908 - acc: 0.5490 - val_loss: 0.6602 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5952 - acc: 0.6471 - val_loss: 0.6676 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4646 - acc: 0.8039 - val_loss: 0.7404 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3168 - acc: 0.8824 - val_loss: 0.6688 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1293 - acc: 1.0000 - val_loss: 0.9230 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0666 - acc: 1.0000 - val_loss: 0.8326 - val_acc: 0.7647\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.5416 - val_acc: 0.7059\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.9422 - val_acc: 0.7647\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.8858 - val_acc: 0.7647\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6068 - acc: 0.8333\n",
      "Accuracy : 83.33% ----- Regularización: l1 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.6851 - acc: 0.5686 - val_loss: 0.6678 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5931 - acc: 0.6667 - val_loss: 0.7296 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4681 - acc: 0.7647 - val_loss: 0.8234 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3093 - acc: 0.9412 - val_loss: 0.8019 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1507 - acc: 0.9804 - val_loss: 1.1311 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0768 - acc: 1.0000 - val_loss: 0.7415 - val_acc: 0.5294\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.6976 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.8220 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.8327 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8160 - acc: 0.7222\n",
      "Accuracy : 72.22% ----- Regularización: l2 \n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.6893 - acc: 0.4510 - val_loss: 0.6878 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5906 - acc: 0.6471 - val_loss: 0.8122 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5085 - acc: 0.7255 - val_loss: 0.6951 - val_acc: 0.6471\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.2444 - acc: 1.0000 - val_loss: 0.7203 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0699 - acc: 1.0000 - val_loss: 1.0157 - val_acc: 0.7059\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0283 - acc: 1.0000 - val_loss: 1.0366 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.9169 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.1239 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.0979 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.1169 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.1226 - acc: 0.7222\n",
      "Accuracy : 72.22% ----- Regularización: l1_l2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "regularizers = [None, \"l1\", \"l2\", \"l1_l2\"]\n",
    "\n",
    "for regularizer in regularizers:\n",
    "    modelFC_optuna = models.Sequential()\n",
    "    modelFC_optuna.add(layers.Dense(150, activation=\"relu\", input_shape=(410,)))\n",
    "    modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(150, activation=\"relu\"))\n",
    "    modelFC_optuna.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    modelFC_optuna.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "    modelFC_optuna.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy = modelFC_optuna.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy : {:0.2f}% ----- Regularización: {} \\n\".format(accuracy * 100, regularizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebafe",
   "metadata": {},
   "source": [
    "Otra posibilidad es tratar de tener en cuenta los datos con peores resultados en el backpropagation, es decir, asignar un mayor peso a esos datos. Esta es la idea del método \"AdaBoost\" que sin embargo no está actualmente implementado en keras. La alternativa será definir propiamente otra función de loss dentro de por ejemplo el optimizador Adam (primera configuración de red neuronal optimizada con ``optuna`` anteriormente).\n",
    "\n",
    "Fuente: https://stackoverflow.com/questions/48720197/weight-samples-if-incorrect-guessed-in-binary-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad055d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, tp_weight=0.5, tn_weight=0.5, fp_weight=1, fn_weight=1):\n",
    "    '''\n",
    "    Función de pérdida personalizada para el optimizador de una red neuronal.\n",
    "    El método recibe las predicciones y el valor real de la clasificación, así como los pesos que se desea asignar a los\n",
    "    clasificaciones tanto erróneas como acertadas.\n",
    "    '''\n",
    "    # Get predictions\n",
    "    y_pred_classes = tf.keras.backend.greater_equal(y_pred, 0.5)\n",
    "    y_pred_classes_float = tf.keras.backend.cast(y_pred_classes, tf.keras.backend.floatx())\n",
    "    y_true_float = tf.keras.backend.cast(y_true, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get misclassified examples\n",
    "    wrongly_classified = tf.keras.backend.not_equal(y_true_float, y_pred_classes_float)\n",
    "    wrongly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get correctly classified examples\n",
    "    correctly_classified = tf.keras.backend.equal(y_true_float, y_pred_classes_float)\n",
    "    correctly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get tp, fp, tn, fn\n",
    "    tp = correctly_classified_float * y_true_float\n",
    "    tn = correctly_classified_float * (1 - y_true_float)\n",
    "    fp = wrongly_classified_float * y_true_float\n",
    "    fn = wrongly_classified_float * (1 - y_true_float)\n",
    "\n",
    "    # Get weights\n",
    "    weight = tp_weight * tp + fp_weight * fp + tn_weight * tn + fn_weight * fn\n",
    "\n",
    "    loss = tf.keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weight\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337200f",
   "metadata": {},
   "source": [
    "weight_tensor es un único valor numérico????? Todos los valores del vector weighted_loss son los mismos????\n",
    "$\\color{red}{\\text{REVISAR}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bb9f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.4190 - acc: 0.5686 - val_loss: 0.5201 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4364 - acc: 0.5490 - val_loss: 0.5733 - val_acc: 0.4118\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2555 - acc: 0.7059 - val_loss: 0.6253 - val_acc: 0.3529\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3604 - acc: 0.6078 - val_loss: 0.5639 - val_acc: 0.4118\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3278 - acc: 0.6275 - val_loss: 0.3897 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1135 - acc: 0.8431 - val_loss: 0.3289 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2608 - acc: 0.6863 - val_loss: 0.3236 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1220 - acc: 0.8235 - val_loss: 0.4251 - val_acc: 0.5294\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0451 - acc: 0.9216 - val_loss: 0.3675 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0209 - acc: 0.9608 - val_loss: 0.3649 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0590 - acc: 0.9020 - val_loss: 0.3619 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1660 - acc: 0.7778\n",
      "Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_CL = models.Sequential()\n",
    "modelFC_CL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_CL.add(layers.Dropout(0.3))\n",
    "modelFC_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dropout(0.3))\n",
    "modelFC_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dropout(0.3))\n",
    "modelFC_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_CL.add(layers.Dropout(0.3))\n",
    "modelFC_CL.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_CL.compile(loss=custom_loss, optimizer=\"adam\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=5)\n",
    "modelFC_CL.fit(X_train, y_train, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "# modelFC_CL.fit(X_train, y_train, epochs=100, validation_split=0.25, verbose=0)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_CL.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16fe8",
   "metadata": {},
   "source": [
    "Una alternativa a lo anterior, sería usar un vector de pesos propio y pasarlo a la función ``fit`` del modelo, a través del parámetro ``sample_weight``, en lugar de definir una función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "065bb881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 163ms/step - loss: 0.0102 - acc: 0.5686 - val_loss: 0.0103 - val_acc: 0.4706\n",
      "0.69013834\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3415 - acc: 0.5098 - val_loss: 0.3523 - val_acc: 0.4118\n",
      "0.69202477\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3244 - acc: 0.5686 - val_loss: 0.3483 - val_acc: 0.4706\n",
      "0.6966955\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2943 - acc: 0.6275 - val_loss: 0.3466 - val_acc: 0.5294\n",
      "0.70022327\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2792 - acc: 0.7059 - val_loss: 0.3446 - val_acc: 0.5294\n",
      "0.7028742\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2583 - acc: 0.6863 - val_loss: 0.3432 - val_acc: 0.5882\n",
      "0.7129002\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2265 - acc: 0.6863 - val_loss: 0.3401 - val_acc: 0.6471\n",
      "0.7295184\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1882 - acc: 0.7843 - val_loss: 0.3351 - val_acc: 0.6471\n",
      "0.7549407\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1359 - acc: 0.9412 - val_loss: 0.3378 - val_acc: 0.6471\n",
      "0.7964008\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1289 - acc: 0.8627 - val_loss: 0.3473 - val_acc: 0.6471\n",
      "0.8542236\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0972 - acc: 0.9020 - val_loss: 0.3671 - val_acc: 0.6471\n",
      "0.9361782\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0629 - acc: 0.9412 - val_loss: 0.3937 - val_acc: 0.6471\n",
      "1.0439306\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.4302 - val_acc: 0.7059\n",
      "1.1804743\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0280 - acc: 0.9608 - val_loss: 0.4737 - val_acc: 0.7059\n",
      "1.3337951\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.7647\n",
      "1.4943861\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0093 - acc: 0.9804 - val_loss: 0.5502 - val_acc: 0.7647\n",
      "1.6526217\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.5847 - val_acc: 0.7647\n",
      "1.8061091\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0055 - acc: 0.9608 - val_loss: 0.6079 - val_acc: 0.7647\n",
      "1.9450188\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6274 - val_acc: 0.7647\n",
      "2.070332\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 7.7626e-04 - acc: 1.0000 - val_loss: 0.6459 - val_acc: 0.6471\n",
      "2.1817927\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.2089e-04 - acc: 1.0000 - val_loss: 0.6598 - val_acc: 0.6471\n",
      "2.2784984\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 7.0284e-04 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.6471\n",
      "2.3623316\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.3361e-04 - acc: 0.9804 - val_loss: 0.6832 - val_acc: 0.6471\n",
      "2.4347396\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 3.7899e-04 - acc: 1.0000 - val_loss: 0.6953 - val_acc: 0.6471\n",
      "2.4984598\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 3.6450e-04 - acc: 1.0000 - val_loss: 0.7060 - val_acc: 0.6471\n",
      "2.5535796\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.3104e-04 - acc: 0.9804 - val_loss: 0.7170 - val_acc: 0.6471\n",
      "2.6017528\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.6141e-04 - acc: 1.0000 - val_loss: 0.7274 - val_acc: 0.6471\n",
      "2.6431575\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.6737e-04 - acc: 1.0000 - val_loss: 0.7367 - val_acc: 0.6471\n",
      "2.6788435\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.1359e-05 - acc: 1.0000 - val_loss: 0.7444 - val_acc: 0.6471\n",
      "2.708934\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.3711e-04 - acc: 1.0000 - val_loss: 0.7514 - val_acc: 0.6471\n",
      "2.7349355\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.4958e-05 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.6471\n",
      "2.7573476\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 6.6075e-05 - acc: 1.0000 - val_loss: 0.7641 - val_acc: 0.6471\n",
      "2.7769797\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.4535e-04 - acc: 0.9804 - val_loss: 0.7692 - val_acc: 0.6471\n",
      "2.7937303\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.0374e-04 - acc: 1.0000 - val_loss: 0.7743 - val_acc: 0.6471\n",
      "2.8084843\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.8564e-04 - acc: 0.9804 - val_loss: 0.7795 - val_acc: 0.6471\n",
      "2.8218236\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.1723e-04 - acc: 1.0000 - val_loss: 0.7828 - val_acc: 0.6471\n",
      "2.8329952\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 1.3794e-04 - acc: 1.0000 - val_loss: 0.7846 - val_acc: 0.6471\n",
      "2.8422399\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.0126e-04 - acc: 0.9804 - val_loss: 0.7815 - val_acc: 0.6471\n",
      "2.8476276\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 1.6931e-04 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.6471\n",
      "2.8539097\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.4287e-04 - acc: 1.0000 - val_loss: 0.7841 - val_acc: 0.6471\n",
      "2.860638\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.6230e-04 - acc: 1.0000 - val_loss: 0.7882 - val_acc: 0.6471\n",
      "2.867961\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8288e-04 - acc: 0.9804 - val_loss: 0.7888 - val_acc: 0.6471\n",
      "2.8726323\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 1.9575e-04 - acc: 0.9804 - val_loss: 0.7881 - val_acc: 0.6471\n",
      "2.8762455\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 1.7246e-04 - acc: 0.9804 - val_loss: 0.7886 - val_acc: 0.6471\n",
      "2.879699\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 5.7201e-05 - acc: 1.0000 - val_loss: 0.7889 - val_acc: 0.6471\n",
      "2.8828826\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 6.9980e-05 - acc: 1.0000 - val_loss: 0.7896 - val_acc: 0.6471\n",
      "2.8861215\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 8.9571e-05 - acc: 1.0000 - val_loss: 0.7905 - val_acc: 0.6471\n",
      "2.8894105\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.5291e-05 - acc: 1.0000 - val_loss: 0.7911 - val_acc: 0.6471\n",
      "2.8923259\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 4.6120e-05 - acc: 1.0000 - val_loss: 0.7917 - val_acc: 0.6471\n",
      "2.8950846\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 5.2022e-05 - acc: 0.9804 - val_loss: 0.7922 - val_acc: 0.6471\n",
      "2.8977046\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 5.8209e-05 - acc: 1.0000 - val_loss: 0.7924 - val_acc: 0.6471\n",
      "2.9001255\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 5.0428e-05 - acc: 1.0000 - val_loss: 0.7927 - val_acc: 0.6471\n",
      "2.9024866\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 5.8793e-05 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.6471\n",
      "2.904919\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.4016e-05 - acc: 1.0000 - val_loss: 0.7935 - val_acc: 0.6471\n",
      "2.9071345\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.1252e-05 - acc: 1.0000 - val_loss: 0.7936 - val_acc: 0.6471\n",
      "2.909204\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.6392e-05 - acc: 1.0000 - val_loss: 0.7937 - val_acc: 0.6471\n",
      "2.9111593\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.8521e-05 - acc: 1.0000 - val_loss: 0.7936 - val_acc: 0.6471\n",
      "2.9130914\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 6.0885e-05 - acc: 1.0000 - val_loss: 0.7940 - val_acc: 0.6471\n",
      "2.915128\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 5.4446e-05 - acc: 1.0000 - val_loss: 0.7942 - val_acc: 0.6471\n",
      "2.9170094\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.4725e-05 - acc: 1.0000 - val_loss: 0.7947 - val_acc: 0.6471\n",
      "2.9190557\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 7.8926e-05 - acc: 1.0000 - val_loss: 0.7943 - val_acc: 0.6471\n",
      "2.9207118\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 6.3758e-05 - acc: 1.0000 - val_loss: 0.7951 - val_acc: 0.6471\n",
      "2.9228535\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 6.0288e-05 - acc: 1.0000 - val_loss: 0.7953 - val_acc: 0.6471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9249766\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0255e-04 - acc: 1.0000 - val_loss: 0.7941 - val_acc: 0.6471\n",
      "2.9263673\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.9030e-05 - acc: 1.0000 - val_loss: 0.7935 - val_acc: 0.6471\n",
      "2.9278624\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 7.5053e-05 - acc: 1.0000 - val_loss: 0.7935 - val_acc: 0.6471\n",
      "2.9297209\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 8.0058e-05 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.6471\n",
      "2.931605\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.8808e-05 - acc: 1.0000 - val_loss: 0.7934 - val_acc: 0.6471\n",
      "2.9338124\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.3895e-05 - acc: 1.0000 - val_loss: 0.7940 - val_acc: 0.6471\n",
      "2.9362504\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.4607e-05 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.6471\n",
      "2.938061\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.0062e-04 - acc: 1.0000 - val_loss: 0.7944 - val_acc: 0.6471\n",
      "2.940914\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.6333e-05 - acc: 1.0000 - val_loss: 0.7956 - val_acc: 0.6471\n",
      "2.943716\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.5888e-04 - acc: 0.9804 - val_loss: 0.7975 - val_acc: 0.6471\n",
      "2.9469187\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.4718e-04 - acc: 0.9804 - val_loss: 0.7974 - val_acc: 0.6471\n",
      "2.9490042\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 7.7828e-05 - acc: 0.9804 - val_loss: 0.7980 - val_acc: 0.6471\n",
      "2.951558\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 4.1152e-05 - acc: 1.0000 - val_loss: 0.7985 - val_acc: 0.6471\n",
      "2.9540834\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 7.5593e-05 - acc: 0.9804 - val_loss: 0.7998 - val_acc: 0.6471\n",
      "2.957014\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.3110e-05 - acc: 1.0000 - val_loss: 0.8009 - val_acc: 0.6471\n",
      "2.9598267\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.8814e-05 - acc: 1.0000 - val_loss: 0.8017 - val_acc: 0.6471\n",
      "2.96237\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 4.1937e-05 - acc: 0.9804 - val_loss: 0.8024 - val_acc: 0.6471\n",
      "2.9645672\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.5318e-05 - acc: 1.0000 - val_loss: 0.8024 - val_acc: 0.6471\n",
      "2.9664314\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.7408e-05 - acc: 1.0000 - val_loss: 0.8023 - val_acc: 0.6471\n",
      "2.9681427\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 2.6521e-05 - acc: 1.0000 - val_loss: 0.8019 - val_acc: 0.6471\n",
      "2.9697392\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.4754e-05 - acc: 1.0000 - val_loss: 0.8020 - val_acc: 0.6471\n",
      "2.9714878\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.5421e-05 - acc: 1.0000 - val_loss: 0.8019 - val_acc: 0.6471\n",
      "2.9731903\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.4548e-05 - acc: 1.0000 - val_loss: 0.8025 - val_acc: 0.6471\n",
      "2.9750674\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 4.7884e-05 - acc: 1.0000 - val_loss: 0.8032 - val_acc: 0.6471\n",
      "2.9770513\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.8098e-05 - acc: 1.0000 - val_loss: 0.8041 - val_acc: 0.6471\n",
      "2.9793077\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.8211e-05 - acc: 1.0000 - val_loss: 0.8051 - val_acc: 0.6471\n",
      "2.98168\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7201e-05 - acc: 1.0000 - val_loss: 0.8064 - val_acc: 0.6471\n",
      "2.9842453\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.3872e-05 - acc: 1.0000 - val_loss: 0.8075 - val_acc: 0.6471\n",
      "2.9866695\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.0042e-05 - acc: 1.0000 - val_loss: 0.8084 - val_acc: 0.6471\n",
      "2.9890394\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 4.7959e-05 - acc: 1.0000 - val_loss: 0.8095 - val_acc: 0.6471\n",
      "2.991495\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.7905e-05 - acc: 1.0000 - val_loss: 0.8111 - val_acc: 0.6471\n",
      "2.9940925\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 1.9995e-05 - acc: 1.0000 - val_loss: 0.8127 - val_acc: 0.6471\n",
      "2.9966357\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 2.2592e-05 - acc: 1.0000 - val_loss: 0.8145 - val_acc: 0.6471\n",
      "2.999185\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 5.0551e-05 - acc: 1.0000 - val_loss: 0.8177 - val_acc: 0.6471\n",
      "3.0024226\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 4.4735e-05 - acc: 1.0000 - val_loss: 0.8209 - val_acc: 0.6471\n",
      "3.0058384\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 4.7383e-05 - acc: 1.0000 - val_loss: 0.8242 - val_acc: 0.7059\n",
      "3.0094671\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.0405e-05 - acc: 0.9804 - val_loss: 0.8271 - val_acc: 0.7059\n",
      "3.0127997\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.7366 - acc: 0.7222\n",
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "modelFC_SW = models.Sequential()\n",
    "modelFC_SW.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "modelFC_SW.add(layers.Dropout(0.3))\n",
    "modelFC_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dropout(0.3))\n",
    "modelFC_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dropout(0.3))\n",
    "modelFC_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "modelFC_SW.add(layers.Dropout(0.3))\n",
    "modelFC_SW.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "modelFC_SW.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "samples = np.ones(y_train.shape) / len(y_train)\n",
    "y_true = y_train\n",
    "for epoch in range(100):\n",
    "    modelFC_SW.fit(X_train, y_train, epochs=1, validation_split=0.25, sample_weight=samples)\n",
    "    # Tiene sentido validation split?????\n",
    "    y_pred = modelFC_SW.predict(X_train)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = bce(y_true, y_pred).numpy() # Pero esto es un número, el vector de samples va a tener el mismo valor en cada punto\n",
    "    print(loss)\n",
    "#     samples = (samples*loss) / np.sum(loss)\n",
    "    err = np.abs(y_true.to_numpy() - y_pred.squeeze())\n",
    "    samples = err\n",
    "    \n",
    "# Precisión en partición de test\n",
    "loss, accuracy = modelFC_SW.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb1ca991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.1875134>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(axis=0)\n",
    "bce(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f83720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.83579731e-03, 1.60333514e-03, 2.18811631e-03, 1.53493881e-03,\n",
       "       6.08438253e-03, 2.99647450e-03, 6.19739294e-04, 4.42028046e-04,\n",
       "       3.25292349e-03, 1.16205215e-03, 2.03558803e-03, 1.23658776e-03,\n",
       "       1.90865993e-03, 3.10093164e-04, 1.96456909e-04, 1.14202499e-03,\n",
       "       7.91668892e-04, 4.81596762e-05, 1.56036019e-03, 3.70991230e-03,\n",
       "       4.70578671e-04, 4.44859266e-04, 8.96275043e-04, 5.22404909e-04,\n",
       "       3.56310606e-03, 6.17504120e-05, 9.64522362e-04, 2.17801332e-03,\n",
       "       1.13582611e-03, 6.06238842e-04, 1.23915076e-03, 1.32924318e-03,\n",
       "       1.47080421e-03, 1.71756744e-03, 1.34557440e-05, 3.92451257e-05,\n",
       "       2.36833096e-03, 6.34878874e-04, 2.42710114e-03, 1.26895308e-03,\n",
       "       1.81031227e-03, 1.13070011e-04, 4.70411777e-03, 1.61570311e-03,\n",
       "       3.91995907e-03, 2.53778696e-03, 5.69641590e-04, 2.50980258e-03,\n",
       "       3.47790122e-03, 2.74181366e-03, 1.14855170e-03, 5.06307662e-01,\n",
       "       8.48310471e-01, 4.92429674e-01, 2.16606349e-01, 4.50640917e-03,\n",
       "       9.90425020e-01, 1.27381057e-01, 1.01892948e-02, 8.63879919e-04,\n",
       "       5.76537848e-02, 9.85645324e-01, 1.00115538e-02, 9.52634066e-01,\n",
       "       1.60119832e-02, 5.67981601e-03, 5.43498993e-03, 8.17060471e-04])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb8dcf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.09944153e-06, -1.73007061e-06, -2.43502534e-08,  5.96046448e-07,\n",
       "        3.74317169e-05, -5.03104184e-06, -7.78569316e-08,  5.96046448e-07,\n",
       "       -4.50759990e-06,  0.00000000e+00, -5.95975926e-08, -1.00698981e-06,\n",
       "        4.41074371e-06, -3.55587844e-08, -3.17438031e-09,  2.38418579e-07,\n",
       "       -3.80091393e-07, -1.94900124e-10, -7.43830185e-07,  2.80141830e-06,\n",
       "       -1.39007724e-07, -2.85966991e-07,  1.07288361e-06, -1.71431029e-08,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.11002373e-06,\n",
       "       -3.86553509e-08, -1.95678460e-08, -1.43264060e-07,  2.50339508e-06,\n",
       "        0.00000000e+00,  1.43051147e-06, -2.64180865e-12, -7.89365850e-10,\n",
       "        1.36494637e-05, -3.16481810e-08, -8.58034696e-07, -5.25288257e-08,\n",
       "        1.40666962e-05,  0.00000000e+00,  1.12056732e-05,  4.17232513e-07,\n",
       "        2.22921371e-05,  7.98702240e-06, -3.81227494e-08, -8.45480827e-07,\n",
       "       -5.53516256e-05,  3.63588333e-06, -2.51406611e-08, -1.17625386e-01,\n",
       "       -7.50944138e-01, -4.08245027e-02, -4.13293242e-02,  3.00407410e-05,\n",
       "        9.99905575e-01, -2.61822343e-03, -5.73838843e-05, -5.91139546e-08,\n",
       "        2.58483887e-02,  9.99935786e-01,  7.56978989e-06,  9.99997930e-01,\n",
       "       -3.77451397e-06, -3.95371490e-06,  1.39117241e-04, -1.24286359e-07])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22c0f9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becac86",
   "metadata": {},
   "source": [
    "Finalmente, hay una manera de definir un modelo en keras y adaptarlo para su uso en ``sklearn``, donde sí existe una función para aplicar AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "874a2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_to_sklearn_model():\n",
    "    modelFC_AdaBoost = models.Sequential()\n",
    "    modelFC_AdaBoost.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "    modelFC_AdaBoost.add(layers.Dropout(0.3))\n",
    "    modelFC_AdaBoost.add(layers.Dense(200, activation=\"relu\"))\n",
    "    modelFC_AdaBoost.add(layers.Dropout(0.3))\n",
    "    modelFC_AdaBoost.add(layers.Dense(200, activation=\"relu\"))\n",
    "    modelFC_AdaBoost.add(layers.Dropout(0.3))\n",
    "    modelFC_AdaBoost.add(layers.Dense(200, activation=\"relu\"))\n",
    "    modelFC_AdaBoost.add(layers.Dropout(0.3))\n",
    "    modelFC_AdaBoost.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    modelFC_AdaBoost.compile(loss=custom_loss, optimizer=\"adam\", metrics=[\"acc\"])\n",
    "    return modelFC_AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bbdf455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saral\\AppData\\Local\\Temp\\ipykernel_15372\\1299115203.py:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  modelFC_AdaBoost = KerasClassifier(build_fn=keras_to_sklearn_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "modelFC_AdaBoost = KerasClassifier(build_fn=keras_to_sklearn_model, verbose=0)\n",
    "AdaBoost_classif = AdaBoostClassifier(base_estimator=modelFC_AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f3b25a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "KerasClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mAdaBoost_classif\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Predicción en partición de test\u001b[39;00m\n\u001b[0;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m AdaBoost_classif\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:486\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[1;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:132\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight cannot contain negative weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Check parameters\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Clear any previous fit results\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:503\u001b[0m, in \u001b[0;36mAdaBoostClassifier._validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    496\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaBoostClassifier with algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat the weak learner supports the calculation of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m         )\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_fit_parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support sample_weight.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    506\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: KerasClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Entrenar el modelo\n",
    "AdaBoost_classif.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred = AdaBoost_classif.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b78757",
   "metadata": {},
   "source": [
    "Thats because the AdaboostClassifier only support those base_estimators which have a sample_weight in their fit() method.\n",
    "\n",
    "Now although KerasClassifier supports sample_weight (from Sequential.fit()) by using the **kwargs in KerasClassifier.fit(), but scikit-learn uses a signature detecting function which checks the parameters available only in the KerasClassifier.fit()\n",
    "\n",
    "This is what it returns:\n",
    "\n",
    "from sklearn.utils.fixes import signature\n",
    "print(signature(model.fit))\n",
    "**Output: (x, y, kwargs)\n",
    "\n",
    "As you can see that the sample_weight is not present here, so AdaBoostClassifier will throw an error. One possible workaround is to extend the KerasClassifier class and then update the fit() method to add sample_weight there.\n",
    "\n",
    "I am not sure what can be done in the source code of KerasClassifier or in AdaboostClassifier for that case to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e09558",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242fd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, test_id=testFNC[\"Id\"]):\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape) # Comprobación del tamaño, debe ser: (119748, 2)\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    parent_path = current_path.parent\n",
    "    submissionDF.to_csv(f\"{parent_path}\\submissions\\MLSP_submission_NN_{current_time}.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
