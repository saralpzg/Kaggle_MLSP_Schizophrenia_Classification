{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter tunning libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ca4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"../data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"../data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on='Id', right_on='Id')\n",
    "data = pd.merge(left=train_labels, right=train, left_on='Id', right_on='Id')\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74697a",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0f9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aae0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"../data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"../data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test = pd.merge(left=testFNC, right=testSBM, left_on='Id', right_on='Id')\n",
    "test.drop(\"Id\", inplace=True, axis=1)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: 0.0907 - val_loss: 0.0469\n",
      "Epoch 2/100\n",
      "17963/17963 [==============================] - 57s 3ms/step - loss: 0.0327 - val_loss: 0.0310\n",
      "Epoch 3/100\n",
      "17963/17963 [==============================] - 57s 3ms/step - loss: 0.0226 - val_loss: 0.0241\n",
      "Epoch 4/100\n",
      "17963/17963 [==============================] - 57s 3ms/step - loss: 0.0250 - val_loss: 0.0249\n",
      "Epoch 5/100\n",
      "17963/17963 [==============================] - 58s 3ms/step - loss: 0.0321 - val_loss: 0.0358\n",
      "Epoch 6/100\n",
      "17963/17963 [==============================] - 57s 3ms/step - loss: 0.0304 - val_loss: 0.0284\n",
      "Epoch 7/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: 0.0256 - val_loss: 0.0201\n",
      "Epoch 8/100\n",
      "17963/17963 [==============================] - 53s 3ms/step - loss: 0.0187 - val_loss: 0.0136\n",
      "Epoch 9/100\n",
      "17963/17963 [==============================] - 55s 3ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 10/100\n",
      "17963/17963 [==============================] - 53s 3ms/step - loss: 0.0015 - val_loss: -0.0046\n",
      "Epoch 11/100\n",
      "17963/17963 [==============================] - 51s 3ms/step - loss: -0.0217 - val_loss: -0.0355\n",
      "Epoch 12/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0385 - val_loss: -0.0472\n",
      "Epoch 13/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0518 - val_loss: -0.0578\n",
      "Epoch 14/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0537 - val_loss: -0.0531\n",
      "Epoch 15/100\n",
      "17963/17963 [==============================] - 53s 3ms/step - loss: -0.0574 - val_loss: -0.0609\n",
      "Epoch 16/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0630 - val_loss: -0.0663\n",
      "Epoch 17/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0672 - val_loss: -0.0689\n",
      "Epoch 18/100\n",
      "17963/17963 [==============================] - 52s 3ms/step - loss: -0.0714 - val_loss: -0.0726\n",
      "Epoch 19/100\n",
      "17963/17963 [==============================] - 54s 3ms/step - loss: -0.0800 - val_loss: -0.0874\n",
      "Epoch 20/100\n",
      "17963/17963 [==============================] - 56s 3ms/step - loss: -0.0856 - val_loss: -0.0942\n",
      "Epoch 21/100\n",
      "17963/17963 [==============================] - 56s 3ms/step - loss: -0.0977 - val_loss: -0.0984\n",
      "Epoch 22/100\n",
      "17963/17963 [==============================] - 56s 3ms/step - loss: -0.0999 - val_loss: -0.1030\n",
      "Epoch 23/100\n",
      "17963/17963 [==============================] - 55s 3ms/step - loss: -0.1022 - val_loss: -0.1006\n",
      "Epoch 24/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: -0.0985 - val_loss: -0.0940\n",
      "Epoch 25/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: -0.0977 - val_loss: -0.0871\n",
      "Epoch 26/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: -0.0937 - val_loss: -0.0950\n",
      "Epoch 27/100\n",
      "17963/17963 [==============================] - 59s 3ms/step - loss: -0.0928 - val_loss: -0.0952\n",
      "Epoch 28/100\n",
      "17963/17963 [==============================] - 60s 3ms/step - loss: -0.0893 - val_loss: -0.0889\n",
      "Epoch 29/100\n",
      "17963/17963 [==============================] - 62s 3ms/step - loss: -0.0789 - val_loss: -0.0626\n",
      "Epoch 30/100\n",
      "17963/17963 [==============================] - 62s 3ms/step - loss: -0.0653 - val_loss: -0.0639\n",
      "Epoch 31/100\n",
      "17963/17963 [==============================] - 63s 4ms/step - loss: -0.0601 - val_loss: -0.0566\n",
      "Epoch 32/100\n",
      "17963/17963 [==============================] - 64s 4ms/step - loss: -0.0499 - val_loss: -0.0507\n",
      "Epoch 33/100\n",
      "17963/17963 [==============================] - 63s 4ms/step - loss: -0.0413 - val_loss: -0.0405\n",
      "Epoch 34/100\n",
      "17963/17963 [==============================] - 64s 4ms/step - loss: -0.0274 - val_loss: -0.0204\n",
      "Epoch 35/100\n",
      "17963/17963 [==============================] - 66s 4ms/step - loss: -0.0168 - val_loss: -0.0177\n",
      "Epoch 36/100\n",
      "17963/17963 [==============================] - 66s 4ms/step - loss: -0.0130 - val_loss: -0.0045\n",
      "Epoch 37/100\n",
      "17963/17963 [==============================] - 65s 4ms/step - loss: -0.0012 - val_loss: 0.0206\n",
      "Epoch 38/100\n",
      "17963/17963 [==============================] - 66s 4ms/step - loss: 0.0175 - val_loss: 0.0202\n",
      "Epoch 39/100\n",
      "17963/17963 [==============================] - 69s 4ms/step - loss: 0.0188 - val_loss: 0.0310\n",
      "Epoch 40/100\n",
      "17963/17963 [==============================] - 68s 4ms/step - loss: 0.0254 - val_loss: 0.0295\n",
      "Epoch 41/100\n",
      "17963/17963 [==============================] - 68s 4ms/step - loss: 0.0379 - val_loss: 0.0509\n",
      "Epoch 42/100\n",
      "17963/17963 [==============================] - 69s 4ms/step - loss: 0.0499 - val_loss: 0.0614\n",
      "Epoch 43/100\n",
      "17963/17963 [==============================] - 71s 4ms/step - loss: 0.0616 - val_loss: 0.0602\n",
      "Epoch 44/100\n",
      "17963/17963 [==============================] - 71s 4ms/step - loss: 0.0609 - val_loss: 0.0568\n",
      "Epoch 45/100\n",
      "17963/17963 [==============================] - 79s 4ms/step - loss: 0.0565 - val_loss: 0.0497\n",
      "Epoch 46/100\n",
      "17963/17963 [==============================] - 75s 4ms/step - loss: 0.0541 - val_loss: 0.0503\n",
      "Epoch 47/100\n",
      "17963/17963 [==============================] - 77s 4ms/step - loss: 0.0565 - val_loss: 0.0782\n",
      "Epoch 48/100\n",
      "17963/17963 [==============================] - 82s 5ms/step - loss: 0.0664 - val_loss: 0.0615\n",
      "Epoch 49/100\n",
      "17963/17963 [==============================] - 83s 5ms/step - loss: 0.0584 - val_loss: 0.0544\n",
      "Epoch 50/100\n",
      "17963/17963 [==============================] - 85s 5ms/step - loss: 0.0562 - val_loss: 0.0583\n",
      "Epoch 51/100\n",
      "17963/17963 [==============================] - 85s 5ms/step - loss: 0.0655 - val_loss: 0.0641\n",
      "Epoch 52/100\n",
      "17963/17963 [==============================] - 81s 4ms/step - loss: 0.0751 - val_loss: 0.0811\n",
      "Epoch 53/100\n",
      "17963/17963 [==============================] - 79s 4ms/step - loss: 0.0815 - val_loss: 0.0908\n",
      "Epoch 54/100\n",
      "17963/17963 [==============================] - 77s 4ms/step - loss: 0.0775 - val_loss: 0.0744\n",
      "Epoch 55/100\n",
      "17963/17963 [==============================] - 78s 4ms/step - loss: 0.0798 - val_loss: 0.0797\n",
      "Epoch 56/100\n",
      "17963/17963 [==============================] - 77s 4ms/step - loss: 0.0815 - val_loss: 0.0968\n",
      "Epoch 57/100\n",
      "17963/17963 [==============================] - 79s 4ms/step - loss: 0.0922 - val_loss: 0.0946\n",
      "Epoch 58/100\n",
      "17963/17963 [==============================] - 80s 4ms/step - loss: 0.1056 - val_loss: 0.1099\n",
      "Epoch 59/100\n",
      "17963/17963 [==============================] - 80s 4ms/step - loss: 0.1136 - val_loss: 0.1290\n",
      "Epoch 60/100\n",
      "17963/17963 [==============================] - 79s 4ms/step - loss: 0.1178 - val_loss: 0.1113\n",
      "Epoch 61/100\n",
      "17963/17963 [==============================] - 79s 4ms/step - loss: 0.1210 - val_loss: 0.1246\n",
      "Epoch 62/100\n",
      "17963/17963 [==============================] - 88s 5ms/step - loss: 0.1247 - val_loss: 0.1241\n",
      "Epoch 63/100\n",
      "17963/17963 [==============================] - 89s 5ms/step - loss: 0.1269 - val_loss: 0.1278\n",
      "Epoch 64/100\n",
      "17963/17963 [==============================] - 88s 5ms/step - loss: 0.1317 - val_loss: 0.1232\n",
      "Epoch 65/100\n",
      "17963/17963 [==============================] - 90s 5ms/step - loss: 0.1327 - val_loss: 0.1413\n",
      "Epoch 66/100\n",
      "17963/17963 [==============================] - 91s 5ms/step - loss: 0.1344 - val_loss: 0.1350\n",
      "Epoch 67/100\n",
      "17963/17963 [==============================] - 93s 5ms/step - loss: 0.1408 - val_loss: 0.1558\n",
      "Epoch 68/100\n",
      "17963/17963 [==============================] - 92s 5ms/step - loss: 0.1484 - val_loss: 0.1474\n",
      "Epoch 69/100\n",
      "17963/17963 [==============================] - 89s 5ms/step - loss: 0.1518 - val_loss: 0.1551\n",
      "Epoch 70/100\n",
      "17963/17963 [==============================] - 88s 5ms/step - loss: 0.1580 - val_loss: 0.1588\n",
      "Epoch 71/100\n",
      "17963/17963 [==============================] - 91s 5ms/step - loss: 0.1642 - val_loss: 0.1635\n",
      "Epoch 72/100\n",
      "17963/17963 [==============================] - 91s 5ms/step - loss: 0.1737 - val_loss: 0.1897\n",
      "Epoch 73/100\n",
      "17963/17963 [==============================] - 90s 5ms/step - loss: 0.1837 - val_loss: 0.2011\n",
      "Epoch 74/100\n",
      "17963/17963 [==============================] - 94s 5ms/step - loss: 0.1815 - val_loss: 0.1781\n",
      "Epoch 75/100\n",
      "17963/17963 [==============================] - 94s 5ms/step - loss: 0.1873 - val_loss: 0.1875\n",
      "Epoch 76/100\n",
      "17963/17963 [==============================] - 96s 5ms/step - loss: 0.1780 - val_loss: 0.1768\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17963/17963 [==============================] - 96s 5ms/step - loss: 0.1714 - val_loss: 0.1665\n",
      "Epoch 78/100\n",
      "17963/17963 [==============================] - 97s 5ms/step - loss: 0.1807 - val_loss: 0.1795\n",
      "Epoch 79/100\n",
      "17963/17963 [==============================] - 98s 5ms/step - loss: 0.1881 - val_loss: 0.1917\n",
      "Epoch 80/100\n",
      "17963/17963 [==============================] - 98s 5ms/step - loss: 0.1928 - val_loss: 0.1918\n",
      "Epoch 81/100\n",
      "17963/17963 [==============================] - 99s 6ms/step - loss: 0.1991 - val_loss: 0.2007\n",
      "Epoch 82/100\n",
      "17963/17963 [==============================] - 119s 7ms/step - loss: 0.2051 - val_loss: 0.2026\n",
      "Epoch 83/100\n",
      "17963/17963 [==============================] - 104s 6ms/step - loss: 0.1950 - val_loss: 0.1964\n",
      "Epoch 84/100\n",
      "17963/17963 [==============================] - 109s 6ms/step - loss: 0.1842 - val_loss: 0.1921\n",
      "Epoch 85/100\n",
      "17963/17963 [==============================] - 117s 7ms/step - loss: 0.1994 - val_loss: 0.2080\n",
      "Epoch 86/100\n",
      "17963/17963 [==============================] - 116s 6ms/step - loss: 0.2036 - val_loss: 0.2011\n",
      "Epoch 87/100\n",
      "17963/17963 [==============================] - 113s 6ms/step - loss: 0.2033 - val_loss: 0.2154\n",
      "Epoch 88/100\n",
      "17963/17963 [==============================] - 109s 6ms/step - loss: 0.2125 - val_loss: 0.2067\n",
      "Epoch 89/100\n",
      "17963/17963 [==============================] - 111s 6ms/step - loss: 0.1780 - val_loss: 0.1594\n",
      "Epoch 90/100\n",
      "17963/17963 [==============================] - 122s 7ms/step - loss: 0.1809 - val_loss: 0.1910\n",
      "Epoch 91/100\n",
      "17963/17963 [==============================] - 120s 7ms/step - loss: 0.1967 - val_loss: 0.1936\n",
      "Epoch 92/100\n",
      "17963/17963 [==============================] - 116s 6ms/step - loss: 0.1768 - val_loss: 0.1590\n",
      "Epoch 93/100\n",
      "17963/17963 [==============================] - 114s 6ms/step - loss: 0.1634 - val_loss: 0.1401\n",
      "Epoch 94/100\n",
      "17963/17963 [==============================] - 115s 6ms/step - loss: 0.1456 - val_loss: 0.1481\n",
      "Epoch 95/100\n",
      "17963/17963 [==============================] - 112s 6ms/step - loss: 0.1261 - val_loss: 0.1087\n",
      "Epoch 96/100\n",
      "17963/17963 [==============================] - 113s 6ms/step - loss: 0.0865 - val_loss: 0.0747\n",
      "Epoch 97/100\n",
      "17963/17963 [==============================] - 114s 6ms/step - loss: 0.1016 - val_loss: 0.0915\n",
      "Epoch 98/100\n",
      "17963/17963 [==============================] - 124s 7ms/step - loss: 0.1037 - val_loss: 0.1211\n",
      "Epoch 99/100\n",
      "17963/17963 [==============================] - 137s 8ms/step - loss: 0.1030 - val_loss: 0.1286\n",
      "Epoch 100/100\n",
      "17963/17963 [==============================] - 130s 7ms/step - loss: 0.1173 - val_loss: 0.1090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cd20015460>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"relu\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"relu\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(410)(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "best_batch_size= 5\n",
    "\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "# Qué optimizador utilizo?? No es viable hacer búsqueda de parámetros con lo que tarda\n",
    "autoencoder.fit(test, test, epochs=100, batch_size=best_batch_size, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879fd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a743db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")\n",
    "encoder.save(\"encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9933cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")\n",
    "encoder = models.load_model(\"encoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como el MSE y el MAPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error in X_train data prediction: tf.Tensor(305.43213, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305.432131294834"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Precisión en partición de test (datos no etiquetados)\n",
    "test_pred = autoencoder.predict(test)\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "# print(\"Mean squared error in X_train data prediction:\", mse(X_train, X_train_pred).numpy())\n",
    "print(\"Mean absolute percentage error in X_train data prediction:\", mape(test.to_numpy()[:, 0], test_pred[:, 0]))\n",
    "sum(abs((test.to_numpy()[:, 0] - test_pred[:, 0]) / test.to_numpy()[:, 0])) * (100 / len(test_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c38d1",
   "metadata": {},
   "source": [
    "Un error de MAPE superior al 100% indica un error muy alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_encoder = encoder # loading the previously saved model.\n",
    "\n",
    "new_encoder = models.Sequential()\n",
    "new_encoder.add(prev_encoder)\n",
    "new_encoder.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 346ms/step - loss: 0.7135 - acc: 0.4510 - val_loss: 0.7251 - val_acc: 0.4118\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7109 - acc: 0.4510 - val_loss: 0.7232 - val_acc: 0.4118\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.7094 - acc: 0.4510 - val_loss: 0.7218 - val_acc: 0.4118\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7082 - acc: 0.4510 - val_loss: 0.7205 - val_acc: 0.4118\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7071 - acc: 0.4510 - val_loss: 0.7192 - val_acc: 0.4118\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.7060 - acc: 0.4510 - val_loss: 0.7181 - val_acc: 0.4118\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7060 - acc: 0.4510 - val_loss: 0.7178 - val_acc: 0.4118\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7049 - acc: 0.4706 - val_loss: 0.7171 - val_acc: 0.4118\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7045 - acc: 0.4706 - val_loss: 0.7166 - val_acc: 0.4118\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7038 - acc: 0.4706 - val_loss: 0.7158 - val_acc: 0.4118\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7032 - acc: 0.4706 - val_loss: 0.7152 - val_acc: 0.4118\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7027 - acc: 0.4706 - val_loss: 0.7142 - val_acc: 0.4118\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7023 - acc: 0.4510 - val_loss: 0.7140 - val_acc: 0.4118\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7016 - acc: 0.4510 - val_loss: 0.7133 - val_acc: 0.4118\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7011 - acc: 0.4510 - val_loss: 0.7127 - val_acc: 0.4118\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7005 - acc: 0.4510 - val_loss: 0.7121 - val_acc: 0.4118\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7004 - acc: 0.4510 - val_loss: 0.7113 - val_acc: 0.4118\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6996 - acc: 0.4118 - val_loss: 0.7106 - val_acc: 0.4118\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6989 - acc: 0.4118 - val_loss: 0.7100 - val_acc: 0.4118\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6985 - acc: 0.4118 - val_loss: 0.7094 - val_acc: 0.4118\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6980 - acc: 0.4118 - val_loss: 0.7088 - val_acc: 0.4118\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6974 - acc: 0.4118 - val_loss: 0.7083 - val_acc: 0.3529\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6970 - acc: 0.4314 - val_loss: 0.7079 - val_acc: 0.3529\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6967 - acc: 0.4314 - val_loss: 0.7072 - val_acc: 0.3529\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6962 - acc: 0.4118 - val_loss: 0.7068 - val_acc: 0.3529\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6958 - acc: 0.3922 - val_loss: 0.7063 - val_acc: 0.3529\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6955 - acc: 0.3922 - val_loss: 0.7057 - val_acc: 0.3529\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6949 - acc: 0.3922 - val_loss: 0.7051 - val_acc: 0.3529\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6945 - acc: 0.4118 - val_loss: 0.7048 - val_acc: 0.3529\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6941 - acc: 0.4314 - val_loss: 0.7042 - val_acc: 0.3529\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6940 - acc: 0.4314 - val_loss: 0.7040 - val_acc: 0.3529\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6936 - acc: 0.4510 - val_loss: 0.7038 - val_acc: 0.2941\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6935 - acc: 0.4902 - val_loss: 0.7036 - val_acc: 0.2941\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6932 - acc: 0.4902 - val_loss: 0.7033 - val_acc: 0.2941\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6928 - acc: 0.5098 - val_loss: 0.7028 - val_acc: 0.2941\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6924 - acc: 0.4902 - val_loss: 0.7024 - val_acc: 0.4118\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6920 - acc: 0.4902 - val_loss: 0.7019 - val_acc: 0.4118\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6917 - acc: 0.5098 - val_loss: 0.7014 - val_acc: 0.4118\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6913 - acc: 0.5294 - val_loss: 0.7010 - val_acc: 0.4118\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6910 - acc: 0.5294 - val_loss: 0.7004 - val_acc: 0.4118\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6905 - acc: 0.5490 - val_loss: 0.7000 - val_acc: 0.4118\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6902 - acc: 0.5882 - val_loss: 0.6997 - val_acc: 0.4118\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6900 - acc: 0.5882 - val_loss: 0.6992 - val_acc: 0.4118\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6896 - acc: 0.6078 - val_loss: 0.6987 - val_acc: 0.4706\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6896 - acc: 0.5686 - val_loss: 0.6982 - val_acc: 0.4706\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6889 - acc: 0.5490 - val_loss: 0.6978 - val_acc: 0.4706\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6887 - acc: 0.5294 - val_loss: 0.6977 - val_acc: 0.4706\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6886 - acc: 0.5294 - val_loss: 0.6973 - val_acc: 0.4706\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6881 - acc: 0.5490 - val_loss: 0.6970 - val_acc: 0.4706\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6879 - acc: 0.5490 - val_loss: 0.6968 - val_acc: 0.4706\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6879 - acc: 0.5490 - val_loss: 0.6964 - val_acc: 0.4706\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6875 - acc: 0.5294 - val_loss: 0.6963 - val_acc: 0.4706\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6874 - acc: 0.5490 - val_loss: 0.6959 - val_acc: 0.4706\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6871 - acc: 0.5294 - val_loss: 0.6958 - val_acc: 0.5294\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6871 - acc: 0.5490 - val_loss: 0.6954 - val_acc: 0.5294\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6866 - acc: 0.5686 - val_loss: 0.6953 - val_acc: 0.5294\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6865 - acc: 0.5686 - val_loss: 0.6952 - val_acc: 0.5294\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6865 - acc: 0.5686 - val_loss: 0.6948 - val_acc: 0.5294\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6861 - acc: 0.6078 - val_loss: 0.6946 - val_acc: 0.4706\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6862 - acc: 0.6078 - val_loss: 0.6943 - val_acc: 0.5294\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6857 - acc: 0.6471 - val_loss: 0.6941 - val_acc: 0.5294\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6856 - acc: 0.6471 - val_loss: 0.6938 - val_acc: 0.5294\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6854 - acc: 0.6667 - val_loss: 0.6935 - val_acc: 0.5882\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6853 - acc: 0.6275 - val_loss: 0.6935 - val_acc: 0.5882\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6851 - acc: 0.6471 - val_loss: 0.6932 - val_acc: 0.5882\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6848 - acc: 0.6275 - val_loss: 0.6930 - val_acc: 0.5882\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6846 - acc: 0.6078 - val_loss: 0.6928 - val_acc: 0.5882\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6845 - acc: 0.6275 - val_loss: 0.6927 - val_acc: 0.5882\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6843 - acc: 0.6275 - val_loss: 0.6925 - val_acc: 0.5882\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6841 - acc: 0.6275 - val_loss: 0.6923 - val_acc: 0.5882\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6840 - acc: 0.6275 - val_loss: 0.6920 - val_acc: 0.5882\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6838 - acc: 0.6275 - val_loss: 0.6918 - val_acc: 0.5294\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6836 - acc: 0.6078 - val_loss: 0.6915 - val_acc: 0.5294\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6834 - acc: 0.6078 - val_loss: 0.6913 - val_acc: 0.5294\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6833 - acc: 0.6078 - val_loss: 0.6910 - val_acc: 0.5294\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6830 - acc: 0.5882 - val_loss: 0.6908 - val_acc: 0.5294\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6829 - acc: 0.5882 - val_loss: 0.6907 - val_acc: 0.5294\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6827 - acc: 0.5882 - val_loss: 0.6905 - val_acc: 0.5294\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6825 - acc: 0.5882 - val_loss: 0.6903 - val_acc: 0.5294\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6828 - acc: 0.5882 - val_loss: 0.6903 - val_acc: 0.5294\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6823 - acc: 0.5882 - val_loss: 0.6903 - val_acc: 0.5294\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6821 - acc: 0.5882 - val_loss: 0.6900 - val_acc: 0.5882\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6824 - acc: 0.5882 - val_loss: 0.6901 - val_acc: 0.5882\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6820 - acc: 0.5882 - val_loss: 0.6898 - val_acc: 0.5882\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6819 - acc: 0.5882 - val_loss: 0.6896 - val_acc: 0.5882\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6816 - acc: 0.5882 - val_loss: 0.6896 - val_acc: 0.5882\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6818 - acc: 0.5882 - val_loss: 0.6894 - val_acc: 0.5882\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6814 - acc: 0.5882 - val_loss: 0.6893 - val_acc: 0.5882\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6812 - acc: 0.5882 - val_loss: 0.6893 - val_acc: 0.5882\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6811 - acc: 0.5882 - val_loss: 0.6892 - val_acc: 0.5882\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6810 - acc: 0.5882 - val_loss: 0.6892 - val_acc: 0.5882\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6809 - acc: 0.5882 - val_loss: 0.6890 - val_acc: 0.5882\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6812 - acc: 0.5686 - val_loss: 0.6888 - val_acc: 0.5882\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6807 - acc: 0.5686 - val_loss: 0.6888 - val_acc: 0.5882\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6808 - acc: 0.5686 - val_loss: 0.6889 - val_acc: 0.5882\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6804 - acc: 0.5882 - val_loss: 0.6888 - val_acc: 0.5882\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6803 - acc: 0.5686 - val_loss: 0.6887 - val_acc: 0.5882\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6808 - acc: 0.5882 - val_loss: 0.6886 - val_acc: 0.5882\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6802 - acc: 0.5686 - val_loss: 0.6884 - val_acc: 0.5882\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6801 - acc: 0.5686 - val_loss: 0.6884 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6989 - acc: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1258",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, method, test_id=testFNC[\"Id\"]):\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape) # Comprobación del tamaño, debe ser: (119748, 2)\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    parent_path = current_path.parent\n",
    "    submissionDF.to_csv(f\"{parent_path}\\submissions\\MLSP_submission_{method}_{current_time}.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
