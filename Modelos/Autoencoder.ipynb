{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter tunning libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ca4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.124680</td>\n",
       "      <td>-0.353800</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>-0.035222</td>\n",
       "      <td>0.259040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257114</td>\n",
       "      <td>0.597229</td>\n",
       "      <td>1.220756</td>\n",
       "      <td>-0.059213</td>\n",
       "      <td>-0.435494</td>\n",
       "      <td>-0.092971</td>\n",
       "      <td>1.090910</td>\n",
       "      <td>-0.448562</td>\n",
       "      <td>-0.508497</td>\n",
       "      <td>0.350434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.410730</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-0.187140</td>\n",
       "      <td>0.168450</td>\n",
       "      <td>0.599790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050862</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>1.181878</td>\n",
       "      <td>-2.279469</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.012693</td>\n",
       "      <td>-1.244346</td>\n",
       "      <td>-1.080442</td>\n",
       "      <td>-0.788502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.212550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539922</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>1.643866</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>1.521086</td>\n",
       "      <td>-1.988432</td>\n",
       "      <td>-0.267471</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>1.104566</td>\n",
       "      <td>-1.067206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.389380</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.251230</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077353</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>-0.619508</td>\n",
       "      <td>-1.410523</td>\n",
       "      <td>-0.304622</td>\n",
       "      <td>-1.521928</td>\n",
       "      <td>0.593691</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>-0.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.157780</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.039780</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.048222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044457</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>1.063052</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>1.604964</td>\n",
       "      <td>-0.359736</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.730287</td>\n",
       "      <td>-0.323557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class      FNC1      FNC2      FNC3      FNC4      FNC5      FNC6  \\\n",
       "2       0  0.245850  0.216620 -0.124680 -0.353800  0.161500 -0.002032   \n",
       "13      1  0.410730 -0.031925  0.210700  0.242260  0.320100 -0.419290   \n",
       "53      1  0.070919  0.034179 -0.011755  0.019158  0.024645 -0.032022   \n",
       "41      0  0.087377 -0.052462 -0.007835 -0.112830  0.389380  0.216080   \n",
       "74      0  0.202750  0.191420 -0.056662 -0.157780  0.244040  0.039780   \n",
       "\n",
       "        FNC7      FNC8      FNC9  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "2  -0.133020 -0.035222  0.259040  ...  -0.257114   0.597229   1.220756   \n",
       "13 -0.187140  0.168450  0.599790  ...  -0.050862   0.870602   0.609465   \n",
       "53  0.004620  0.318170  0.212550  ...  -1.539922  -1.495822   1.643866   \n",
       "41  0.063572 -0.251230 -0.080568  ...  -0.077353  -0.459463  -0.204328   \n",
       "74 -0.001503  0.001056 -0.048222  ...   0.044457   0.593326   1.063052   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "2   -0.059213  -0.435494  -0.092971   1.090910  -0.448562  -0.508497   \n",
       "13   1.181878  -2.279469  -0.013484  -0.012693  -1.244346  -1.080442   \n",
       "53   1.687780   1.521086  -1.988432  -0.267471   0.510576   1.104566   \n",
       "41  -0.619508  -1.410523  -0.304622  -1.521928   0.593691   0.073638   \n",
       "74   0.434726   1.604964  -0.359736   0.210107   0.355922   0.730287   \n",
       "\n",
       "    SBM_map75  \n",
       "2    0.350434  \n",
       "13  -0.788502  \n",
       "53  -1.067206  \n",
       "41  -0.260920  \n",
       "74  -0.323557  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "trainFNC = pd.read_csv(\"../data/train_FNC.csv\")\n",
    "trainSBM = pd.read_csv(\"../data/train_SBM.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "train = pd.merge(left=trainFNC, right=trainSBM, left_on='Id', right_on='Id')\n",
    "data = pd.merge(left=train_labels, right=train, left_on='Id', right_on='Id')\n",
    "data.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "# Shuffle de los datos de train\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74697a",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0f9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aae0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0  0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1  0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2 -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3 -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4  0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "\n",
       "       FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0 -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1  0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2 -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3 -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4  0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "\n",
       "   SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  SBM_map75  \n",
       "0   1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   0.192076  \n",
       "1  -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   0.174160  \n",
       "2  -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701  -0.476647  \n",
       "3   1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   2.790871  \n",
       "4  -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948  -1.544345  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de test\n",
    "testFNC = pd.read_csv(\"../data/test_FNC.csv\")\n",
    "testSBM = pd.read_csv(\"../data/test_SBM.csv\")\n",
    "\n",
    "# DataFrame con ambas fuentes de datos\n",
    "test = pd.merge(left=testFNC, right=testSBM, left_on='Id', right_on='Id')\n",
    "test.drop(\"Id\", inplace=True, axis=1)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 0.0931 - val_loss: 0.0859\n",
      "Epoch 2/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0840 - val_loss: 0.0849\n",
      "Epoch 3/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0835 - val_loss: 0.0847\n",
      "Epoch 4/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0833 - val_loss: 0.0841\n",
      "Epoch 5/100\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 0.0831 - val_loss: 0.0839\n",
      "Epoch 6/100\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 0.0831 - val_loss: 0.0841\n",
      "Epoch 7/100\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 0.0830 - val_loss: 0.0839\n",
      "Epoch 8/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0829 - val_loss: 0.0836\n",
      "Epoch 9/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0829 - val_loss: 0.0839\n",
      "Epoch 10/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0828 - val_loss: 0.0836\n",
      "Epoch 11/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0828 - val_loss: 0.0834\n",
      "Epoch 12/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0828 - val_loss: 0.0832\n",
      "Epoch 13/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0827 - val_loss: 0.0835\n",
      "Epoch 14/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0827 - val_loss: 0.0836\n",
      "Epoch 15/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0827 - val_loss: 0.0836\n",
      "Epoch 16/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0826 - val_loss: 0.0838\n",
      "Epoch 17/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0826 - val_loss: 0.0834\n",
      "Epoch 18/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0826 - val_loss: 0.0835\n",
      "Epoch 19/100\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 0.0826 - val_loss: 0.0832\n",
      "Epoch 20/100\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 0.0825 - val_loss: 0.0832\n",
      "Epoch 21/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0825 - val_loss: 0.0834\n",
      "Epoch 22/100\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 0.0825 - val_loss: 0.0830\n",
      "Epoch 23/100\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 0.0825 - val_loss: 0.0836\n",
      "Epoch 24/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0825 - val_loss: 0.0829\n",
      "Epoch 25/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0825 - val_loss: 0.0830\n",
      "Epoch 26/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0825 - val_loss: 0.0831\n",
      "Epoch 27/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 28/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 29/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 30/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0833\n",
      "Epoch 31/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0833\n",
      "Epoch 32/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 33/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 34/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 35/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0831\n",
      "Epoch 36/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 37/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 38/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0828\n",
      "Epoch 39/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0832\n",
      "Epoch 40/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0824 - val_loss: 0.0829\n",
      "Epoch 41/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 42/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 43/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 44/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 45/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 46/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 47/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 48/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 49/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 50/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0832\n",
      "Epoch 51/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 52/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 53/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 54/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0832\n",
      "Epoch 55/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 56/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 57/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 58/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 59/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 60/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 61/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0827\n",
      "Epoch 62/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 63/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 64/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 65/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 66/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 67/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 68/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 69/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 70/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 71/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 72/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 73/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0830\n",
      "Epoch 74/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 75/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0826\n",
      "Epoch 76/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 77/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 78/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 79/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 81/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 82/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 83/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 84/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 85/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 86/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 87/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 88/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 89/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 90/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 91/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0830\n",
      "Epoch 92/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 93/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 94/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 95/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0825\n",
      "Epoch 96/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0828\n",
      "Epoch 97/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 98/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0826\n",
      "Epoch 99/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 100/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0822 - val_loss: 0.0827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27d884c96a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"linear\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"linear\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"linear\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"linear\")(decoded)\n",
    "decoded = layers.Dense(410)(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "# autoencoder.fit(test, test, epochs=1, batch_size=64, validation_split=0.25)\n",
    "autoencoder.fit(test, test, epochs=100, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c774fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "747650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como:\n",
    "\n",
    "* MSE = $\\frac{1}{n} \\sum_{i=1}^{n} (Y_{true}^{i} - Y_{pred}^{i})^{2}$\n",
    "* MAPE = $\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_{true} - Y_{pred}}{Y_{true}} \\right|$\n",
    "\n",
    "donde $Y_{true}$ es el valor real, $Y_{pred}$ el valor de la predicción y $n$ el número de predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(268.11365, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "y_true = X_train\n",
    "\n",
    "# Precisión en partición de test (datos no etiquetados)\n",
    "y_pred = autoencoder.predict(y_true)\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "# print(\"Mean squared error in X_train data prediction:\", mse(X_train, X_train_pred).numpy())\n",
    "\n",
    "y_true = y_true.to_numpy()\n",
    "print(\"MAPE:\", mape(y_true, y_pred))\n",
    "# sum(abs((test.to_numpy()[:, 0] - test_pred[:, 0]) / test.to_numpy()[:, 0])) * (100 / len(test_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caa2eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 410) (68, 410)\n"
     ]
    }
   ],
   "source": [
    "print(y_true.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb439169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.833100341823826"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs((y_true - y_pred) / y_true)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b490b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476127</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.608133</td>\n",
       "      <td>0.073988</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>-0.192434</td>\n",
       "      <td>-0.004025</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451994</td>\n",
       "      <td>1.123770</td>\n",
       "      <td>2.083006</td>\n",
       "      <td>1.145440</td>\n",
       "      <td>-0.067608</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>-0.159739</td>\n",
       "      <td>0.192076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.267183</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>-0.167151</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.191869</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.088761</td>\n",
       "      <td>0.177048</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696987</td>\n",
       "      <td>1.397832</td>\n",
       "      <td>1.046136</td>\n",
       "      <td>-0.191733</td>\n",
       "      <td>-2.192023</td>\n",
       "      <td>-0.369276</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>-0.109342</td>\n",
       "      <td>-0.580476</td>\n",
       "      <td>0.174160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.435452</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.243742</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>-0.147821</td>\n",
       "      <td>0.173620</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.400985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>1.906989</td>\n",
       "      <td>-2.661633</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>-0.758046</td>\n",
       "      <td>0.154701</td>\n",
       "      <td>-0.476647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.204510</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.740495</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.349926</td>\n",
       "      <td>-0.273826</td>\n",
       "      <td>-0.174384</td>\n",
       "      <td>-0.120248</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974828</td>\n",
       "      <td>-1.997087</td>\n",
       "      <td>-2.083782</td>\n",
       "      <td>1.154107</td>\n",
       "      <td>-0.643947</td>\n",
       "      <td>2.332424</td>\n",
       "      <td>0.659124</td>\n",
       "      <td>-0.809445</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>2.790871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599435</td>\n",
       "      <td>-0.166441</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.274734</td>\n",
       "      <td>0.211510</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789153</td>\n",
       "      <td>1.578984</td>\n",
       "      <td>1.402592</td>\n",
       "      <td>-1.230440</td>\n",
       "      <td>0.296686</td>\n",
       "      <td>2.806314</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>-0.196948</td>\n",
       "      <td>-1.544345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119743</th>\n",
       "      <td>-0.199963</td>\n",
       "      <td>0.237939</td>\n",
       "      <td>-0.163156</td>\n",
       "      <td>-0.040444</td>\n",
       "      <td>-0.260439</td>\n",
       "      <td>-0.165075</td>\n",
       "      <td>-0.304439</td>\n",
       "      <td>0.273075</td>\n",
       "      <td>0.265433</td>\n",
       "      <td>-0.050194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605117</td>\n",
       "      <td>1.079632</td>\n",
       "      <td>-1.525021</td>\n",
       "      <td>2.479249</td>\n",
       "      <td>0.955424</td>\n",
       "      <td>-0.900318</td>\n",
       "      <td>-0.326580</td>\n",
       "      <td>-1.010058</td>\n",
       "      <td>-0.660097</td>\n",
       "      <td>0.548342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119744</th>\n",
       "      <td>0.349568</td>\n",
       "      <td>0.407478</td>\n",
       "      <td>-0.246833</td>\n",
       "      <td>-0.391152</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.289385</td>\n",
       "      <td>-0.230535</td>\n",
       "      <td>-0.132515</td>\n",
       "      <td>-0.296383</td>\n",
       "      <td>0.311725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534909</td>\n",
       "      <td>0.192909</td>\n",
       "      <td>-0.336587</td>\n",
       "      <td>0.062809</td>\n",
       "      <td>-1.863223</td>\n",
       "      <td>-0.200127</td>\n",
       "      <td>-0.859949</td>\n",
       "      <td>1.152197</td>\n",
       "      <td>0.840467</td>\n",
       "      <td>-0.690686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119745</th>\n",
       "      <td>0.424527</td>\n",
       "      <td>0.032844</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.154465</td>\n",
       "      <td>0.401733</td>\n",
       "      <td>0.038589</td>\n",
       "      <td>-0.536111</td>\n",
       "      <td>0.154705</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.425772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347514</td>\n",
       "      <td>0.430025</td>\n",
       "      <td>1.452216</td>\n",
       "      <td>0.759654</td>\n",
       "      <td>0.295776</td>\n",
       "      <td>-1.001644</td>\n",
       "      <td>-1.071604</td>\n",
       "      <td>0.739943</td>\n",
       "      <td>0.135163</td>\n",
       "      <td>1.607317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119746</th>\n",
       "      <td>-0.081085</td>\n",
       "      <td>0.335802</td>\n",
       "      <td>0.219624</td>\n",
       "      <td>-0.702985</td>\n",
       "      <td>0.110103</td>\n",
       "      <td>0.351589</td>\n",
       "      <td>0.437899</td>\n",
       "      <td>-0.057477</td>\n",
       "      <td>-0.118451</td>\n",
       "      <td>0.094075</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.589449</td>\n",
       "      <td>-2.599313</td>\n",
       "      <td>-0.161822</td>\n",
       "      <td>0.183669</td>\n",
       "      <td>-0.792102</td>\n",
       "      <td>1.889364</td>\n",
       "      <td>1.103609</td>\n",
       "      <td>0.373980</td>\n",
       "      <td>0.086772</td>\n",
       "      <td>-1.313079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119747</th>\n",
       "      <td>0.251729</td>\n",
       "      <td>0.119240</td>\n",
       "      <td>0.394440</td>\n",
       "      <td>-0.225384</td>\n",
       "      <td>-0.021766</td>\n",
       "      <td>0.160016</td>\n",
       "      <td>-0.221715</td>\n",
       "      <td>0.264397</td>\n",
       "      <td>-0.149024</td>\n",
       "      <td>-0.040617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513703</td>\n",
       "      <td>0.785046</td>\n",
       "      <td>2.369759</td>\n",
       "      <td>-0.168919</td>\n",
       "      <td>0.708511</td>\n",
       "      <td>2.393801</td>\n",
       "      <td>-0.361271</td>\n",
       "      <td>0.725078</td>\n",
       "      <td>-1.213793</td>\n",
       "      <td>-0.668412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119748 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "0       0.476127  0.064466  0.053238 -0.608133  0.073988 -0.637038  0.113556   \n",
       "1       0.013833  0.267183  0.232178 -0.167151 -0.261327  0.191869  0.406493   \n",
       "2      -0.435452  0.046780  0.243742  0.397030 -0.147821  0.173620 -0.461963   \n",
       "3      -0.204510 -0.036735 -0.760705 -0.740495  0.064668  0.349926 -0.273826   \n",
       "4       0.599435 -0.166441  0.122431  0.011539  0.346906 -0.017430 -0.274734   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119743 -0.199963  0.237939 -0.163156 -0.040444 -0.260439 -0.165075 -0.304439   \n",
       "119744  0.349568  0.407478 -0.246833 -0.391152  0.006136  0.289385 -0.230535   \n",
       "119745  0.424527  0.032844  0.005272  0.154465  0.401733  0.038589 -0.536111   \n",
       "119746 -0.081085  0.335802  0.219624 -0.702985  0.110103  0.351589  0.437899   \n",
       "119747  0.251729  0.119240  0.394440 -0.225384 -0.021766  0.160016 -0.221715   \n",
       "\n",
       "            FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "0      -0.192434 -0.004025 -0.060474  ...  -0.451994   1.123770   2.083006   \n",
       "1       0.088761  0.177048  0.036718  ...   0.696987   1.397832   1.046136   \n",
       "2      -0.610736  0.419753  0.400985  ...   0.160145   1.906989  -2.661633   \n",
       "3      -0.174384 -0.120248  0.175618  ...   0.974828  -1.997087  -2.083782   \n",
       "4       0.211510  0.151012 -0.033434  ...  -0.789153   1.578984   1.402592   \n",
       "...          ...       ...       ...  ...        ...        ...        ...   \n",
       "119743  0.273075  0.265433 -0.050194  ...   0.605117   1.079632  -1.525021   \n",
       "119744 -0.132515 -0.296383  0.311725  ...   1.534909   0.192909  -0.336587   \n",
       "119745  0.154705  0.033401  0.425772  ...  -0.347514   0.430025   1.452216   \n",
       "119746 -0.057477 -0.118451  0.094075  ...  -1.589449  -2.599313  -0.161822   \n",
       "119747  0.264397 -0.149024 -0.040617  ...   0.513703   0.785046   2.369759   \n",
       "\n",
       "        SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "0        1.145440  -0.067608   1.202529   0.851587   0.451583  -0.159739   \n",
       "1       -0.191733  -2.192023  -0.369276   0.822225  -0.109342  -0.580476   \n",
       "2       -0.193911   0.440873   0.641739   0.918397  -0.758046   0.154701   \n",
       "3        1.154107  -0.643947   2.332424   0.659124  -0.809445   0.558960   \n",
       "4       -1.230440   0.296686   2.806314   0.427184  -0.240682  -0.196948   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "119743   2.479249   0.955424  -0.900318  -0.326580  -1.010058  -0.660097   \n",
       "119744   0.062809  -1.863223  -0.200127  -0.859949   1.152197   0.840467   \n",
       "119745   0.759654   0.295776  -1.001644  -1.071604   0.739943   0.135163   \n",
       "119746   0.183669  -0.792102   1.889364   1.103609   0.373980   0.086772   \n",
       "119747  -0.168919   0.708511   2.393801  -0.361271   0.725078  -1.213793   \n",
       "\n",
       "        SBM_map75  \n",
       "0        0.192076  \n",
       "1        0.174160  \n",
       "2       -0.476647  \n",
       "3        2.790871  \n",
       "4       -1.544345  \n",
       "...           ...  \n",
       "119743   0.548342  \n",
       "119744  -0.690686  \n",
       "119745   1.607317  \n",
       "119746  -1.313079  \n",
       "119747  -0.668412  \n",
       "\n",
       "[119748 rows x 410 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "003e8878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FNC1</th>\n",
       "      <th>FNC2</th>\n",
       "      <th>FNC3</th>\n",
       "      <th>FNC4</th>\n",
       "      <th>FNC5</th>\n",
       "      <th>FNC6</th>\n",
       "      <th>FNC7</th>\n",
       "      <th>FNC8</th>\n",
       "      <th>FNC9</th>\n",
       "      <th>FNC10</th>\n",
       "      <th>...</th>\n",
       "      <th>SBM_map55</th>\n",
       "      <th>SBM_map61</th>\n",
       "      <th>SBM_map64</th>\n",
       "      <th>SBM_map67</th>\n",
       "      <th>SBM_map69</th>\n",
       "      <th>SBM_map71</th>\n",
       "      <th>SBM_map72</th>\n",
       "      <th>SBM_map73</th>\n",
       "      <th>SBM_map74</th>\n",
       "      <th>SBM_map75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.435160</td>\n",
       "      <td>0.225050</td>\n",
       "      <td>0.057172</td>\n",
       "      <td>-0.353480</td>\n",
       "      <td>0.447420</td>\n",
       "      <td>0.183180</td>\n",
       "      <td>0.122420</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.404830</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314668</td>\n",
       "      <td>-0.114078</td>\n",
       "      <td>-0.476524</td>\n",
       "      <td>-0.556896</td>\n",
       "      <td>0.505738</td>\n",
       "      <td>0.873278</td>\n",
       "      <td>0.040048</td>\n",
       "      <td>0.211690</td>\n",
       "      <td>0.536933</td>\n",
       "      <td>-0.424864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.468280</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>0.072178</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.266450</td>\n",
       "      <td>0.350770</td>\n",
       "      <td>-0.210100</td>\n",
       "      <td>-0.089635</td>\n",
       "      <td>-0.140530</td>\n",
       "      <td>-0.208800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200450</td>\n",
       "      <td>1.817908</td>\n",
       "      <td>-0.299583</td>\n",
       "      <td>0.740836</td>\n",
       "      <td>1.491966</td>\n",
       "      <td>0.993555</td>\n",
       "      <td>-0.043188</td>\n",
       "      <td>0.564047</td>\n",
       "      <td>-0.916360</td>\n",
       "      <td>2.771659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.137060</td>\n",
       "      <td>-0.188670</td>\n",
       "      <td>-0.160220</td>\n",
       "      <td>-0.152680</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.142780</td>\n",
       "      <td>0.349660</td>\n",
       "      <td>-0.046245</td>\n",
       "      <td>-0.133690</td>\n",
       "      <td>0.324320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.922012</td>\n",
       "      <td>-0.197890</td>\n",
       "      <td>1.585873</td>\n",
       "      <td>-0.056353</td>\n",
       "      <td>0.806093</td>\n",
       "      <td>-1.517281</td>\n",
       "      <td>1.672678</td>\n",
       "      <td>-0.376343</td>\n",
       "      <td>-0.061299</td>\n",
       "      <td>-0.945018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.174850</td>\n",
       "      <td>-0.119840</td>\n",
       "      <td>-0.366770</td>\n",
       "      <td>-0.354050</td>\n",
       "      <td>0.065508</td>\n",
       "      <td>-0.085309</td>\n",
       "      <td>-0.295600</td>\n",
       "      <td>0.311750</td>\n",
       "      <td>-0.013669</td>\n",
       "      <td>0.513440</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200334</td>\n",
       "      <td>0.313340</td>\n",
       "      <td>0.287729</td>\n",
       "      <td>-0.370420</td>\n",
       "      <td>0.224179</td>\n",
       "      <td>1.149330</td>\n",
       "      <td>1.842975</td>\n",
       "      <td>1.458239</td>\n",
       "      <td>0.729352</td>\n",
       "      <td>0.522059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.020630</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.210830</td>\n",
       "      <td>0.423430</td>\n",
       "      <td>0.263870</td>\n",
       "      <td>0.298310</td>\n",
       "      <td>-0.088201</td>\n",
       "      <td>0.081132</td>\n",
       "      <td>-0.025001</td>\n",
       "      <td>0.261250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266496</td>\n",
       "      <td>-1.527078</td>\n",
       "      <td>-1.028776</td>\n",
       "      <td>0.437655</td>\n",
       "      <td>-0.938700</td>\n",
       "      <td>0.215549</td>\n",
       "      <td>-0.575946</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>1.351451</td>\n",
       "      <td>0.619411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.820240</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.588390</td>\n",
       "      <td>0.731570</td>\n",
       "      <td>0.763950</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>0.612370</td>\n",
       "      <td>0.059279</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>-0.121400</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.465429</td>\n",
       "      <td>-0.935685</td>\n",
       "      <td>1.415247</td>\n",
       "      <td>-0.097483</td>\n",
       "      <td>0.073954</td>\n",
       "      <td>-2.566518</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>-0.249365</td>\n",
       "      <td>-0.409918</td>\n",
       "      <td>-0.384427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.071453</td>\n",
       "      <td>-0.202410</td>\n",
       "      <td>-0.328480</td>\n",
       "      <td>-0.609580</td>\n",
       "      <td>-0.137080</td>\n",
       "      <td>-0.434230</td>\n",
       "      <td>-0.637690</td>\n",
       "      <td>-0.377230</td>\n",
       "      <td>-0.295430</td>\n",
       "      <td>-0.164490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453766</td>\n",
       "      <td>-0.163926</td>\n",
       "      <td>0.953385</td>\n",
       "      <td>0.402673</td>\n",
       "      <td>0.025229</td>\n",
       "      <td>-1.975778</td>\n",
       "      <td>0.112912</td>\n",
       "      <td>-2.014501</td>\n",
       "      <td>-1.661052</td>\n",
       "      <td>-0.421040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.302150</td>\n",
       "      <td>0.203110</td>\n",
       "      <td>0.529190</td>\n",
       "      <td>-0.458170</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>0.168580</td>\n",
       "      <td>0.236150</td>\n",
       "      <td>-0.440140</td>\n",
       "      <td>0.279060</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131957</td>\n",
       "      <td>1.284969</td>\n",
       "      <td>0.859234</td>\n",
       "      <td>1.034285</td>\n",
       "      <td>2.116060</td>\n",
       "      <td>-1.098940</td>\n",
       "      <td>1.072767</td>\n",
       "      <td>-0.208299</td>\n",
       "      <td>-0.473763</td>\n",
       "      <td>0.762586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.343120</td>\n",
       "      <td>0.045761</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>0.150340</td>\n",
       "      <td>0.180820</td>\n",
       "      <td>0.289160</td>\n",
       "      <td>0.069545</td>\n",
       "      <td>-0.052489</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.477620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078220</td>\n",
       "      <td>-0.982331</td>\n",
       "      <td>1.070363</td>\n",
       "      <td>0.220316</td>\n",
       "      <td>0.776855</td>\n",
       "      <td>-2.022404</td>\n",
       "      <td>1.203256</td>\n",
       "      <td>1.083516</td>\n",
       "      <td>0.564201</td>\n",
       "      <td>-0.002006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.558360</td>\n",
       "      <td>0.356460</td>\n",
       "      <td>0.357290</td>\n",
       "      <td>0.289110</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.442630</td>\n",
       "      <td>0.318440</td>\n",
       "      <td>-0.110290</td>\n",
       "      <td>0.302120</td>\n",
       "      <td>0.055681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030834</td>\n",
       "      <td>1.304894</td>\n",
       "      <td>0.835378</td>\n",
       "      <td>-0.741826</td>\n",
       "      <td>-0.206366</td>\n",
       "      <td>0.476910</td>\n",
       "      <td>1.693937</td>\n",
       "      <td>1.269684</td>\n",
       "      <td>0.681868</td>\n",
       "      <td>1.511061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FNC1      FNC2      FNC3      FNC4      FNC5      FNC6      FNC7  \\\n",
       "83  0.435160  0.225050  0.057172 -0.353480  0.447420  0.183180  0.122420   \n",
       "40  0.468280  0.108890  0.072178  0.002432  0.266450  0.350770 -0.210100   \n",
       "60  0.137060 -0.188670 -0.160220 -0.152680  0.143570  0.142780  0.349660   \n",
       "45 -0.174850 -0.119840 -0.366770 -0.354050  0.065508 -0.085309 -0.295600   \n",
       "73 -0.020630  0.250100  0.210830  0.423430  0.263870  0.298310 -0.088201   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "67  0.820240  0.766600  0.588390  0.731570  0.763950  0.607150  0.612370   \n",
       "84  0.071453 -0.202410 -0.328480 -0.609580 -0.137080 -0.434230 -0.637690   \n",
       "29  0.302150  0.203110  0.529190 -0.458170  0.033536  0.168580  0.236150   \n",
       "0   0.343120  0.045761 -0.131120  0.150340  0.180820  0.289160  0.069545   \n",
       "5   0.558360  0.356460  0.357290  0.289110  0.276200  0.442630  0.318440   \n",
       "\n",
       "        FNC8      FNC9     FNC10  ...  SBM_map55  SBM_map61  SBM_map64  \\\n",
       "83  0.024561  0.404830  0.006682  ...   0.314668  -0.114078  -0.476524   \n",
       "40 -0.089635 -0.140530 -0.208800  ...   1.200450   1.817908  -0.299583   \n",
       "60 -0.046245 -0.133690  0.324320  ...  -0.922012  -0.197890   1.585873   \n",
       "45  0.311750 -0.013669  0.513440  ...   1.200334   0.313340   0.287729   \n",
       "73  0.081132 -0.025001  0.261250  ...  -0.266496  -1.527078  -1.028776   \n",
       "..       ...       ...       ...  ...        ...        ...        ...   \n",
       "67  0.059279  0.729200 -0.121400  ...  -2.465429  -0.935685   1.415247   \n",
       "84 -0.377230 -0.295430 -0.164490  ...   0.453766  -0.163926   0.953385   \n",
       "29 -0.440140  0.279060  0.474550  ...  -0.131957   1.284969   0.859234   \n",
       "0  -0.052489  0.124000  0.477620  ...  -0.078220  -0.982331   1.070363   \n",
       "5  -0.110290  0.302120  0.055681  ...  -0.030834   1.304894   0.835378   \n",
       "\n",
       "    SBM_map67  SBM_map69  SBM_map71  SBM_map72  SBM_map73  SBM_map74  \\\n",
       "83  -0.556896   0.505738   0.873278   0.040048   0.211690   0.536933   \n",
       "40   0.740836   1.491966   0.993555  -0.043188   0.564047  -0.916360   \n",
       "60  -0.056353   0.806093  -1.517281   1.672678  -0.376343  -0.061299   \n",
       "45  -0.370420   0.224179   1.149330   1.842975   1.458239   0.729352   \n",
       "73   0.437655  -0.938700   0.215549  -0.575946   0.804762   1.351451   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "67  -0.097483   0.073954  -2.566518   0.117317  -0.249365  -0.409918   \n",
       "84   0.402673   0.025229  -1.975778   0.112912  -2.014501  -1.661052   \n",
       "29   1.034285   2.116060  -1.098940   1.072767  -0.208299  -0.473763   \n",
       "0    0.220316   0.776855  -2.022404   1.203256   1.083516   0.564201   \n",
       "5   -0.741826  -0.206366   0.476910   1.693937   1.269684   0.681868   \n",
       "\n",
       "    SBM_map75  \n",
       "83  -0.424864  \n",
       "40   2.771659  \n",
       "60  -0.945018  \n",
       "45   0.522059  \n",
       "73   0.619411  \n",
       "..        ...  \n",
       "67  -0.384427  \n",
       "84  -0.421040  \n",
       "29   0.762586  \n",
       "0   -0.002006  \n",
       "5    1.511061  \n",
       "\n",
       "[68 rows x 410 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c3ee16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35325298,  0.29549155,  0.00394221, ...,  0.2626707 ,\n",
       "        -0.26928842, -0.40556264],\n",
       "       [ 0.3752999 ,  0.21652031, -0.13274395, ..., -0.21425363,\n",
       "        -0.13988139,  0.18686482],\n",
       "       [ 0.28138414,  0.22083123, -0.09751541, ...,  0.13979204,\n",
       "        -0.16703935, -0.9001069 ],\n",
       "       ...,\n",
       "       [ 0.27666494,  0.18889482, -0.1312739 , ...,  0.13291588,\n",
       "         0.00901235,  1.2514106 ],\n",
       "       [ 0.3432356 ,  0.26523814, -0.01914146, ..., -0.30108863,\n",
       "        -0.67390233, -0.9425636 ],\n",
       "       [ 0.26021218,  0.14602269, -0.06723667, ...,  1.1386008 ,\n",
       "         0.14941818, -0.36245304]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565102ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error in X_train data prediction: tf.Tensor(480.35306, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "480.3530193819757"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Precisión en partición de test (datos no etiquetados)\n",
    "test_pred = autoencoder.predict(test)\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "# print(\"Mean squared error in X_train data prediction:\", mse(X_train, X_train_pred).numpy())\n",
    "print(\"Mean absolute percentage error in X_train data prediction:\", mape(test.to_numpy()[:, 0], test_pred[:, 0]))\n",
    "sum(abs((test.to_numpy()[:, 0] - test_pred[:, 0]) / test.to_numpy()[:, 0])) * (100 / len(test_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c38d1",
   "metadata": {},
   "source": [
    "Un error de MAPE superior al 100% indica un error muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4813a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "462c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_encoder = encoder # loading the previously saved model.\n",
    "\n",
    "new_encoder = models.Sequential()\n",
    "new_encoder.add(prev_encoder)\n",
    "new_encoder.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 874ms/step - loss: 0.7230 - acc: 0.5294 - val_loss: 0.7189 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.7166 - acc: 0.5294 - val_loss: 0.7149 - val_acc: 0.2941\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.7128 - acc: 0.4706 - val_loss: 0.7123 - val_acc: 0.3529\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.7102 - acc: 0.5098 - val_loss: 0.7100 - val_acc: 0.2941\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7081 - acc: 0.5098 - val_loss: 0.7081 - val_acc: 0.2941\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.7060 - acc: 0.5294 - val_loss: 0.7068 - val_acc: 0.3529\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.7072 - acc: 0.4902 - val_loss: 0.7064 - val_acc: 0.3529\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7041 - acc: 0.5294 - val_loss: 0.7055 - val_acc: 0.4118\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.7038 - acc: 0.5686 - val_loss: 0.7051 - val_acc: 0.4118\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.7025 - acc: 0.5882 - val_loss: 0.7043 - val_acc: 0.4118\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7016 - acc: 0.5686 - val_loss: 0.7035 - val_acc: 0.5294\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7011 - acc: 0.5490 - val_loss: 0.7025 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7006 - acc: 0.4902 - val_loss: 0.7023 - val_acc: 0.5882\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6991 - acc: 0.5098 - val_loss: 0.7016 - val_acc: 0.5882\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6984 - acc: 0.4902 - val_loss: 0.7012 - val_acc: 0.5882\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6976 - acc: 0.4902 - val_loss: 0.7007 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6982 - acc: 0.4902 - val_loss: 0.7000 - val_acc: 0.6471\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6965 - acc: 0.4902 - val_loss: 0.6994 - val_acc: 0.6471\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6954 - acc: 0.4510 - val_loss: 0.6990 - val_acc: 0.6471\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6949 - acc: 0.4314 - val_loss: 0.6985 - val_acc: 0.6471\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6944 - acc: 0.4510 - val_loss: 0.6980 - val_acc: 0.5882\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6934 - acc: 0.4510 - val_loss: 0.6977 - val_acc: 0.5882\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6928 - acc: 0.4510 - val_loss: 0.6974 - val_acc: 0.5882\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6927 - acc: 0.4510 - val_loss: 0.6970 - val_acc: 0.5882\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6917 - acc: 0.4314 - val_loss: 0.6968 - val_acc: 0.5882\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6912 - acc: 0.4314 - val_loss: 0.6965 - val_acc: 0.5882\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6910 - acc: 0.4510 - val_loss: 0.6961 - val_acc: 0.5882\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6903 - acc: 0.4314 - val_loss: 0.6958 - val_acc: 0.5882\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6895 - acc: 0.4510 - val_loss: 0.6955 - val_acc: 0.5882\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6893 - acc: 0.4706 - val_loss: 0.6952 - val_acc: 0.5882\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6890 - acc: 0.4706 - val_loss: 0.6951 - val_acc: 0.5882\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6881 - acc: 0.4706 - val_loss: 0.6949 - val_acc: 0.5882\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6886 - acc: 0.4706 - val_loss: 0.6948 - val_acc: 0.5882\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6873 - acc: 0.4706 - val_loss: 0.6946 - val_acc: 0.5882\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6869 - acc: 0.4706 - val_loss: 0.6944 - val_acc: 0.5882\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6862 - acc: 0.4902 - val_loss: 0.6942 - val_acc: 0.5882\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6857 - acc: 0.4902 - val_loss: 0.6939 - val_acc: 0.5882\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6851 - acc: 0.4902 - val_loss: 0.6936 - val_acc: 0.5882\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6845 - acc: 0.4902 - val_loss: 0.6933 - val_acc: 0.5882\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6845 - acc: 0.4902 - val_loss: 0.6931 - val_acc: 0.5882\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6834 - acc: 0.5098 - val_loss: 0.6929 - val_acc: 0.5882\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6829 - acc: 0.5294 - val_loss: 0.6927 - val_acc: 0.5882\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6825 - acc: 0.5294 - val_loss: 0.6925 - val_acc: 0.6471\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6820 - acc: 0.5294 - val_loss: 0.6924 - val_acc: 0.6471\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6827 - acc: 0.5294 - val_loss: 0.6922 - val_acc: 0.6471\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6810 - acc: 0.5490 - val_loss: 0.6921 - val_acc: 0.6471\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6805 - acc: 0.5294 - val_loss: 0.6918 - val_acc: 0.6471\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6804 - acc: 0.5490 - val_loss: 0.6917 - val_acc: 0.6471\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6794 - acc: 0.5490 - val_loss: 0.6915 - val_acc: 0.6471\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6790 - acc: 0.5490 - val_loss: 0.6913 - val_acc: 0.6471\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6793 - acc: 0.5490 - val_loss: 0.6912 - val_acc: 0.6471\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6784 - acc: 0.5490 - val_loss: 0.6910 - val_acc: 0.6471\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6781 - acc: 0.5686 - val_loss: 0.6909 - val_acc: 0.6471\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6772 - acc: 0.5686 - val_loss: 0.6906 - val_acc: 0.6471\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6774 - acc: 0.5686 - val_loss: 0.6905 - val_acc: 0.6471\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6764 - acc: 0.5686 - val_loss: 0.6903 - val_acc: 0.6471\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6760 - acc: 0.5686 - val_loss: 0.6901 - val_acc: 0.6471\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6758 - acc: 0.5686 - val_loss: 0.6899 - val_acc: 0.6471\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6749 - acc: 0.5686 - val_loss: 0.6897 - val_acc: 0.6471\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6756 - acc: 0.5686 - val_loss: 0.6896 - val_acc: 0.6471\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6741 - acc: 0.5686 - val_loss: 0.6895 - val_acc: 0.6471\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6739 - acc: 0.5686 - val_loss: 0.6893 - val_acc: 0.6471\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6740 - acc: 0.5686 - val_loss: 0.6892 - val_acc: 0.6471\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6732 - acc: 0.5686 - val_loss: 0.6890 - val_acc: 0.6471\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6727 - acc: 0.5686 - val_loss: 0.6889 - val_acc: 0.6471\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6720 - acc: 0.5686 - val_loss: 0.6888 - val_acc: 0.6471\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6716 - acc: 0.5686 - val_loss: 0.6886 - val_acc: 0.6471\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6711 - acc: 0.5686 - val_loss: 0.6884 - val_acc: 0.6471\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6705 - acc: 0.5686 - val_loss: 0.6881 - val_acc: 0.6471\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6700 - acc: 0.5686 - val_loss: 0.6879 - val_acc: 0.6471\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6698 - acc: 0.5686 - val_loss: 0.6877 - val_acc: 0.6471\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6690 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.6471\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6685 - acc: 0.5686 - val_loss: 0.6872 - val_acc: 0.6471\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6679 - acc: 0.5686 - val_loss: 0.6870 - val_acc: 0.6471\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6673 - acc: 0.5686 - val_loss: 0.6868 - val_acc: 0.6471\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6667 - acc: 0.5686 - val_loss: 0.6865 - val_acc: 0.6471\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6664 - acc: 0.5686 - val_loss: 0.6863 - val_acc: 0.6471\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6655 - acc: 0.5686 - val_loss: 0.6860 - val_acc: 0.6471\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6651 - acc: 0.5686 - val_loss: 0.6859 - val_acc: 0.6471\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6653 - acc: 0.5882 - val_loss: 0.6856 - val_acc: 0.6471\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6638 - acc: 0.6078 - val_loss: 0.6855 - val_acc: 0.6471\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6633 - acc: 0.6078 - val_loss: 0.6852 - val_acc: 0.6471\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6638 - acc: 0.5686 - val_loss: 0.6850 - val_acc: 0.6471\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6625 - acc: 0.6078 - val_loss: 0.6849 - val_acc: 0.6471\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6623 - acc: 0.6275 - val_loss: 0.6847 - val_acc: 0.6471\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6614 - acc: 0.6275 - val_loss: 0.6846 - val_acc: 0.6471\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6622 - acc: 0.6275 - val_loss: 0.6845 - val_acc: 0.6471\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6605 - acc: 0.6078 - val_loss: 0.6844 - val_acc: 0.6471\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6601 - acc: 0.6275 - val_loss: 0.6842 - val_acc: 0.6471\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6596 - acc: 0.6275 - val_loss: 0.6840 - val_acc: 0.6471\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6592 - acc: 0.6275 - val_loss: 0.6838 - val_acc: 0.6471\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6589 - acc: 0.6275 - val_loss: 0.6837 - val_acc: 0.6471\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6598 - acc: 0.6275 - val_loss: 0.6836 - val_acc: 0.6471\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6582 - acc: 0.5882 - val_loss: 0.6834 - val_acc: 0.6471\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6584 - acc: 0.6078 - val_loss: 0.6833 - val_acc: 0.6471\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6570 - acc: 0.6275 - val_loss: 0.6831 - val_acc: 0.6471\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6567 - acc: 0.6275 - val_loss: 0.6830 - val_acc: 0.6471\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6578 - acc: 0.6275 - val_loss: 0.6829 - val_acc: 0.6471\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6561 - acc: 0.6275 - val_loss: 0.6828 - val_acc: 0.6471\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6558 - acc: 0.6078 - val_loss: 0.6825 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6867 - acc: 0.5556\n",
      "Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1913fc",
   "metadata": {},
   "source": [
    "Tras unas pocas iteraciones, descongelamos todas las capas y hacemos unas pocas épocas más entrenando y actualizando los pesos para el modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f79e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 3s 759ms/step - loss: 0.6478 - acc: 0.6275 - val_loss: 0.6612 - val_acc: 0.5882\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5367 - acc: 0.8431 - val_loss: 0.6574 - val_acc: 0.5882\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4409 - acc: 0.9216 - val_loss: 0.6530 - val_acc: 0.6471\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3530 - acc: 0.9412 - val_loss: 0.6662 - val_acc: 0.6471\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2746 - acc: 1.0000 - val_loss: 0.6828 - val_acc: 0.6471\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2111 - acc: 1.0000 - val_loss: 0.6944 - val_acc: 0.7059\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1920 - acc: 0.9804 - val_loss: 0.6964 - val_acc: 0.5294\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1353 - acc: 1.0000 - val_loss: 0.7327 - val_acc: 0.6471\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.0937 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.6471\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0725 - acc: 1.0000 - val_loss: 0.7543 - val_acc: 0.6471\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0559 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.6471\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0444 - acc: 1.0000 - val_loss: 0.8238 - val_acc: 0.6471\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.8154 - val_acc: 0.5882\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.8490 - val_acc: 0.6471\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.8439 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6034 - acc: 0.7778\n",
      "Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1258",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "def create_submission(pred, test_id=testFNC[\"Id\"]):\n",
    "    submissionDF = pd.DataFrame(list(zip(test_id, pred)), columns=[\"Id\", \"Probability\"])\n",
    "    print(submissionDF.shape) # Comprobación del tamaño, debe ser: (119748, 2)\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mmin\")\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    parent_path = current_path.parent\n",
    "    submissionDF.to_csv(f\"{parent_path}\\submissions\\MLSP_submission_AE_{current_time}.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
