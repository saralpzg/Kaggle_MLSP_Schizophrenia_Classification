{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f259b",
   "metadata": {},
   "source": [
    "Este notebook recoge los resultados de una serie de métodos para tratar de mejorar el entrenamiento de una red neuronal o poder dar uso al conjunto de datos no etiquetado.\n",
    "\n",
    "Las pruebas se van a realizar sobre la configuración de red óptima encontrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40286d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de optimización de hiperparámetros\n",
    "import optuna\n",
    "\n",
    "# Modelo\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks, backend, preprocessing, regularizers\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea4b44",
   "metadata": {},
   "source": [
    "Vamos a usar la siguiente partición de los datos:\n",
    "\n",
    "* 60% train $\\sim$ 50 datos\n",
    "* 20% validation $\\sim$ 18 datos (se define al aplicar cross-validación en el ajuste)\n",
    "* 20% test $\\sim$ 18 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3014e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231ee89",
   "metadata": {},
   "source": [
    "###  Modelo óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516cc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# En primer lugar, hay que adaptar los datos\n",
    "NUM_CLASSES = 2\n",
    "y_train_softmax = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_softmax = np_utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e4bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 9s 1s/step - loss: 0.6864 - acc: 0.5686 - val_loss: 0.6845 - val_acc: 0.6471\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5365 - acc: 0.7451 - val_loss: 0.7852 - val_acc: 0.6471\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4486 - acc: 0.7647 - val_loss: 0.7125 - val_acc: 0.7059\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.2293 - acc: 0.9804 - val_loss: 0.7357 - val_acc: 0.6471\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0894 - acc: 1.0000 - val_loss: 0.9013 - val_acc: 0.7647\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.9585 - val_acc: 0.7059\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9094 - val_acc: 0.6471\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5813 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_opt = models.Sequential()\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_opt.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_opt.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_opt.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_original = model_opt.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_original * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebafe",
   "metadata": {},
   "source": [
    "# Función de pérdida personalizada\n",
    "\n",
    "Se trata de tener en cuenta los datos con peores resultados en el backpropagation, es decir, asignar un mayor peso a esos datos, definiendo propiamente una función de loss personalizada.\n",
    "\n",
    "Fuente: https://stackoverflow.com/questions/48720197/weight-samples-if-incorrect-guessed-in-binary-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad055d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, tp_weight=0.5, tn_weight=0.5, fp_weight=1, fn_weight=1):\n",
    "    '''\n",
    "    Función de pérdida personalizada para el optimizador de una red neuronal.\n",
    "    El método recibe las predicciones y el valor real de la clasificación, así como los pesos que se desea asignar a los\n",
    "    clasificaciones tanto erróneas como acertadas.\n",
    "    '''\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_classes = tf.keras.backend.greater_equal(y_pred, 0.5)\n",
    "    y_pred_classes_float = tf.keras.backend.cast(y_pred_classes, tf.keras.backend.floatx())\n",
    "    y_true_float = tf.keras.backend.cast(y_true, tf.keras.backend.floatx())\n",
    "\n",
    "    # Get misclassified examples\n",
    "    wrongly_classified = tf.keras.backend.not_equal(y_true_float, y_pred_classes_float)\n",
    "    wrongly_classified_float = tf.keras.backend.cast(wrongly_classified, tf.keras.backend.floatx())\n",
    "    wrongly_classified_float2 = tf.gather(wrongly_classified_float, [0], axis=1)\n",
    "        \n",
    "    # Get correctly classified examples\n",
    "    correctly_classified = tf.keras.backend.equal(y_true_float, y_pred_classes_float)\n",
    "    correctly_classified_float = tf.keras.backend.cast(correctly_classified, tf.keras.backend.floatx())\n",
    "    correctly_classified_float2 = tf.gather(correctly_classified_float, [0], axis=1)\n",
    "    \n",
    "    # Get tp, fp, tn, fn\n",
    "    tp = correctly_classified_float * y_true_float\n",
    "    tn = correctly_classified_float * (1 - y_true_float)\n",
    "    fp = wrongly_classified_float * y_true_float\n",
    "    fn = wrongly_classified_float * (1 - y_true_float)\n",
    "\n",
    "    # Get weights\n",
    "    weight = tp_weight * tp + fp_weight * fp + tn_weight * tn + fn_weight * fn\n",
    "    weight2 = tf.gather(weight, [0], axis=1)\n",
    "    weight3 = tf.math.reduce_sum(weight2, axis=1)\n",
    "    \n",
    "    loss = tf.keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weight3\n",
    "        \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73f781",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que los anteriores ``tp``, ``fn``, ... son tensores (vectores de ``tensorflow``) y por tanto, el resultado guardado en ``weight`` es igualmente otro tensor, no un único valor numérico.\n",
    "\n",
    "_NOTA: se entienden los positivos como aquellas muestras correspondientes con una etiqueta \"1\" (enfermos) y por tanto como negativos los registros con etiqueta \"0\" (grupo de control)._\n",
    "\n",
    "Vamos a probar cómo funciona la función personalizada sobre los modelos de redes neuronales que peores resultados han obtenido para comprobar si de este modo podemos mejorar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb9f5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 837ms/step - loss: 0.5316 - acc: 0.5686 - val_loss: 0.4673 - val_acc: 0.7059\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3406 - acc: 0.9020 - val_loss: 0.5205 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2365 - acc: 0.9804 - val_loss: 0.5716 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1206 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3440 - acc: 0.7843 - val_loss: 1.0695 - val_acc: 0.6471\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1280 - acc: 0.9412 - val_loss: 0.7024 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.7442 - val_acc: 0.5882\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.7961 - val_acc: 0.5882\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.8185 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4409 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_CL = models.Sequential()\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_CL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_CL.compile(loss=custom_loss, optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=5)\n",
    "model_CL.fit(X_train, y_train_softmax, epochs=100, validation_split=0.25, callbacks=[es])\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_CL = model_CL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_CL * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a69621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_CL = model_CL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_CL, \"NN_CustomLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16fe8",
   "metadata": {},
   "source": [
    "# Sample-weight\n",
    "\n",
    "Una alternativa a lo anterior, sería usar un vector de pesos propio y pasarlo a la función ``fit`` del modelo, a través del parámetro ``sample_weight``, en lugar de definir una función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065bb881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 931ms/step - loss: 0.3475 - acc: 0.5686 - val_loss: 0.3414 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 329ms/step - loss: 0.2470 - acc: 0.5098 - val_loss: 0.2745 - val_acc: 0.4118\n",
      "2/2 [==============================] - 0s 334ms/step - loss: 0.3152 - acc: 0.6471 - val_loss: 0.2306 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 403ms/step - loss: 0.2156 - acc: 0.7059 - val_loss: 0.2567 - val_acc: 0.5294\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 0.1106 - acc: 0.9608 - val_loss: 0.3408 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.3547 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.3733 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.3857 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 412ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3920 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 305ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3987 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 255ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4053 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 362ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4121 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 302ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4256 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 281ms/step - loss: 9.4525e-04 - acc: 1.0000 - val_loss: 0.4327 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 348ms/step - loss: 7.3577e-04 - acc: 1.0000 - val_loss: 0.4398 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 364ms/step - loss: 5.7974e-04 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 292ms/step - loss: 4.6185e-04 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 3.7296e-04 - acc: 1.0000 - val_loss: 0.4581 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 326ms/step - loss: 3.0341e-04 - acc: 1.0000 - val_loss: 0.4635 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 258ms/step - loss: 2.4874e-04 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 2.0503e-04 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 1.6953e-04 - acc: 1.0000 - val_loss: 0.4818 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 1.4058e-04 - acc: 1.0000 - val_loss: 0.4879 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 1.1711e-04 - acc: 1.0000 - val_loss: 0.4939 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 264ms/step - loss: 9.7745e-05 - acc: 1.0000 - val_loss: 0.4996 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 373ms/step - loss: 8.1903e-05 - acc: 1.0000 - val_loss: 0.5055 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 310ms/step - loss: 6.8878e-05 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 316ms/step - loss: 5.8192e-05 - acc: 1.0000 - val_loss: 0.5162 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 314ms/step - loss: 4.9310e-05 - acc: 1.0000 - val_loss: 0.5215 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 352ms/step - loss: 4.1931e-05 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 261ms/step - loss: 3.5722e-05 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 374ms/step - loss: 3.0528e-05 - acc: 1.0000 - val_loss: 0.5370 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 317ms/step - loss: 2.6139e-05 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 360ms/step - loss: 2.2429e-05 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 320ms/step - loss: 1.9267e-05 - acc: 1.0000 - val_loss: 0.5518 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 314ms/step - loss: 1.6596e-05 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 271ms/step - loss: 1.4319e-05 - acc: 1.0000 - val_loss: 0.5605 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 1.2385e-05 - acc: 1.0000 - val_loss: 0.5645 - val_acc: 0.5882\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 1.0727e-05 - acc: 1.0000 - val_loss: 0.5684 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 9.3051e-06 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 290ms/step - loss: 8.0796e-06 - acc: 1.0000 - val_loss: 0.5762 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 320ms/step - loss: 7.0269e-06 - acc: 1.0000 - val_loss: 0.5802 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 334ms/step - loss: 6.1198e-06 - acc: 1.0000 - val_loss: 0.5840 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 329ms/step - loss: 5.3400e-06 - acc: 1.0000 - val_loss: 0.5880 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 384ms/step - loss: 4.6716e-06 - acc: 1.0000 - val_loss: 0.5918 - val_acc: 0.6471\n",
      "2/2 [==============================] - 0s 359ms/step - loss: 4.0940e-06 - acc: 1.0000 - val_loss: 0.5958 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 313ms/step - loss: 3.5943e-06 - acc: 1.0000 - val_loss: 0.5993 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 328ms/step - loss: 3.1564e-06 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 325ms/step - loss: 2.7761e-06 - acc: 1.0000 - val_loss: 0.6066 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 317ms/step - loss: 2.4437e-06 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 293ms/step - loss: 2.1534e-06 - acc: 1.0000 - val_loss: 0.6142 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 295ms/step - loss: 1.8983e-06 - acc: 1.0000 - val_loss: 0.6177 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 265ms/step - loss: 1.6762e-06 - acc: 1.0000 - val_loss: 0.6213 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 1.4832e-06 - acc: 1.0000 - val_loss: 0.6243 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 1.3140e-06 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 1.1662e-06 - acc: 1.0000 - val_loss: 0.6308 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 264ms/step - loss: 1.0358e-06 - acc: 1.0000 - val_loss: 0.6341 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 309ms/step - loss: 9.2125e-07 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 282ms/step - loss: 8.2014e-07 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 309ms/step - loss: 7.3192e-07 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 337ms/step - loss: 6.5402e-07 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 343ms/step - loss: 5.8538e-07 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 306ms/step - loss: 5.2465e-07 - acc: 1.0000 - val_loss: 0.6509 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 327ms/step - loss: 4.7095e-07 - acc: 1.0000 - val_loss: 0.6534 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 4.2346e-07 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 442ms/step - loss: 3.8142e-07 - acc: 1.0000 - val_loss: 0.6581 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 440ms/step - loss: 3.4407e-07 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 386ms/step - loss: 3.1094e-07 - acc: 1.0000 - val_loss: 0.6625 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 447ms/step - loss: 2.8134e-07 - acc: 1.0000 - val_loss: 0.6647 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 253ms/step - loss: 2.5497e-07 - acc: 1.0000 - val_loss: 0.6667 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 2.3148e-07 - acc: 1.0000 - val_loss: 0.6686 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 2.1037e-07 - acc: 1.0000 - val_loss: 0.6706 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 333ms/step - loss: 1.9153e-07 - acc: 1.0000 - val_loss: 0.6725 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 376ms/step - loss: 1.7461e-07 - acc: 1.0000 - val_loss: 0.6743 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 355ms/step - loss: 1.5947e-07 - acc: 1.0000 - val_loss: 0.6761 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 276ms/step - loss: 1.4580e-07 - acc: 1.0000 - val_loss: 0.6780 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 308ms/step - loss: 1.3357e-07 - acc: 1.0000 - val_loss: 0.6797 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 266ms/step - loss: 1.2249e-07 - acc: 1.0000 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 315ms/step - loss: 1.1253e-07 - acc: 1.0000 - val_loss: 0.6833 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 334ms/step - loss: 1.0349e-07 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 363ms/step - loss: 9.5368e-08 - acc: 1.0000 - val_loss: 0.6867 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 275ms/step - loss: 8.7987e-08 - acc: 1.0000 - val_loss: 0.6886 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 305ms/step - loss: 8.1333e-08 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 399ms/step - loss: 7.5265e-08 - acc: 1.0000 - val_loss: 0.6919 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 274ms/step - loss: 6.9781e-08 - acc: 1.0000 - val_loss: 0.6934 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 295ms/step - loss: 6.4738e-08 - acc: 1.0000 - val_loss: 0.6950 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 6.0196e-08 - acc: 1.0000 - val_loss: 0.6966 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 5.6040e-08 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 5.2261e-08 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 4.8824e-08 - acc: 1.0000 - val_loss: 0.7013 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.5657e-08 - acc: 1.0000 - val_loss: 0.7028 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.2732e-08 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.0080e-08 - acc: 1.0000 - val_loss: 0.7058 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 3.7669e-08 - acc: 1.0000 - val_loss: 0.7073 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.5415e-08 - acc: 1.0000 - val_loss: 0.7087 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.3342e-08 - acc: 1.0000 - val_loss: 0.7102 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.1413e-08 - acc: 1.0000 - val_loss: 0.7116 - val_acc: 0.7059\n",
      "2/2 [==============================] - 0s 260ms/step - loss: 2.9671e-08 - acc: 1.0000 - val_loss: 0.7130 - val_acc: 0.7059\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5105 - acc: 0.8889\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0) # Reproducibilidad de resultados\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_SW = models.Sequential()\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_SW.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_SW.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "args = y_train_softmax.astype(bool)\n",
    "y_pred = model_SW.predict(X_train)\n",
    "samples = 1 - y_pred[args]\n",
    "for epoch in range(100):\n",
    "    model_SW.fit(X_train, y_train_softmax, epochs=1, validation_split=0.25, sample_weight=samples)\n",
    "    y_pred = model_SW.predict(X_train)\n",
    "    samples = 1 - y_pred[args]\n",
    "    \n",
    "# Precisión en partición de test\n",
    "loss, accuracy_SW = model_SW.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_SW * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0965d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_SW = model_SW.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_SW, \"NN_SampleWeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570984a",
   "metadata": {},
   "source": [
    "# Pseudo-labeling\n",
    "\n",
    "Finalmente, esta técnica va a permitir aprovechar el conjunto de datos no etiquetado (método de aprendizaje semi-automático).\n",
    "\n",
    "Fuente: https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04feba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_epoch(epoch, val, start, stop):\n",
    "    if epoch < start:\n",
    "        alpha = 0\n",
    "    elif epoch < stop:\n",
    "        alpha = ((epoch-start) / (stop-start)) * val\n",
    "    else:\n",
    "        alpha = val\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba7172a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step - loss: 16.3597 - acc: 0.7778\n",
      "Accuracy: 77.78% ------- alpha = 0.25\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 16.8173 - acc: 0.8333\n",
      "Accuracy: 83.33% ------- alpha = 0.50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 16.9020 - acc: 0.8333\n",
      "Accuracy: 83.33% ------- alpha = 0.75\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 16.9690 - acc: 0.8333\n",
      "Accuracy: 83.33% ------- alpha = 1.00\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha_values = [0.25, 0.5, 0.75, 1]\n",
    "iters = 100\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for i in range(iters):\n",
    "        pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "        y_tot = np.concatenate((y_train_softmax, pseudolabels))\n",
    "        alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "        samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "        model_PL.fit(X_tot, y_tot, sample_weight=samples, epochs=1, validation_split=0.25, verbose = 0)\n",
    "    \n",
    "    # Precisión en partición de test\n",
    "    loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "    print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3114c",
   "metadata": {},
   "source": [
    "Vamos a pintar la evolución del valor del peso $\\alpha$ (función $\\alpha(t)$) elegido a lo largo de 100 iteraciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4838bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start = 15\n",
    "stop = 90\n",
    "alpha = 0.5\n",
    "iters = 100\n",
    "\n",
    "alpha_per_epoch = []\n",
    "for i in range(iters):\n",
    "    alpha_per_epoch.append(alpha_epoch(i+1, alpha, start, stop))\n",
    "    \n",
    "# Pintamos el vector\n",
    "plt.plot(alpha_per_epoch)\n",
    "plt.xlabel(\"Iteración\", fontsize=13)\n",
    "plt.ylabel(r\"$\\alpha(t)$\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bda9ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 2s 6ms/step - loss: 0.0077 - acc: 0.4810 - val_loss: 0.0000e+00 - val_acc: 0.4930\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0043 - acc: 0.6807 - val_loss: 0.0000e+00 - val_acc: 0.8621\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0021 - acc: 0.7538 - val_loss: 0.0000e+00 - val_acc: 0.5681\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 8.7677e-04 - acc: 0.5530 - val_loss: 0.0000e+00 - val_acc: 0.5769\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 4.0722e-04 - acc: 0.8555 - val_loss: 0.0000e+00 - val_acc: 0.8975\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.8771 - val_loss: 0.0000e+00 - val_acc: 0.9456\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 0.0015 - acc: 0.9405 - val_loss: 0.0000e+00 - val_acc: 0.9293\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 1.0100e-09 - acc: 0.9980 - val_loss: 0.0000e+00 - val_acc: 0.9996\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 5.0687e-10 - acc: 0.9977 - val_loss: 0.0000e+00 - val_acc: 0.9984\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 3.0169e-10 - acc: 0.9985 - val_loss: 0.0000e+00 - val_acc: 0.9984\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 2.1888e-10 - acc: 0.9995 - val_loss: 0.0000e+00 - val_acc: 0.9988\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.7288e-10 - acc: 0.9996 - val_loss: 0.0000e+00 - val_acc: 0.9996\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.4388e-10 - acc: 0.9997 - val_loss: 0.0000e+00 - val_acc: 0.9988\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 1.2292e-10 - acc: 0.9996 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 1.0742e-10 - acc: 0.9999 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9281 - val_loss: 7.3538e-04 - val_acc: 0.9499\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9582 - val_loss: 0.0016 - val_acc: 0.9607\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0016 - acc: 0.9698 - val_loss: 0.0024 - val_acc: 0.9551\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9795 - val_loss: 0.0031 - val_acc: 0.9599\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0016 - acc: 0.9848 - val_loss: 0.0064 - val_acc: 0.9337\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9824 - val_loss: 0.0091 - val_acc: 0.9233\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0019 - acc: 0.9870 - val_loss: 0.0072 - val_acc: 0.9567\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0016 - acc: 0.9891 - val_loss: 0.0230 - val_acc: 0.8991\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0032 - acc: 0.9828 - val_loss: 0.0086 - val_acc: 0.9396\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0017 - acc: 0.9927 - val_loss: 0.0093 - val_acc: 0.9607\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9928 - val_loss: 0.0120 - val_acc: 0.9543\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0019 - acc: 0.9928 - val_loss: 0.0173 - val_acc: 0.9416\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9939 - val_loss: 0.0152 - val_acc: 0.9583\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0012 - acc: 0.9970 - val_loss: 0.0128 - val_acc: 0.9634\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0019 - acc: 0.9943 - val_loss: 0.0145 - val_acc: 0.9567\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9956 - val_loss: 0.0146 - val_acc: 0.9575\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0018 - acc: 0.9952 - val_loss: 0.0309 - val_acc: 0.9337\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9963 - val_loss: 0.0236 - val_acc: 0.9360\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0023 - acc: 0.9947 - val_loss: 0.0234 - val_acc: 0.9555\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9952 - val_loss: 0.0178 - val_acc: 0.9638\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9976 - val_loss: 0.0310 - val_acc: 0.9623\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0024 - acc: 0.9964 - val_loss: 0.0216 - val_acc: 0.9603\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9956 - val_loss: 0.0226 - val_acc: 0.9539\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0018 - acc: 0.9971 - val_loss: 0.0211 - val_acc: 0.9591\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9970 - val_loss: 0.0344 - val_acc: 0.9595\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0022 - acc: 0.9970 - val_loss: 0.0284 - val_acc: 0.9615\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0011 - acc: 0.9980 - val_loss: 0.0240 - val_acc: 0.9682\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9979 - val_loss: 0.0495 - val_acc: 0.9615\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9977 - val_loss: 0.0274 - val_acc: 0.9623\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0013 - acc: 0.9989 - val_loss: 0.0253 - val_acc: 0.9634\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0017 - acc: 0.9977 - val_loss: 0.0314 - val_acc: 0.9690\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0010 - acc: 0.9988 - val_loss: 0.0430 - val_acc: 0.9674\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0024 - acc: 0.9977 - val_loss: 0.0274 - val_acc: 0.9615\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0011 - acc: 0.9981 - val_loss: 0.0564 - val_acc: 0.9646\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 0.0382 - val_acc: 0.9634\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 0.1111 - val_acc: 0.9539\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0039 - acc: 0.9951 - val_loss: 0.0409 - val_acc: 0.9619\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0034 - acc: 0.9967 - val_loss: 0.0771 - val_acc: 0.9289\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0035 - acc: 0.9966 - val_loss: 0.0480 - val_acc: 0.9583\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 0.1116 - val_acc: 0.9543\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0038 - acc: 0.9959 - val_loss: 0.0711 - val_acc: 0.9384\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9991 - val_loss: 0.0608 - val_acc: 0.9666\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0019 - acc: 0.9987 - val_loss: 0.0886 - val_acc: 0.9487\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0028 - acc: 0.9970 - val_loss: 0.0656 - val_acc: 0.9642\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0013 - acc: 0.9991 - val_loss: 0.0533 - val_acc: 0.9646\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9991 - val_loss: 0.0831 - val_acc: 0.9480\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0060 - acc: 0.9968 - val_loss: 0.0386 - val_acc: 0.9631\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0396 - val_acc: 0.9511\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9987 - val_loss: 0.0815 - val_acc: 0.9682\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0034 - acc: 0.9983 - val_loss: 0.0548 - val_acc: 0.9575\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9987 - val_loss: 0.0849 - val_acc: 0.9654\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0036 - acc: 0.9981 - val_loss: 0.0299 - val_acc: 0.9730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0041 - acc: 0.9975 - val_loss: 0.1181 - val_acc: 0.9543\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0573 - val_acc: 0.9682\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0013 - acc: 0.9992 - val_loss: 0.1040 - val_acc: 0.9738\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0738 - val_acc: 0.9678\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0051 - acc: 0.9975 - val_loss: 0.0777 - val_acc: 0.9428\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9974 - val_loss: 0.1517 - val_acc: 0.9380\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0065 - acc: 0.9954 - val_loss: 0.0584 - val_acc: 0.9515\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0594 - val_acc: 0.9722\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0904 - val_acc: 0.9690\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0046 - acc: 0.9971 - val_loss: 0.0512 - val_acc: 0.9674\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0702 - val_acc: 0.9587\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 0.0678 - val_acc: 0.9718\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 4.9878e-04 - acc: 0.9996 - val_loss: 0.0929 - val_acc: 0.9785\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 0.0587 - val_acc: 0.9654\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 8.3827e-04 - acc: 0.9996 - val_loss: 0.1232 - val_acc: 0.9662\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0894 - val_acc: 0.9559\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0642 - val_acc: 0.9599\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.2325 - val_acc: 0.9595\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.0904 - val_acc: 0.9452\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 5.4219e-04 - acc: 0.9997 - val_loss: 0.3196 - val_acc: 0.9603\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.1093 - val_acc: 0.9650\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9992 - val_loss: 0.0712 - val_acc: 0.9678\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.3458 - val_acc: 0.9642\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.0102 - acc: 0.9983 - val_loss: 0.0515 - val_acc: 0.9646\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 0.0585 - val_acc: 0.9631\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0406 - val_acc: 0.9718\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0896 - val_acc: 0.9722\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0842 - val_acc: 0.9742\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 2.8265e-04 - acc: 0.9997 - val_loss: 0.1115 - val_acc: 0.9750\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1868 - val_acc: 0.9710\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0537 - val_acc: 0.9627\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1126 - val_acc: 0.9690\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.0038 - acc: 0.9980 - val_loss: 0.1597 - val_acc: 0.9619\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 4.4990 - acc: 0.8333\n",
      "Accuracy: 83.33% ------- alpha = 0.50\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "\n",
    "alpha=0.5\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model_PL = models.Sequential()\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\", input_shape=(410,)))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(200, activation=\"relu\"))\n",
    "model_PL.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_PL.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "\n",
    "test_reduc = test_kaggle.iloc[:10000, :]\n",
    "X_tot = np.concatenate((X_train, test_reduc))\n",
    "\n",
    "for i in range(iters):\n",
    "    pseudolabels = model_PL.predict(test_reduc).squeeze()\n",
    "    y_tot = np.concatenate((y_train_softmax, pseudolabels))\n",
    "    alpha_t = alpha_epoch(i+1, alpha, start, stop)\n",
    "    samples = np.concatenate((np.ones(len(y_train)), alpha_t*np.ones(len(pseudolabels))))\n",
    "    model_PL.fit(X_tot, y_tot, sample_weight=samples, epochs=1, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_PL = model_PL.evaluate(X_test, y_test_softmax)\n",
    "print(\"Accuracy: {:0.2f}% ------- alpha = {:0.2f}\".format(accuracy_PL * 100, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e13294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119748, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_PL = model_PL.predict(test_kaggle)\n",
    "\n",
    "create_submission(y_pred_PL, \"NN_PseudoLabels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a0af6",
   "metadata": {},
   "source": [
    "# Comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04d6b06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_acc = [accuracy_original, accuracy_CL, accuracy_SW, accuracy_PL]\n",
    "xaxis = [\"Red neuronal original\", \"Función de pérdida personalizada\", \n",
    "         \"Sample-weight\", \"Pseudo-labels\"]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "barlist = plt.bar(xaxis, results_acc, width=0.2)\n",
    "barlist[0].set_color(\"r\")\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(bottom=0.5)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
