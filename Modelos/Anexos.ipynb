{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e86c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de optimización de hiperparámetros\n",
    "import optuna\n",
    "\n",
    "# Modelo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluación del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451ac83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633d9bf",
   "metadata": {},
   "source": [
    "# Anexo A: sobre la optimización de los hiperparámetros en métodos de cross-validación\n",
    "\n",
    "Al principio de este estudio, se observó que en varias ocasiones distintas ejecuciones del mismo código podían retornar distintos resultados de accuracy en la partición de test. Más en concreto, se retornaban diferentes modelos óptimos si se repetían las ejecuciones de los métodos de optimización de hiperparámetros, dando lugar a valores de accuracy distintos.\n",
    "\n",
    "El motivo de esto es que las pruebas en la búsqueda de los hiperparámetros no se realizan siempre en el mismo orden y por tanto, el óptimo devuelto en cada ejecución sería la primera combinación de hiperparámetros probada que diese lugar al accuracy máximo obtenido en las pruebas, aunque no fuese el único con ese valor.\n",
    "\n",
    "El código debajo permite demostrar lo anterior, sobre un modelo clasificador de XGBoost optimizado mediante cross-validación de ``optuna``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "xgb.set_config(verbosity=0)\n",
    "\n",
    "# Modelo e hiperparámetros sobre los que se realizan las pruebas\n",
    "model_XGB = XGBClassifier(eval_metric=\"logloss\", random_state=0, use_label_encoder=False)\n",
    "param_grid_XGB_optuna = {\n",
    "    \"booster\": optuna.distributions.CategoricalDistribution([\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "    \"learning_rate\": optuna.distributions.CategoricalDistribution([0.001, 0.05, 0.1, 0.5])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be69a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster': 'dart', 'learning_rate': 0.1}\n",
      "{'booster': 'dart', 'learning_rate': 0.05}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.001}\n",
      "{'booster': 'dart', 'learning_rate': 0.1}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.001}\n",
      "{'booster': 'dart', 'learning_rate': 0.05}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.05}\n",
      "{'booster': 'dart', 'learning_rate': 0.001}\n",
      "{'booster': 'dart', 'learning_rate': 0.001}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "optuna_search = optuna.integration.OptunaSearchCV(model_XGB, param_grid_XGB_optuna, cv=4)\n",
    "optuna_search.fit(X_train, y_train)\n",
    "\n",
    "for trial in optuna_search.trials_:\n",
    "    print(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9537c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster': 'gbtree', 'learning_rate': 0.5}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.5}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.001}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.5}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.1}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.001}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.5}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.05}\n",
      "{'booster': 'gblinear', 'learning_rate': 0.05}\n",
      "{'booster': 'gbtree', 'learning_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "optuna_search = optuna.integration.OptunaSearchCV(model_XGB, param_grid_XGB_optuna, cv=4)\n",
    "optuna_search.fit(X_train, y_train)\n",
    "\n",
    "for trial in optuna_search.trials_:\n",
    "    print(trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbdc34",
   "metadata": {},
   "source": [
    "Como se puede observar, el método de optimización empleado hace que el orden en el que se realizan las pruebas no sea siempre el mismo.\n",
    "\n",
    "En este caso, este problema se puede solucionar fijando una semilla en el optimizador mediante el parámetro ``random_state=0``.\n",
    "Sin embargo, esta observación dió pie al siguiente planteamiento: sean o no reproducibles los resultados, el optimizador siempre va a retornar la primera combinación de parámetros que haya probado que genere la máxima accuracy en las pruebas y aunque esto a priori no parece ser un problema, podría estar causando una pérdida de precisión en aquellos métodos que usan cross-validación.\n",
    "\n",
    "El motivo de lo anterior se encuentra en la propia forma de entrenamiento de este tipo de métodos. La forma en la que se comprueba el accuracy de un modelo durante el entrenamiento para escoger un óptimo es, en este caso que se utiliza una cross-validación con 4 folds, calcular el accuracy en cada partición de entrenamiento, esto es, el 75% de los datos (los valores se guardan en los arrays ``split0_train_score``, ``split1_train_score``, ``split2_train_score`` y ``split3_train_score``) y la precisión final se estima promediando estos 4 valores (``mean_test_score``). \n",
    "\n",
    "Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d46d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_XGB_GridSearch = {\n",
    "    \"booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "    \"learning_rate\": [0.001, 0.05, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b9c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, param_grid, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "    '''\n",
    "    Función para realizar el entrenamiento y el ajuste de parámetros.\n",
    "    Adicionalmente retorna varias propiedades del método de cross-validación para hacer estas comprobaciones\n",
    "    '''\n",
    "    grid_search_XGB = GridSearchCV(estimator=model, param_grid=param_grid, cv=4, return_train_score=True)\n",
    "    grid_search_XGB.fit(X_train, y_train)\n",
    "    model_XGB_opt = grid_search_XGB.best_estimator_\n",
    "    \n",
    "    # Predicción en partición de test\n",
    "    y_pred_XGB = model_XGB_opt.predict(X_test)\n",
    "    \n",
    "    # Precisión en partición de test\n",
    "    accuracy = accuracy_score(y_test, y_pred_XGB)\n",
    "    \n",
    "    return accuracy, grid_search_XGB, grid_search_XGB.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29bea5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, grid, cv_results = train_model(model_XGB, param_grid_XGB_GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "206f4ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96078431 1.         1.         1.         0.98039216 1.\n",
      " 1.         1.         0.96078431 1.         1.         1.        ]\n",
      "[1.         1.         1.         1.         0.98039216 1.\n",
      " 1.         1.         1.         1.         1.         1.        ]\n",
      "[0.98039216 1.         1.         1.         0.96078431 1.\n",
      " 1.         1.         0.98039216 1.         1.         1.        ]\n",
      "[0.94117647 1.         1.         1.         0.94117647 1.\n",
      " 1.         1.         0.94117647 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results[\"split0_train_score\"])\n",
    "print(cv_results[\"split1_train_score\"])\n",
    "print(cv_results[\"split2_train_score\"])\n",
    "print(cv_results[\"split3_train_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f20c146f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.52941176, 0.54411765, 0.58823529, 0.63235294,\n",
       "       0.63235294, 0.63235294, 0.63235294, 0.5       , 0.52941176,\n",
       "       0.54411765, 0.58823529])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4834049",
   "metadata": {},
   "source": [
    "Se observa que hay por tanto 4 modelos que alcanzan la misma accuracy. En particular los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a9a940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'booster': 'gblinear', 'learning_rate': 0.001},\n",
       " {'booster': 'gblinear', 'learning_rate': 0.05},\n",
       " {'booster': 'gblinear', 'learning_rate': 0.1},\n",
       " {'booster': 'gblinear', 'learning_rate': 0.5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[\"params\"][4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3576d5a",
   "metadata": {},
   "source": [
    "De entre los resultados que se devuelven, generalmente los que mejor precisión en test permitan obtener serán aquellos más grandes o complejos, entendiendo en este caso mayor complejidad como un learning_rate más pequeño. El motivo de esto es que al hacer cross-validación el conjunto de entrenamiento es más pequeño (75% de los datos de train), por lo que al entrenar un modelo sobre el 100% de los datos, modelos menos complejos pueden resultar excesivamente sencillos al aplicarlos sobre un dataset más grande.\n",
    "\n",
    "Debajo incluimos la precisión en test de los 4 modelos anteriores, ordenados desde el más simple hasta el más complejo para observar este hecho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de2aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "XGBoost_lr1 = XGBClassifier(eval_metric=\"logloss\", booster=\"gblinear\", learning_rate=0.5, \n",
    "                            random_state=0, use_label_encoder=False)  \n",
    "XGBoost_lr1.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_XGBoost_lr1 = XGBoost_lr1.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_XGBoost_lr1)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaa8695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "XGBoost_lr2 = XGBClassifier(eval_metric=\"logloss\", booster=\"gblinear\", learning_rate=0.1, \n",
    "                            random_state=0, use_label_encoder=False)  \n",
    "XGBoost_lr2.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_XGBoost_lr2 = XGBoost_lr2.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_XGBoost_lr2)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e2ebc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.22%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "XGBoost_lr3 = XGBClassifier(eval_metric=\"logloss\", booster=\"gblinear\", learning_rate=0.05, \n",
    "                            random_state=0, use_label_encoder=False)  \n",
    "XGBoost_lr3.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_XGBoost_lr3 = XGBoost_lr3.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_XGBoost_lr3)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1c249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Definir y entrenar el modelo\n",
    "XGBoost_lr4 = XGBClassifier(eval_metric=\"logloss\", booster=\"gblinear\", learning_rate=0.001, \n",
    "                            random_state=0, use_label_encoder=False)  \n",
    "XGBoost_lr4.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en partición de test\n",
    "y_pred_XGBoost_lr4 = XGBoost_lr4.predict(X_test)\n",
    "\n",
    "# Precisión en partición de test\n",
    "accuracy = accuracy_score(y_test, y_pred_XGBoost_lr4)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426d33c",
   "metadata": {},
   "source": [
    "### Observaciones adicionales:\n",
    "\n",
    "Si se repite la anterior comprobación podríamos obtener valores de accuracy diferentes incluso durante el propio entrenamiento de cross-validación y para los mismos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9229e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.52941176, 0.54411765, 0.58823529, 0.63235294,\n",
       "       0.63235294, 0.63235294, 0.63235294, 0.5       , 0.52941176,\n",
       "       0.54411765, 0.58823529])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf4cf7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.52941176, 0.54411765, 0.58823529, 0.63235294,\n",
       "       0.63235294, 0.63235294, 0.64705882, 0.5       , 0.52941176,\n",
       "       0.54411765, 0.58823529])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, grid, cv_results = train_model(model_XGB, param_grid_XGB_GridSearch)\n",
    "cv_results[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f3f1f",
   "metadata": {},
   "source": [
    "Las variaciones se deben a las pruebas que utilizan el método ``solver = gblinear``. Este solver cuando con un algoritmo por defecto ``shotgun``, que es no determinista. El problema se soluciona si se sustituye el algoritmo de base por otro ``coord_descent`` que sí es determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0147ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan,        nan, 0.63235294,\n",
       "       0.63235294, 0.63235294, 0.69117647,        nan,        nan,\n",
       "              nan,        nan])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_XGB2 = XGBClassifier(eval_metric=\"logloss\", random_state=0, use_label_encoder=False, updater=\"coord_descent\")\n",
    "\n",
    "accuracy, grid, cv_results = train_model(model_XGB2, param_grid_XGB_GridSearch)\n",
    "cv_results[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b03010ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan,        nan, 0.63235294,\n",
       "       0.63235294, 0.63235294, 0.69117647,        nan,        nan,\n",
       "              nan,        nan])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, grid, cv_results = train_model(model_XGB2, param_grid_XGB_GridSearch)\n",
    "cv_results[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7125b",
   "metadata": {},
   "source": [
    "# Anexo B: Optuna TPE vs. búsqueda aleatoria de hiperparámetros\n",
    "\n",
    "Para la optimización de los modelos con ``optuna``, se han probado dos métodos de \"sampler\": GridSampler y TPE, siendo este último algoritmo de especial interés ya que, si resulta ser efectivo, al hacer una búsqueda inteligente de los hiperparámetros más adecuados podría ahorrar mucho tiempo y recursos de computación respecto a cualquiera de las otras alternativas vistas en este trabajo.\n",
    "\n",
    "Para comprobar la eficacia de este modelo vamos a realizar una prueba en la que compararemos 20 combinaciones de los hiperparámetros de manera aleatoria y 20 combinaciones (iteraciones) con este algoritmo de Optuna. Si TPE es eficaz, lo que se espera es que los resultados de este algoritmo mejoren los resultados de la búsqueda aleatoria y que cada iteración sea mejor que la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17adaa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_RF = {\n",
    "    \"n_estimators\": range(50, 1050, 50),\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": range(1, 21)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e65b38",
   "metadata": {},
   "source": [
    "### Resultados prueba aleatoria\n",
    "\n",
    "En primer lugar, se generan 20 combinaciones aleatorias de parámetros (de las 20 * 2 * 20 = 800 combinaciones existentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4776bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "random_params = {}\n",
    "for key in param_grid_RF.keys():\n",
    "    random_params[key] = random.choices(param_grid_RF[key], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54e10295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba aleatoria 0 ------ Accuracy: 61.11%\n",
      "Prueba aleatoria 1 ------ Accuracy: 88.89%\n",
      "Prueba aleatoria 2 ------ Accuracy: 83.33%\n",
      "Prueba aleatoria 3 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 4 ------ Accuracy: 66.67%\n",
      "Prueba aleatoria 5 ------ Accuracy: 66.67%\n",
      "Prueba aleatoria 6 ------ Accuracy: 66.67%\n",
      "Prueba aleatoria 7 ------ Accuracy: 83.33%\n",
      "Prueba aleatoria 8 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 9 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 10 ------ Accuracy: 72.22%\n",
      "Prueba aleatoria 11 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 12 ------ Accuracy: 66.67%\n",
      "Prueba aleatoria 13 ------ Accuracy: 88.89%\n",
      "Prueba aleatoria 14 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 15 ------ Accuracy: 72.22%\n",
      "Prueba aleatoria 16 ------ Accuracy: 77.78%\n",
      "Prueba aleatoria 17 ------ Accuracy: 72.22%\n",
      "Prueba aleatoria 18 ------ Accuracy: 88.89%\n",
      "Prueba aleatoria 19 ------ Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Definir y entrenar el modelo\n",
    "    modelRF_random = RandomForestClassifier(criterion=random_params[\"criterion\"][i], max_depth=random_params[\"max_depth\"][i], \n",
    "                                            n_estimators=random_params[\"n_estimators\"][i], random_state=0)  \n",
    "    modelRF_random.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción en partición de test\n",
    "    y_pred_RF_random = modelRF_random.predict(X_test)\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    accuracy = accuracy_score(y_test, y_pred_RF_random)\n",
    "    print(\"Prueba aleatoria {} ------ Accuracy: {:0.2f}%\".format(i, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b85eb2",
   "metadata": {},
   "source": [
    "### Resultados prueba con Optuna + TPE sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58b02ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveRF_TPE(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    En este caso se trata de maximizar el accuracy\n",
    "    '''\n",
    "    n_estimators =  trial.suggest_int(\"n_estimators\", 50, 1000, 50) # optuna incluye en el rango el máximo y el mínimo\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n",
    "    \n",
    "    modelRF_optuna = RandomForestClassifier(criterion = criterion, max_depth = max_depth, n_estimators = n_estimators, \n",
    "                                            random_state=0)\n",
    "    \n",
    "    modelRF_optuna.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_RF_optuna = modelRF_optuna.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_RF_optuna)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce5a2793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-02 20:08:53,155]\u001b[0m A new study created in memory with name: no-name-1fc80066-6add-4f4e-bd48-6b6458d3a37c\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:08:56,910]\u001b[0m Trial 0 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 550, 'criterion': 'gini', 'max_depth': 11}. Best is trial 0 with value: 0.6666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:01,108]\u001b[0m Trial 1 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 450, 'criterion': 'gini', 'max_depth': 18}. Best is trial 0 with value: 0.6666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:11,390]\u001b[0m Trial 2 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 1000, 'criterion': 'entropy', 'max_depth': 11}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:16,588]\u001b[0m Trial 3 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 600, 'criterion': 'gini', 'max_depth': 2}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:17,115]\u001b[0m Trial 4 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 18}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:26,446]\u001b[0m Trial 5 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 1000, 'criterion': 'gini', 'max_depth': 16}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:27,824]\u001b[0m Trial 6 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'max_depth': 19}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:33,780]\u001b[0m Trial 7 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 550, 'criterion': 'gini', 'max_depth': 16}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:38,239]\u001b[0m Trial 8 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 500, 'criterion': 'gini', 'max_depth': 13}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:44,397]\u001b[0m Trial 9 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 650, 'criterion': 'entropy', 'max_depth': 14}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:09:54,761]\u001b[0m Trial 10 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 1000, 'criterion': 'entropy', 'max_depth': 6}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:06,494]\u001b[0m Trial 11 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 1000, 'criterion': 'entropy', 'max_depth': 6}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:15,143]\u001b[0m Trial 12 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 800, 'criterion': 'entropy', 'max_depth': 7}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:22,385]\u001b[0m Trial 13 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 850, 'criterion': 'entropy', 'max_depth': 7}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:29,867]\u001b[0m Trial 14 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 850, 'criterion': 'entropy', 'max_depth': 3}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:33,233]\u001b[0m Trial 15 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 350, 'criterion': 'entropy', 'max_depth': 10}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:38,902]\u001b[0m Trial 16 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 700, 'criterion': 'entropy', 'max_depth': 9}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:46,762]\u001b[0m Trial 17 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 850, 'criterion': 'entropy', 'max_depth': 12}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:10:53,196]\u001b[0m Trial 18 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 750, 'criterion': 'entropy', 'max_depth': 12}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:11:00,081]\u001b[0m Trial 19 finished with value: 0.8888888888888888 and parameters: {'n_estimators': 900, 'criterion': 'entropy', 'max_depth': 8}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prueba con TPE\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=0)  # Asegurar los reproducibilidad de los resultados\n",
    "study_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_TPE.optimize(objectiveRF_TPE, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f570bb4",
   "metadata": {},
   "source": [
    "**Prueba sobre otro conjunto de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "204002dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data2 = load_breast_cancer()\n",
    "\n",
    "X2, y2 = data2['data'], data2['target']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24410ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba aleatoria 0 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 1 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 2 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 3 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 4 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 5 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 6 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 7 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 8 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 9 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 10 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 11 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 12 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 13 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 14 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 15 ------ Accuracy: 98.25%\n",
      "Prueba aleatoria 16 ------ Accuracy: 95.61%\n",
      "Prueba aleatoria 17 ------ Accuracy: 96.49%\n",
      "Prueba aleatoria 18 ------ Accuracy: 97.37%\n",
      "Prueba aleatoria 19 ------ Accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Definir y entrenar el modelo\n",
    "    modelRF_random = RandomForestClassifier(criterion=random_params[\"criterion\"][i], max_depth=random_params[\"max_depth\"][i], \n",
    "                                            n_estimators=random_params[\"n_estimators\"][i], random_state=0)  \n",
    "    modelRF_random.fit(X_train2, y_train2)\n",
    "\n",
    "    # Predicción en partición de test\n",
    "    y_pred_RF_random2 = modelRF_random.predict(X_test2)\n",
    "\n",
    "    # Precisión en partición de test\n",
    "    accuracy = accuracy_score(y_test2, y_pred_RF_random2)\n",
    "    print(\"Prueba aleatoria {} ------ Accuracy: {:0.2f}%\".format(i, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6593a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveRF_TPE2(trial):\n",
    "    '''\n",
    "    Define la función a optimizar por medio de un sampler de tipo TPE.\n",
    "    En este caso se trata de maximizar el accuracy\n",
    "    '''\n",
    "    n_estimators =  trial.suggest_int(\"n_estimators\", 50, 1000, 50) # optuna incluye en el rango el máximo y el mínimo\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n",
    "    \n",
    "    modelRF_optuna = RandomForestClassifier(criterion = criterion, max_depth = max_depth, n_estimators = n_estimators, \n",
    "                                            random_state=0)\n",
    "    \n",
    "    modelRF_optuna.fit(X_train2, y_train2)\n",
    "\n",
    "    y_pred_RF_optuna2 = modelRF_optuna.predict(X_test2)\n",
    "    accuracy = accuracy_score(y_test2, y_pred_RF_optuna2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c9f958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-02 20:13:07,125]\u001b[0m A new study created in memory with name: no-name-cbbb2365-136f-4508-9c4f-853781df1179\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:12,394]\u001b[0m Trial 0 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 550, 'criterion': 'gini', 'max_depth': 11}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:16,748]\u001b[0m Trial 1 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 450, 'criterion': 'gini', 'max_depth': 18}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:26,475]\u001b[0m Trial 2 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 1000, 'criterion': 'entropy', 'max_depth': 11}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:31,283]\u001b[0m Trial 3 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 600, 'criterion': 'gini', 'max_depth': 2}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:31,612]\u001b[0m Trial 4 finished with value: 0.956140350877193 and parameters: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 18}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:41,085]\u001b[0m Trial 5 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 1000, 'criterion': 'gini', 'max_depth': 16}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:42,667]\u001b[0m Trial 6 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'max_depth': 19}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:47,162]\u001b[0m Trial 7 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 550, 'criterion': 'gini', 'max_depth': 16}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:52,326]\u001b[0m Trial 8 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 500, 'criterion': 'gini', 'max_depth': 13}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:57,863]\u001b[0m Trial 9 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 650, 'criterion': 'entropy', 'max_depth': 14}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:58,496]\u001b[0m Trial 10 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 300, 'criterion': 'entropy', 'max_depth': 6}. Best is trial 0 with value: 0.9736842105263158.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:13:59,137]\u001b[0m Trial 11 finished with value: 0.9824561403508771 and parameters: {'n_estimators': 350, 'criterion': 'gini', 'max_depth': 8}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:00,865]\u001b[0m Trial 12 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 750, 'criterion': 'gini', 'max_depth': 7}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:01,506]\u001b[0m Trial 13 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 250, 'criterion': 'gini', 'max_depth': 8}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:02,402]\u001b[0m Trial 14 finished with value: 0.956140350877193 and parameters: {'n_estimators': 350, 'criterion': 'entropy', 'max_depth': 3}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:04,182]\u001b[0m Trial 15 finished with value: 0.956140350877193 and parameters: {'n_estimators': 800, 'criterion': 'gini', 'max_depth': 10}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:04,884]\u001b[0m Trial 16 finished with value: 0.956140350877193 and parameters: {'n_estimators': 400, 'criterion': 'gini', 'max_depth': 5}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:06,665]\u001b[0m Trial 17 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 750, 'criterion': 'gini', 'max_depth': 10}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:07,354]\u001b[0m Trial 18 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 250, 'criterion': 'entropy', 'max_depth': 12}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n",
      "\u001b[32m[I 2022-07-02 20:14:07,743]\u001b[0m Trial 19 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'max_depth': 9}. Best is trial 11 with value: 0.9824561403508771.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prueba con TPE\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=0)  # Asegurar los reproducibilidad de los resultados\n",
    "study_TPE = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_TPE.optimize(objectiveRF_TPE2, n_trials=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
