{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aec554",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Un modelo de autoencoder se descompone a su vez en dos modelos de redes neuronales. La primera, el encoder, tiene el objetivo de comprimir la información de los datos; la segunda, el decoder, trata de reconstruir la información original a partir de los datos comprimidos. \n",
    "\n",
    "La motivación para el estudio de un encoder en este problema es:\n",
    "1. Una vez entrenado el autoencoder completo, podemos separar las dos redes neuronales subyacentes y utilizar la parte encoder (con alguna modificación) para probar su rendimiento como modelo de red de clasificación.\n",
    "2. Para los datos de test proporcionados por la competición de Kaggle (no se dispone de la clasificación verdadera), el encoder puede ayudar a \"recrear\" sus etiquetas.\n",
    "\n",
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1832fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuras de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models\n",
    "\n",
    "# Cargar los datos\n",
    "from data_and_submissions import *\n",
    "\n",
    "# Métodos para los entrenamientos con CV\n",
    "from train_cv_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb1ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de train: (68, 410)\n",
      "Tamaño del dataset de test: (18, 410)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, test_kaggle = load_data()\n",
    "print(\"Tamaño del dataset de train:\", X_train.shape)\n",
    "print(\"Tamaño del dataset de test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1cda16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tot = pd.concat((X_train, X_test), axis=0)\n",
    "X_train_tot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e95fe1",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Para crear el autoencoder, se utilizarán redes simétricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6e6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 0.1411 - val_loss: 0.1272\n",
      "Epoch 2/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.1208 - val_loss: 0.1164\n",
      "Epoch 3/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1154 - val_loss: 0.1174\n",
      "Epoch 4/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1136 - val_loss: 0.1120\n",
      "Epoch 5/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1108 - val_loss: 0.1107\n",
      "Epoch 6/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 7/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1074 - val_loss: 0.1093\n",
      "Epoch 8/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1063 - val_loss: 0.1052\n",
      "Epoch 9/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1047 - val_loss: 0.1057\n",
      "Epoch 10/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1043 - val_loss: 0.1069\n",
      "Epoch 11/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1040 - val_loss: 0.1049\n",
      "Epoch 12/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1037 - val_loss: 0.1033\n",
      "Epoch 13/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1027 - val_loss: 0.1026\n",
      "Epoch 14/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1021 - val_loss: 0.1029\n",
      "Epoch 15/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1018 - val_loss: 0.1017\n",
      "Epoch 16/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1015 - val_loss: 0.1018\n",
      "Epoch 17/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1013 - val_loss: 0.1014\n",
      "Epoch 18/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1011 - val_loss: 0.1005\n",
      "Epoch 19/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1009 - val_loss: 0.1004\n",
      "Epoch 20/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1007 - val_loss: 0.1009\n",
      "Epoch 21/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1006 - val_loss: 0.1002\n",
      "Epoch 22/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1004 - val_loss: 0.1027\n",
      "Epoch 23/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.1003 - val_loss: 0.1010\n",
      "Epoch 24/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1001 - val_loss: 0.1012\n",
      "Epoch 25/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 26/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0998 - val_loss: 0.1001\n",
      "Epoch 27/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0997 - val_loss: 0.1002\n",
      "Epoch 28/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0996 - val_loss: 0.0995\n",
      "Epoch 29/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0995 - val_loss: 0.0999\n",
      "Epoch 30/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0993 - val_loss: 0.1000\n",
      "Epoch 31/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0988 - val_loss: 0.0983\n",
      "Epoch 32/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0979 - val_loss: 0.0973\n",
      "Epoch 33/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0977 - val_loss: 0.0991\n",
      "Epoch 34/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0975 - val_loss: 0.0969\n",
      "Epoch 35/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0974 - val_loss: 0.0975\n",
      "Epoch 36/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0972 - val_loss: 0.0991\n",
      "Epoch 37/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0971 - val_loss: 0.0979\n",
      "Epoch 38/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0970 - val_loss: 0.0972\n",
      "Epoch 39/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 40/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0968 - val_loss: 0.0963\n",
      "Epoch 41/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0967 - val_loss: 0.0977\n",
      "Epoch 42/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0966 - val_loss: 0.0972\n",
      "Epoch 43/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 44/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0965 - val_loss: 0.0955\n",
      "Epoch 45/100\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 46/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0964 - val_loss: 0.0970\n",
      "Epoch 47/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 48/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0963 - val_loss: 0.0960\n",
      "Epoch 49/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0963 - val_loss: 0.0963\n",
      "Epoch 50/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0962 - val_loss: 0.0974\n",
      "Epoch 51/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0962 - val_loss: 0.0955\n",
      "Epoch 52/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0961 - val_loss: 0.0965\n",
      "Epoch 53/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0961 - val_loss: 0.0953\n",
      "Epoch 54/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0960 - val_loss: 0.0963\n",
      "Epoch 55/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0960 - val_loss: 0.0959\n",
      "Epoch 56/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0959 - val_loss: 0.0966\n",
      "Epoch 57/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0959 - val_loss: 0.0955\n",
      "Epoch 58/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0958 - val_loss: 0.0957\n",
      "Epoch 59/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0958 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0958 - val_loss: 0.0961\n",
      "Epoch 61/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0957 - val_loss: 0.0960\n",
      "Epoch 62/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0957 - val_loss: 0.0966\n",
      "Epoch 63/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0954 - val_loss: 0.0941\n",
      "Epoch 64/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0945 - val_loss: 0.0944\n",
      "Epoch 65/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0943 - val_loss: 0.0933\n",
      "Epoch 66/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0942 - val_loss: 0.0954\n",
      "Epoch 67/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0941 - val_loss: 0.0936\n",
      "Epoch 68/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0941 - val_loss: 0.0937\n",
      "Epoch 69/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0940 - val_loss: 0.0945\n",
      "Epoch 70/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0940 - val_loss: 0.0938\n",
      "Epoch 71/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0933\n",
      "Epoch 72/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0951\n",
      "Epoch 73/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0944\n",
      "Epoch 74/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0939 - val_loss: 0.0944\n",
      "Epoch 75/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0938\n",
      "Epoch 76/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0935\n",
      "Epoch 77/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0938 - val_loss: 0.0939\n",
      "Epoch 78/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0937 - val_loss: 0.0940\n",
      "Epoch 79/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0937 - val_loss: 0.0946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0937 - val_loss: 0.0939\n",
      "Epoch 81/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0937 - val_loss: 0.0938\n",
      "Epoch 82/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0937 - val_loss: 0.0942\n",
      "Epoch 83/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0936 - val_loss: 0.0938\n",
      "Epoch 84/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0936 - val_loss: 0.0944\n",
      "Epoch 85/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0936 - val_loss: 0.0940\n",
      "Epoch 86/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0936 - val_loss: 0.0941\n",
      "Epoch 87/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0935 - val_loss: 0.0933\n",
      "Epoch 88/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0935 - val_loss: 0.0938\n",
      "Epoch 89/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0935 - val_loss: 0.0934\n",
      "Epoch 90/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0935 - val_loss: 0.0936\n",
      "Epoch 91/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0935 - val_loss: 0.0936\n",
      "Epoch 92/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0935 - val_loss: 0.0928\n",
      "Epoch 93/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0934 - val_loss: 0.0948\n",
      "Epoch 94/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0934 - val_loss: 0.0946\n",
      "Epoch 95/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 96/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 97/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0946\n",
      "Epoch 98/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0940\n",
      "Epoch 99/100\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 0.0934 - val_loss: 0.0937\n",
      "Epoch 100/100\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 0.0933 - val_loss: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf35ec4f70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded = layers.Dense(200, activation=\"relu\")(input_layer)\n",
    "encoded = layers.Dense(100, activation=\"relu\")(encoded)\n",
    "encoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "# Capas red decoder\n",
    "decoded = layers.Dense(50, activation=\"relu\")(encoded)\n",
    "decoded = layers.Dense(100, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(200, activation=\"relu\")(decoded)\n",
    "decoded = layers.Dense(410, activation=\"linear\")(decoded)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Model(input_layer, encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder.fit(test_kaggle, test_kaggle, epochs=100, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c774fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946dc7",
   "metadata": {},
   "source": [
    "Evaluación del autoencoder, se utilizarán métricas como:\n",
    "\n",
    "* MSE = $\\frac{1}{n} \\sum_{i=1}^{n} (Y_{true}^{i} - Y_{pred}^{i})^{2}$\n",
    "* MAPE = $\\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_{true}^{i} - Y_{pred}^{i}}{Y_{true}^{i}} \\right|$\n",
    "\n",
    "donde $Y_{true}$ es el valor real, $Y_{pred}$ el valor de la predicción y $n$ el número de predicciones.\n",
    "\n",
    "La anterior fórmula realiza el cálculo para dos \"listas de valores\", por ejemplo, podemos calcular el error MAPE por muestra o por feature. Para el valor del error de la predicción total, se debe promediar el error obtenido para todas las filas/columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbc232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# Definición de las métricas\n",
    "mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a20e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(315.31876, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.08369573, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train_tot (etiquetados)\n",
    "X_train_tot_pred = autoencoder.predict(X_train_tot)\n",
    "\n",
    "print(\"MAPE:\", mape(X_train_tot, X_train_tot_pred))\n",
    "print(\"MSE:\", mse(X_train_tot, X_train_tot_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1e672",
   "metadata": {},
   "source": [
    "El error es bastante elevado. Vamos a comprobar cómo se distribuyen los valores de error MAPE en cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e45bf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp = np.abs((X_train_tot - X_train_tot_pred) / X_train_tot)\n",
    "mape_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532afe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mape_tmp.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a96a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08292682926829269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mape_tmp.mean(axis=0) < 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dace6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd17d6",
   "metadata": {},
   "source": [
    "La anterior expresión indica que únicamente en torno al 8% de las variables en X_train, tienen un error de MAPE que es inferior al 100%. \n",
    "\n",
    "Podíamos pensar que la primera barra en el anterior histograma esconde una gran concentración de valores con errores inferiores al 100% y que el valor final queda desviado por aquellos con valores muy superiores, sin embargo, esta última comprobación lo desmiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(426.45328, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.09347953, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "test_kaggle_pred = autoencoder.predict(test_kaggle)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))\n",
    "print(\"MSE:\", mse(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90c952",
   "metadata": {},
   "source": [
    "Sobre el conjunto de entrenamiento ``test_kaggle``, vamos a seleccionar 100 veces 86 muestras y a promediar los valores de desviación típica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9d3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 415.2279357910156 +- 354.88494873046875\n"
     ]
    }
   ],
   "source": [
    "samples_num = 100\n",
    "mape_samples = []\n",
    "\n",
    "for _ in range(samples_num):\n",
    "    \n",
    "    args = np.random.choice(a=np.arange(0, test_kaggle.shape[0]), size=86, replace=False)\n",
    "\n",
    "    test_kaggle_reduc = test_kaggle.iloc[args, :]\n",
    "\n",
    "    y_pred_kaggle_reduc = autoencoder.predict(test_kaggle_reduc)\n",
    "    tmp_mape = mape(test_kaggle_reduc, y_pred_kaggle_reduc)\n",
    "    mape_samples.append(tmp_mape)\n",
    "    \n",
    "print(f'MAPE: {np.mean(mape_samples)} +- {np.std(mape_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692b393",
   "metadata": {},
   "source": [
    "El valor del MAPE en ``test_kaggle`` es también muy elevado, con grandes desviaciones entre muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c74a80",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Pasamos al problema de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4813a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(410,))\n",
    "encoder = encoder_input\n",
    "for layer in autoencoder.layers[1:4]:\n",
    "    encoder = layer(encoder)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               82200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,350\n",
      "Trainable params: 107,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa99fd",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado la red autoencoder, el componente encoder de la misma ya contará con unos pesos entrenados con el objetivo de comprimir los datos de entrada. Por tanto, podemos considerar de manera independiente esta red encoder y volver a entrenarla como una red para clasificación, con la ventaja de que se parte de un modelo inicializado no con unos pesos aleatorios, sino unos pesos optimizados para un problema similar.\n",
    "\n",
    "Para hacer esto es necesario añadir previamente una capa final de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec1b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_encoder = encoder # loading the previously saved model.\n",
    "\n",
    "new_encoder = models.Sequential()\n",
    "new_encoder.add(prev_encoder)\n",
    "new_encoder.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9630a1",
   "metadata": {},
   "source": [
    "Incluso es posible congelar los pesos de todas las capas del encoder original, ya entrenados \"para un problema similar\", y modificar solamente los pesos de la capa final de clasificación, utilizando los datos de train (etiquetados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a1a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.6913 - acc: 0.5294 - val_loss: 0.7122 - val_acc: 0.4118\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6888 - acc: 0.5098 - val_loss: 0.7108 - val_acc: 0.3529\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6871 - acc: 0.5098 - val_loss: 0.7100 - val_acc: 0.4118\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6862 - acc: 0.5098 - val_loss: 0.7090 - val_acc: 0.4118\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6855 - acc: 0.5098 - val_loss: 0.7083 - val_acc: 0.4706\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6844 - acc: 0.4902 - val_loss: 0.7078 - val_acc: 0.4706\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6866 - acc: 0.4902 - val_loss: 0.7078 - val_acc: 0.4706\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6835 - acc: 0.5098 - val_loss: 0.7075 - val_acc: 0.4706\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6836 - acc: 0.5294 - val_loss: 0.7074 - val_acc: 0.4706\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6829 - acc: 0.5098 - val_loss: 0.7072 - val_acc: 0.4706\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6825 - acc: 0.5294 - val_loss: 0.7070 - val_acc: 0.4706\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6825 - acc: 0.5294 - val_loss: 0.7065 - val_acc: 0.4706\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6825 - acc: 0.5294 - val_loss: 0.7065 - val_acc: 0.4706\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6813 - acc: 0.5294 - val_loss: 0.7062 - val_acc: 0.4706\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6811 - acc: 0.5294 - val_loss: 0.7061 - val_acc: 0.4706\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6806 - acc: 0.5294 - val_loss: 0.7060 - val_acc: 0.5294\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6816 - acc: 0.5294 - val_loss: 0.7056 - val_acc: 0.5294\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6804 - acc: 0.5294 - val_loss: 0.7053 - val_acc: 0.5294\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6795 - acc: 0.5098 - val_loss: 0.7051 - val_acc: 0.5294\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6795 - acc: 0.5098 - val_loss: 0.7048 - val_acc: 0.5294\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6792 - acc: 0.5098 - val_loss: 0.7046 - val_acc: 0.5294\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6785 - acc: 0.5098 - val_loss: 0.7044 - val_acc: 0.5294\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6782 - acc: 0.5098 - val_loss: 0.7043 - val_acc: 0.5294\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6782 - acc: 0.5098 - val_loss: 0.7040 - val_acc: 0.5294\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6775 - acc: 0.5098 - val_loss: 0.7039 - val_acc: 0.5294\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6771 - acc: 0.5098 - val_loss: 0.7038 - val_acc: 0.5294\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6774 - acc: 0.5098 - val_loss: 0.7036 - val_acc: 0.5294\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6767 - acc: 0.5098 - val_loss: 0.7034 - val_acc: 0.5294\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6761 - acc: 0.5098 - val_loss: 0.7033 - val_acc: 0.5294\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6760 - acc: 0.5098 - val_loss: 0.7031 - val_acc: 0.5294\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6760 - acc: 0.5098 - val_loss: 0.7031 - val_acc: 0.5294\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6753 - acc: 0.5098 - val_loss: 0.7030 - val_acc: 0.5294\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6757 - acc: 0.5098 - val_loss: 0.7030 - val_acc: 0.5294\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6747 - acc: 0.5098 - val_loss: 0.7030 - val_acc: 0.5294\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6744 - acc: 0.5294 - val_loss: 0.7028 - val_acc: 0.5294\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6739 - acc: 0.5098 - val_loss: 0.7027 - val_acc: 0.5294\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6736 - acc: 0.5098 - val_loss: 0.7025 - val_acc: 0.5294\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6732 - acc: 0.5098 - val_loss: 0.7023 - val_acc: 0.5294\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6728 - acc: 0.5098 - val_loss: 0.7022 - val_acc: 0.5294\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6727 - acc: 0.5294 - val_loss: 0.7020 - val_acc: 0.5294\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6720 - acc: 0.5098 - val_loss: 0.7019 - val_acc: 0.5294\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6716 - acc: 0.5098 - val_loss: 0.7018 - val_acc: 0.5294\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6715 - acc: 0.5098 - val_loss: 0.7017 - val_acc: 0.5294\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6711 - acc: 0.5098 - val_loss: 0.7015 - val_acc: 0.5294\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6717 - acc: 0.5098 - val_loss: 0.7014 - val_acc: 0.5294\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6704 - acc: 0.5294 - val_loss: 0.7012 - val_acc: 0.5294\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6700 - acc: 0.5686 - val_loss: 0.7012 - val_acc: 0.5294\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6700 - acc: 0.5490 - val_loss: 0.7010 - val_acc: 0.5294\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6692 - acc: 0.5686 - val_loss: 0.7009 - val_acc: 0.5294\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6690 - acc: 0.5686 - val_loss: 0.7009 - val_acc: 0.5294\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6693 - acc: 0.5294 - val_loss: 0.7007 - val_acc: 0.5294\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6686 - acc: 0.5686 - val_loss: 0.7007 - val_acc: 0.5294\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6684 - acc: 0.5686 - val_loss: 0.7005 - val_acc: 0.5294\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6677 - acc: 0.5686 - val_loss: 0.7004 - val_acc: 0.5294\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6679 - acc: 0.5686 - val_loss: 0.7003 - val_acc: 0.5294\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6672 - acc: 0.5686 - val_loss: 0.7002 - val_acc: 0.5294\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6668 - acc: 0.5882 - val_loss: 0.7001 - val_acc: 0.5294\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6667 - acc: 0.5882 - val_loss: 0.7000 - val_acc: 0.5294\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6661 - acc: 0.5882 - val_loss: 0.6999 - val_acc: 0.5294\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6667 - acc: 0.5882 - val_loss: 0.6998 - val_acc: 0.5294\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6655 - acc: 0.5882 - val_loss: 0.6997 - val_acc: 0.5294\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6655 - acc: 0.5882 - val_loss: 0.6996 - val_acc: 0.5294\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6655 - acc: 0.5882 - val_loss: 0.6995 - val_acc: 0.5294\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6650 - acc: 0.5686 - val_loss: 0.6994 - val_acc: 0.5294\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6645 - acc: 0.5882 - val_loss: 0.6994 - val_acc: 0.5294\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6641 - acc: 0.5882 - val_loss: 0.6993 - val_acc: 0.5294\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6637 - acc: 0.5882 - val_loss: 0.6992 - val_acc: 0.5294\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6634 - acc: 0.5882 - val_loss: 0.6991 - val_acc: 0.5294\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6630 - acc: 0.5882 - val_loss: 0.6990 - val_acc: 0.5294\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6626 - acc: 0.5882 - val_loss: 0.6989 - val_acc: 0.5294\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6624 - acc: 0.5882 - val_loss: 0.6988 - val_acc: 0.5294\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6618 - acc: 0.5882 - val_loss: 0.6986 - val_acc: 0.5294\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6615 - acc: 0.5882 - val_loss: 0.6985 - val_acc: 0.5294\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6610 - acc: 0.5882 - val_loss: 0.6985 - val_acc: 0.5294\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6606 - acc: 0.5882 - val_loss: 0.6983 - val_acc: 0.5294\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6601 - acc: 0.5882 - val_loss: 0.6982 - val_acc: 0.5294\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6598 - acc: 0.5882 - val_loss: 0.6982 - val_acc: 0.5294\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6592 - acc: 0.6078 - val_loss: 0.6980 - val_acc: 0.5294\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6588 - acc: 0.6078 - val_loss: 0.6980 - val_acc: 0.5294\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6593 - acc: 0.6078 - val_loss: 0.6979 - val_acc: 0.5294\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6580 - acc: 0.6078 - val_loss: 0.6979 - val_acc: 0.5294\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6576 - acc: 0.6078 - val_loss: 0.6978 - val_acc: 0.5294\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6581 - acc: 0.6078 - val_loss: 0.6977 - val_acc: 0.5294\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6571 - acc: 0.6078 - val_loss: 0.6976 - val_acc: 0.5294\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6569 - acc: 0.6275 - val_loss: 0.6974 - val_acc: 0.5294\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6562 - acc: 0.6078 - val_loss: 0.6974 - val_acc: 0.5294\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6569 - acc: 0.6275 - val_loss: 0.6972 - val_acc: 0.5294\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6556 - acc: 0.6078 - val_loss: 0.6972 - val_acc: 0.5294\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6553 - acc: 0.6078 - val_loss: 0.6971 - val_acc: 0.5294\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6550 - acc: 0.6078 - val_loss: 0.6970 - val_acc: 0.5294\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6546 - acc: 0.6078 - val_loss: 0.6969 - val_acc: 0.5294\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6544 - acc: 0.6078 - val_loss: 0.6968 - val_acc: 0.5294\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6551 - acc: 0.6078 - val_loss: 0.6967 - val_acc: 0.5294\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6540 - acc: 0.6078 - val_loss: 0.6967 - val_acc: 0.5294\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6541 - acc: 0.6078 - val_loss: 0.6966 - val_acc: 0.5882\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6530 - acc: 0.6275 - val_loss: 0.6965 - val_acc: 0.5882\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6528 - acc: 0.6275 - val_loss: 0.6964 - val_acc: 0.5882\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6537 - acc: 0.6275 - val_loss: 0.6963 - val_acc: 0.5882\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6523 - acc: 0.6275 - val_loss: 0.6962 - val_acc: 0.5294\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6522 - acc: 0.6275 - val_loss: 0.6961 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6860 - acc: 0.3889\n",
      "Accuracy: 38.89%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1913fc",
   "metadata": {},
   "source": [
    "Tras unas pocas iteraciones, descongelamos todas las capas y hacemos unas pocas épocas más entrenando y actualizando los pesos para el modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.6432 - acc: 0.6275 - val_loss: 0.6638 - val_acc: 0.7059\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5359 - acc: 0.8039 - val_loss: 0.6515 - val_acc: 0.7059\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4351 - acc: 0.9020 - val_loss: 0.6444 - val_acc: 0.6471\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3410 - acc: 0.9804 - val_loss: 0.6431 - val_acc: 0.6471\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2575 - acc: 1.0000 - val_loss: 0.6678 - val_acc: 0.6471\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1928 - acc: 1.0000 - val_loss: 0.6810 - val_acc: 0.5882\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1661 - acc: 1.0000 - val_loss: 0.6849 - val_acc: 0.5882\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1214 - acc: 1.0000 - val_loss: 0.7137 - val_acc: 0.6471\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0790 - acc: 1.0000 - val_loss: 0.7098 - val_acc: 0.5882\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0599 - acc: 1.0000 - val_loss: 0.7385 - val_acc: 0.5882\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0457 - acc: 1.0000 - val_loss: 0.7716 - val_acc: 0.5882\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.8303 - val_acc: 0.5882\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.5882\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.8815 - val_acc: 0.5882\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.8792 - val_acc: 0.5882\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7296 - acc: 0.6667\n",
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Descongelar los pesos de todas las capas a excepción de la última\n",
    "for layer in new_encoder.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "new_encoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "new_encoder.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = new_encoder.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d4c5ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnew_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\h5py\\_hl\\group.py:149\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    146\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    147\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 149\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    150\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\h5py\\_hl\\dataset.py:142\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 142\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    145\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:87\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "new_encoder.save(\"encoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69276e22",
   "metadata": {},
   "source": [
    "El objetivo es poder comparar con la misma configuración de red que la encoder sin utilizar los pesos pre-entrenados de esta y observar si hay mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c906d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "2/2 [==============================] - 1s 137ms/step - loss: 0.6693 - acc: 0.5098 - val_loss: 0.6605 - val_acc: 0.5882\n",
      "Epoch 2/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4618 - acc: 0.8824 - val_loss: 0.6751 - val_acc: 0.7059\n",
      "Epoch 3/115\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3048 - acc: 0.9608 - val_loss: 0.6953 - val_acc: 0.7059\n",
      "Epoch 4/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1893 - acc: 0.9804 - val_loss: 0.7126 - val_acc: 0.7647\n",
      "Epoch 5/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1170 - acc: 1.0000 - val_loss: 0.7912 - val_acc: 0.7647\n",
      "Epoch 6/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0785 - acc: 1.0000 - val_loss: 0.7260 - val_acc: 0.6471\n",
      "Epoch 7/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0511 - acc: 1.0000 - val_loss: 0.7264 - val_acc: 0.5294\n",
      "Epoch 8/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0435 - acc: 1.0000 - val_loss: 0.8056 - val_acc: 0.7647\n",
      "Epoch 9/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.6471\n",
      "Epoch 10/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.7798 - val_acc: 0.7059\n",
      "Epoch 11/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.8141 - val_acc: 0.7059\n",
      "Epoch 12/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 0.7647\n",
      "Epoch 13/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.8361 - val_acc: 0.6471\n",
      "Epoch 14/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.8772 - val_acc: 0.7059\n",
      "Epoch 15/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.8699 - val_acc: 0.7059\n",
      "Epoch 16/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.9101 - val_acc: 0.7059\n",
      "Epoch 17/115\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.9444 - val_acc: 0.7647\n",
      "Epoch 18/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9446 - val_acc: 0.7059\n",
      "Epoch 19/115\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9507 - val_acc: 0.7059\n",
      "Epoch 20/115\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9587 - val_acc: 0.7059\n",
      "Epoch 21/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.9892 - val_acc: 0.7059\n",
      "Epoch 22/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9875 - val_acc: 0.7059\n",
      "Epoch 23/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9866 - val_acc: 0.7059\n",
      "Epoch 24/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9.0909e-04 - acc: 1.0000 - val_loss: 1.0305 - val_acc: 0.7647\n",
      "Epoch 25/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7.7527e-04 - acc: 1.0000 - val_loss: 1.0220 - val_acc: 0.7059\n",
      "Epoch 26/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.6360e-04 - acc: 1.0000 - val_loss: 1.0279 - val_acc: 0.7059\n",
      "Epoch 27/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.7166e-04 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.7647\n",
      "Epoch 28/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.9303e-04 - acc: 1.0000 - val_loss: 1.0816 - val_acc: 0.7059\n",
      "Epoch 29/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.2080e-04 - acc: 1.0000 - val_loss: 1.0976 - val_acc: 0.7647\n",
      "Epoch 30/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.6369e-04 - acc: 1.0000 - val_loss: 1.1087 - val_acc: 0.7647\n",
      "Epoch 31/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.1627e-04 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.7059\n",
      "Epoch 32/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.7655e-04 - acc: 1.0000 - val_loss: 1.1090 - val_acc: 0.7647\n",
      "Epoch 33/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.3353e-04 - acc: 1.0000 - val_loss: 1.1250 - val_acc: 0.7647\n",
      "Epoch 34/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.0175e-04 - acc: 1.0000 - val_loss: 1.1378 - val_acc: 0.7647\n",
      "Epoch 35/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.7746e-04 - acc: 1.0000 - val_loss: 1.1766 - val_acc: 0.7647\n",
      "Epoch 36/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.5307e-04 - acc: 1.0000 - val_loss: 1.1649 - val_acc: 0.8235\n",
      "Epoch 37/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.3158e-04 - acc: 1.0000 - val_loss: 1.1762 - val_acc: 0.8235\n",
      "Epoch 38/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1439e-04 - acc: 1.0000 - val_loss: 1.1882 - val_acc: 0.8235\n",
      "Epoch 39/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 9.9930e-05 - acc: 1.0000 - val_loss: 1.2040 - val_acc: 0.8235\n",
      "Epoch 40/115\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 8.7287e-05 - acc: 1.0000 - val_loss: 1.2140 - val_acc: 0.8235\n",
      "Epoch 41/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 7.6753e-05 - acc: 1.0000 - val_loss: 1.2289 - val_acc: 0.8235\n",
      "Epoch 42/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.6743e-05 - acc: 1.0000 - val_loss: 1.2334 - val_acc: 0.8235\n",
      "Epoch 43/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.8153e-05 - acc: 1.0000 - val_loss: 1.2421 - val_acc: 0.8235\n",
      "Epoch 44/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.0445e-05 - acc: 1.0000 - val_loss: 1.2615 - val_acc: 0.8235\n",
      "Epoch 45/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.4469e-05 - acc: 1.0000 - val_loss: 1.2803 - val_acc: 0.8235\n",
      "Epoch 46/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3.8873e-05 - acc: 1.0000 - val_loss: 1.2871 - val_acc: 0.8235\n",
      "Epoch 47/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.3714e-05 - acc: 1.0000 - val_loss: 1.2888 - val_acc: 0.8235\n",
      "Epoch 48/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.9351e-05 - acc: 1.0000 - val_loss: 1.3006 - val_acc: 0.8235\n",
      "Epoch 49/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.5925e-05 - acc: 1.0000 - val_loss: 1.3219 - val_acc: 0.8235\n",
      "Epoch 50/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.2679e-05 - acc: 1.0000 - val_loss: 1.3078 - val_acc: 0.8235\n",
      "Epoch 51/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.9890e-05 - acc: 1.0000 - val_loss: 1.3200 - val_acc: 0.8235\n",
      "Epoch 52/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.7367e-05 - acc: 1.0000 - val_loss: 1.3332 - val_acc: 0.8235\n",
      "Epoch 53/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.5276e-05 - acc: 1.0000 - val_loss: 1.3545 - val_acc: 0.8235\n",
      "Epoch 54/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.3623e-05 - acc: 1.0000 - val_loss: 1.3302 - val_acc: 0.8235\n",
      "Epoch 55/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1923e-05 - acc: 1.0000 - val_loss: 1.3651 - val_acc: 0.8235\n",
      "Epoch 56/115\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.0437e-05 - acc: 1.0000 - val_loss: 1.3611 - val_acc: 0.8235\n",
      "Epoch 57/115\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 9.3209e-06 - acc: 1.0000 - val_loss: 1.3616 - val_acc: 0.8235\n",
      "Epoch 58/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 8.1329e-06 - acc: 1.0000 - val_loss: 1.4058 - val_acc: 0.8235\n",
      "Epoch 59/115\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 7.1535e-06 - acc: 1.0000 - val_loss: 1.3867 - val_acc: 0.8235\n",
      "Epoch 60/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.3501e-06 - acc: 1.0000 - val_loss: 1.4234 - val_acc: 0.8235\n",
      "Epoch 61/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.6073e-06 - acc: 1.0000 - val_loss: 1.4211 - val_acc: 0.8235\n",
      "Epoch 62/115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 4.9681e-06 - acc: 1.0000 - val_loss: 1.4391 - val_acc: 0.8235\n",
      "Epoch 63/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.4195e-06 - acc: 1.0000 - val_loss: 1.4541 - val_acc: 0.8235\n",
      "Epoch 64/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.8758e-06 - acc: 1.0000 - val_loss: 1.4368 - val_acc: 0.8235\n",
      "Epoch 65/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.4163e-06 - acc: 1.0000 - val_loss: 1.4705 - val_acc: 0.8235\n",
      "Epoch 66/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.0136e-06 - acc: 1.0000 - val_loss: 1.4726 - val_acc: 0.8235\n",
      "Epoch 67/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.6696e-06 - acc: 1.0000 - val_loss: 1.4833 - val_acc: 0.8235\n",
      "Epoch 68/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.3741e-06 - acc: 1.0000 - val_loss: 1.4731 - val_acc: 0.8235\n",
      "Epoch 69/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.1013e-06 - acc: 1.0000 - val_loss: 1.5017 - val_acc: 0.8235\n",
      "Epoch 70/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.8568e-06 - acc: 1.0000 - val_loss: 1.5054 - val_acc: 0.8235\n",
      "Epoch 71/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.6485e-06 - acc: 1.0000 - val_loss: 1.5266 - val_acc: 0.8235\n",
      "Epoch 72/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.4561e-06 - acc: 1.0000 - val_loss: 1.5191 - val_acc: 0.8235\n",
      "Epoch 73/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.2867e-06 - acc: 1.0000 - val_loss: 1.5250 - val_acc: 0.8235\n",
      "Epoch 74/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1402e-06 - acc: 1.0000 - val_loss: 1.5370 - val_acc: 0.8235\n",
      "Epoch 75/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0109e-06 - acc: 1.0000 - val_loss: 1.5459 - val_acc: 0.8235\n",
      "Epoch 76/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 9.0304e-07 - acc: 1.0000 - val_loss: 1.5627 - val_acc: 0.8235\n",
      "Epoch 77/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.0273e-07 - acc: 1.0000 - val_loss: 1.5694 - val_acc: 0.8235\n",
      "Epoch 78/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.1617e-07 - acc: 1.0000 - val_loss: 1.5684 - val_acc: 0.8235\n",
      "Epoch 79/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.4340e-07 - acc: 1.0000 - val_loss: 1.5763 - val_acc: 0.8235\n",
      "Epoch 80/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.8612e-07 - acc: 1.0000 - val_loss: 1.5682 - val_acc: 0.8235\n",
      "Epoch 81/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.1891e-07 - acc: 1.0000 - val_loss: 1.5983 - val_acc: 0.8235\n",
      "Epoch 82/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.6103e-07 - acc: 1.0000 - val_loss: 1.6143 - val_acc: 0.8235\n",
      "Epoch 83/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.1284e-07 - acc: 1.0000 - val_loss: 1.6132 - val_acc: 0.8235\n",
      "Epoch 84/115\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.6864e-07 - acc: 1.0000 - val_loss: 1.6312 - val_acc: 0.8235\n",
      "Epoch 85/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.3675e-07 - acc: 1.0000 - val_loss: 1.6614 - val_acc: 0.8235\n",
      "Epoch 86/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.0140e-07 - acc: 1.0000 - val_loss: 1.6562 - val_acc: 0.8235\n",
      "Epoch 87/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.7313e-07 - acc: 1.0000 - val_loss: 1.6785 - val_acc: 0.8235\n",
      "Epoch 88/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4393e-07 - acc: 1.0000 - val_loss: 1.6712 - val_acc: 0.8235\n",
      "Epoch 89/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.1880e-07 - acc: 1.0000 - val_loss: 1.6813 - val_acc: 0.8235\n",
      "Epoch 90/115\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.9830e-07 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 0.8235\n",
      "Epoch 91/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.8013e-07 - acc: 1.0000 - val_loss: 1.6994 - val_acc: 0.8235\n",
      "Epoch 92/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.6470e-07 - acc: 1.0000 - val_loss: 1.7166 - val_acc: 0.8235\n",
      "Epoch 93/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.5197e-07 - acc: 1.0000 - val_loss: 1.7270 - val_acc: 0.8235\n",
      "Epoch 94/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.3877e-07 - acc: 1.0000 - val_loss: 1.7034 - val_acc: 0.8235\n",
      "Epoch 95/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2558e-07 - acc: 1.0000 - val_loss: 1.7084 - val_acc: 0.8235\n",
      "Epoch 96/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1337e-07 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 0.8235\n",
      "Epoch 97/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0333e-07 - acc: 1.0000 - val_loss: 1.7283 - val_acc: 0.8235\n",
      "Epoch 98/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 9.4769e-08 - acc: 1.0000 - val_loss: 1.7571 - val_acc: 0.8235\n",
      "Epoch 99/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.6968e-08 - acc: 1.0000 - val_loss: 1.7564 - val_acc: 0.8235\n",
      "Epoch 100/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.0738e-08 - acc: 1.0000 - val_loss: 1.7418 - val_acc: 0.8235\n",
      "Epoch 101/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7.2858e-08 - acc: 1.0000 - val_loss: 1.7559 - val_acc: 0.8235\n",
      "Epoch 102/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.7201e-08 - acc: 1.0000 - val_loss: 1.7734 - val_acc: 0.8235\n",
      "Epoch 103/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.1381e-08 - acc: 1.0000 - val_loss: 1.7806 - val_acc: 0.8235\n",
      "Epoch 104/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.6480e-08 - acc: 1.0000 - val_loss: 1.7856 - val_acc: 0.8235\n",
      "Epoch 105/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.2409e-08 - acc: 1.0000 - val_loss: 1.7968 - val_acc: 0.8235\n",
      "Epoch 106/115\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.8561e-08 - acc: 1.0000 - val_loss: 1.8040 - val_acc: 0.8235\n",
      "Epoch 107/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.4462e-08 - acc: 1.0000 - val_loss: 1.7960 - val_acc: 0.8235\n",
      "Epoch 108/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.1380e-08 - acc: 1.0000 - val_loss: 1.7946 - val_acc: 0.8235\n",
      "Epoch 109/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.8501e-08 - acc: 1.0000 - val_loss: 1.8029 - val_acc: 0.8235\n",
      "Epoch 110/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.5565e-08 - acc: 1.0000 - val_loss: 1.8077 - val_acc: 0.8235\n",
      "Epoch 111/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.3097e-08 - acc: 1.0000 - val_loss: 1.8297 - val_acc: 0.8235\n",
      "Epoch 112/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.0843e-08 - acc: 1.0000 - val_loss: 1.8331 - val_acc: 0.8235\n",
      "Epoch 113/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.9146e-08 - acc: 1.0000 - val_loss: 1.8365 - val_acc: 0.8235\n",
      "Epoch 114/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.7477e-08 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 0.8235\n",
      "Epoch 115/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.5902e-08 - acc: 1.0000 - val_loss: 1.8488 - val_acc: 0.8235\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6085 - acc: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer2 = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded2 = layers.Dense(200, activation=\"relu\")(input_layer2)\n",
    "encoded2 = layers.Dense(100, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(50, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(1, activation=\"sigmoid\")(encoded2)\n",
    "\n",
    "encoder2 = models.Model(input_layer2, encoded2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder2.fit(X_train, y_train, epochs=115, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364e970",
   "metadata": {},
   "source": [
    "**Vamos a probar con una configuración de autoencoder más compleja**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a3da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1404/1404 [==============================] - 14s 9ms/step - loss: 0.1778 - val_loss: 0.1727\n",
      "Epoch 2/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1672 - val_loss: 0.1634\n",
      "Epoch 3/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1604 - val_loss: 0.1585\n",
      "Epoch 4/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1564 - val_loss: 0.1551\n",
      "Epoch 5/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1532 - val_loss: 0.1525\n",
      "Epoch 6/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1508 - val_loss: 0.1500\n",
      "Epoch 7/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1485 - val_loss: 0.1478\n",
      "Epoch 8/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1468 - val_loss: 0.1475\n",
      "Epoch 9/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1458 - val_loss: 0.1473\n",
      "Epoch 10/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1451 - val_loss: 0.1474\n",
      "Epoch 11/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1446 - val_loss: 0.1459\n",
      "Epoch 12/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1440 - val_loss: 0.1456\n",
      "Epoch 13/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1434 - val_loss: 0.1452\n",
      "Epoch 14/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1426 - val_loss: 0.1431\n",
      "Epoch 15/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1415 - val_loss: 0.1436\n",
      "Epoch 16/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1408 - val_loss: 0.1429\n",
      "Epoch 17/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1401 - val_loss: 0.1409\n",
      "Epoch 18/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1393 - val_loss: 0.1409\n",
      "Epoch 19/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1384 - val_loss: 0.1404\n",
      "Epoch 20/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1376 - val_loss: 0.1388\n",
      "Epoch 21/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1369 - val_loss: 0.1409\n",
      "Epoch 22/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1362 - val_loss: 0.1390\n",
      "Epoch 23/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1356 - val_loss: 0.1364\n",
      "Epoch 24/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1350 - val_loss: 0.1361\n",
      "Epoch 25/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1343 - val_loss: 0.1354\n",
      "Epoch 26/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1338 - val_loss: 0.1360\n",
      "Epoch 27/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1333 - val_loss: 0.1369\n",
      "Epoch 28/200\n",
      "1404/1404 [==============================] - 15s 10ms/step - loss: 0.1328 - val_loss: 0.1346\n",
      "Epoch 29/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1323 - val_loss: 0.1337\n",
      "Epoch 30/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1319 - val_loss: 0.1345\n",
      "Epoch 31/200\n",
      "1404/1404 [==============================] - 15s 11ms/step - loss: 0.1317 - val_loss: 0.1329\n",
      "Epoch 32/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1313 - val_loss: 0.1321\n",
      "Epoch 33/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1308 - val_loss: 0.1318\n",
      "Epoch 34/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1307 - val_loss: 0.1312\n",
      "Epoch 35/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1306 - val_loss: 0.1301\n",
      "Epoch 36/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1304 - val_loss: 0.1308\n",
      "Epoch 37/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1300 - val_loss: 0.1307\n",
      "Epoch 38/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1301 - val_loss: 0.1312\n",
      "Epoch 39/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1297 - val_loss: 0.1299\n",
      "Epoch 40/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1290 - val_loss: 0.1308\n",
      "Epoch 41/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1290 - val_loss: 0.1311\n",
      "Epoch 42/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1287 - val_loss: 0.1295\n",
      "Epoch 43/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1282 - val_loss: 0.1286\n",
      "Epoch 44/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1275 - val_loss: 0.1293\n",
      "Epoch 45/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1273 - val_loss: 0.1282\n",
      "Epoch 46/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1267 - val_loss: 0.1288\n",
      "Epoch 47/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1263 - val_loss: 0.1282\n",
      "Epoch 48/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1265 - val_loss: 0.1281\n",
      "Epoch 49/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1269 - val_loss: 0.1277\n",
      "Epoch 50/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1268 - val_loss: 0.1280\n",
      "Epoch 51/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1268 - val_loss: 0.1260\n",
      "Epoch 52/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1261 - val_loss: 0.1278\n",
      "Epoch 53/200\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 0.1257 - val_loss: 0.1263\n",
      "Epoch 54/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1256 - val_loss: 0.1278\n",
      "Epoch 55/200\n",
      "1404/1404 [==============================] - 13s 10ms/step - loss: 0.1263 - val_loss: 0.1289\n",
      "Epoch 56/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1261 - val_loss: 0.1262\n",
      "Epoch 57/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1258 - val_loss: 0.1281\n",
      "Epoch 58/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1262 - val_loss: 0.1269\n",
      "Epoch 59/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1257 - val_loss: 0.1277\n",
      "Epoch 60/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1257 - val_loss: 0.1263\n",
      "Epoch 61/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1259 - val_loss: 0.1262\n",
      "Epoch 62/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1263 - val_loss: 0.1261\n",
      "Epoch 63/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1271 - val_loss: 0.1278\n",
      "Epoch 64/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1265 - val_loss: 0.1273\n",
      "Epoch 65/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1256 - val_loss: 0.1262\n",
      "Epoch 66/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1301\n",
      "Epoch 67/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1262 - val_loss: 0.1270\n",
      "Epoch 68/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1266 - val_loss: 0.1273\n",
      "Epoch 69/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1266 - val_loss: 0.1265\n",
      "Epoch 70/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1273 - val_loss: 0.1287\n",
      "Epoch 71/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1263 - val_loss: 0.1262\n",
      "Epoch 72/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1265 - val_loss: 0.1275\n",
      "Epoch 73/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1265 - val_loss: 0.1296\n",
      "Epoch 74/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1275 - val_loss: 0.1293\n",
      "Epoch 75/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1274 - val_loss: 0.1310\n",
      "Epoch 76/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1271 - val_loss: 0.1287\n",
      "Epoch 77/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1289\n",
      "Epoch 78/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1270 - val_loss: 0.1284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1273 - val_loss: 0.1283\n",
      "Epoch 80/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1272 - val_loss: 0.1279\n",
      "Epoch 81/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1274 - val_loss: 0.1289\n",
      "Epoch 82/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1277 - val_loss: 0.1268\n",
      "Epoch 83/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1278 - val_loss: 0.1292\n",
      "Epoch 84/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1299\n",
      "Epoch 85/200\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 0.1279 - val_loss: 0.1283\n",
      "Epoch 86/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1273 - val_loss: 0.1291\n",
      "Epoch 87/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1278 - val_loss: 0.1282\n",
      "Epoch 88/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1269 - val_loss: 0.1273\n",
      "Epoch 89/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1277 - val_loss: 0.1274\n",
      "Epoch 90/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1274 - val_loss: 0.1287\n",
      "Epoch 91/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1299\n",
      "Epoch 92/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1283\n",
      "Epoch 93/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1272 - val_loss: 0.1298\n",
      "Epoch 94/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1290 - val_loss: 0.1282\n",
      "Epoch 95/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1279 - val_loss: 0.1301\n",
      "Epoch 96/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1282 - val_loss: 0.1295\n",
      "Epoch 97/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1282 - val_loss: 0.1283\n",
      "Epoch 98/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1283 - val_loss: 0.1320\n",
      "Epoch 99/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1286 - val_loss: 0.1312\n",
      "Epoch 100/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1274 - val_loss: 0.1290\n",
      "Epoch 101/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1273 - val_loss: 0.1264\n",
      "Epoch 102/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1276 - val_loss: 0.1279\n",
      "Epoch 103/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1284 - val_loss: 0.1284\n",
      "Epoch 104/200\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 0.1282 - val_loss: 0.1296\n",
      "Epoch 105/200\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 0.1283 - val_loss: 0.1301\n",
      "Epoch 106/200\n",
      "1404/1404 [==============================] - 14s 10ms/step - loss: 0.1285 - val_loss: 0.1324\n",
      "Epoch 107/200\n",
      " 597/1404 [===========>..................] - ETA: 7s - loss: 0.1290"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Compilar y entrenar el autoencoder\u001b[39;00m\n\u001b[0;32m     28\u001b[0m autoencoder2\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mautoencoder2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_kaggle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_kaggle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer2 = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded2 = layers.Dense(300, activation=\"relu\")(input_layer2)\n",
    "encoded2 = layers.Dense(250, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(200, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(150, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(100, activation=\"relu\")(encoded2)\n",
    "encoded2 = layers.Dense(50, activation=\"relu\")(encoded2)\n",
    "# Capas red decoder\n",
    "decoded2 = layers.Dense(50, activation=\"relu\")(encoded2)\n",
    "decoded2 = layers.Dense(100, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(150, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(200, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(250, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(300, activation=\"relu\")(decoded2)\n",
    "decoded2 = layers.Dense(410, activation=\"linear\")(decoded2)\n",
    "\n",
    "# Encoder\n",
    "encoder2 = models.Model(input_layer2, encoded2)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder2 = models.Model(input_layer2, decoded2)\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder2.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder2.fit(test_kaggle, test_kaggle, epochs=200, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cac4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2.save(\"autoencoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f747b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2 = models.load_model(\"autoencoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b26090fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(209.53645, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.9514135, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE X_train_tot (etiquetados)\n",
    "X_train_tot_pred = autoencoder2.predict(X_train_tot)\n",
    "\n",
    "print(\"MAPE:\", mape(X_train_tot, X_train_tot_pred))\n",
    "print(\"MSE:\", mse(X_train_tot, X_train_tot_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ef5c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 410)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_tmp = np.abs((X_train_tot - X_train_tot_pred) / X_train_tot)\n",
    "mape_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f05e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mape_tmp.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d99e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02195121951219512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mape_tmp.mean(axis=0) < 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5379c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: tf.Tensor(276.24133, shape=(), dtype=float32)\n",
      "MSE: tf.Tensor(0.9518892, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ERROR SOBRE test_kaggle (no etiquetados)\n",
    "test_kaggle_pred = autoencoder2.predict(test_kaggle)\n",
    "\n",
    "print(\"MAPE:\", mape(test_kaggle, test_kaggle_pred))\n",
    "print(\"MSE:\", mse(test_kaggle, test_kaggle_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fca423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 296.4784240722656 +- 118.00175476074219\n"
     ]
    }
   ],
   "source": [
    "samples_num = 100\n",
    "mape_samples = []\n",
    "\n",
    "for _ in range(samples_num):\n",
    "    \n",
    "    args = np.random.choice(a=np.arange(0, test_kaggle.shape[0]), size=86, replace=False)\n",
    "\n",
    "    test_kaggle_reduc = test_kaggle[args, :]\n",
    "\n",
    "    y_pred_kaggle_reduc = autoencoder2.predict(test_kaggle_reduc)\n",
    "    tmp_mape = mape(test_kaggle_reduc, y_pred_kaggle_reduc)\n",
    "    mape_samples.append(tmp_mape)\n",
    "    \n",
    "print(f'MAPE: {np.mean(mape_samples)} +- {np.std(mape_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7608b",
   "metadata": {},
   "source": [
    "En este caso, aumentar la complejidad del autoencoder no mejora excesivamente los resultados.\n",
    "\n",
    "Vamos a ver si se observa algún tipo de mejoría en los resultados de la red de clasificación (encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebcb4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder2 = layers.Input(shape=(410,))\n",
    "encoder2 = encoder_input2\n",
    "for layer in autoencoder2.layers[1:7]:\n",
    "    encoder2 = layer(encoder2)\n",
    "encoder2 = models.Model(inputs=encoder_input2, outputs=encoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88afd9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 410)]             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 300)               123300    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 250)               75250     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 200)               50200     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299,050\n",
      "Trainable params: 299,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f591180",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clasificacion2 = models.Sequential()\n",
    "encoder_clasificacion2.add(encoder2)\n",
    "encoder_clasificacion2.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e3d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 199ms/step - loss: 1.5912 - acc: 0.4706 - val_loss: 1.9512 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.4912 - acc: 0.4902 - val_loss: 1.8720 - val_acc: 0.5882\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.4295 - acc: 0.5098 - val_loss: 1.8148 - val_acc: 0.5882\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.3811 - acc: 0.5294 - val_loss: 1.7620 - val_acc: 0.5882\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.3368 - acc: 0.5686 - val_loss: 1.7130 - val_acc: 0.5882\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2967 - acc: 0.5686 - val_loss: 1.6687 - val_acc: 0.6471\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2649 - acc: 0.5686 - val_loss: 1.6367 - val_acc: 0.6471\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2342 - acc: 0.5686 - val_loss: 1.5971 - val_acc: 0.6471\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.2035 - acc: 0.5882 - val_loss: 1.5641 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1765 - acc: 0.5882 - val_loss: 1.5321 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1498 - acc: 0.6078 - val_loss: 1.4987 - val_acc: 0.5882\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1230 - acc: 0.6275 - val_loss: 1.4654 - val_acc: 0.5294\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0989 - acc: 0.6471 - val_loss: 1.4417 - val_acc: 0.5294\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0770 - acc: 0.6471 - val_loss: 1.4113 - val_acc: 0.5294\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0534 - acc: 0.6667 - val_loss: 1.3851 - val_acc: 0.5294\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0325 - acc: 0.6863 - val_loss: 1.3594 - val_acc: 0.5294\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0129 - acc: 0.6863 - val_loss: 1.3319 - val_acc: 0.5294\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9919 - acc: 0.7059 - val_loss: 1.3062 - val_acc: 0.5294\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9717 - acc: 0.7059 - val_loss: 1.2863 - val_acc: 0.5294\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9560 - acc: 0.7059 - val_loss: 1.2644 - val_acc: 0.5294\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9397 - acc: 0.7059 - val_loss: 1.2441 - val_acc: 0.5294\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9247 - acc: 0.7059 - val_loss: 1.2261 - val_acc: 0.5294\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9110 - acc: 0.6863 - val_loss: 1.2076 - val_acc: 0.5294\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8974 - acc: 0.6863 - val_loss: 1.1896 - val_acc: 0.5294\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8858 - acc: 0.6863 - val_loss: 1.1753 - val_acc: 0.5294\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8749 - acc: 0.6863 - val_loss: 1.1603 - val_acc: 0.5294\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8642 - acc: 0.6863 - val_loss: 1.1420 - val_acc: 0.5294\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8520 - acc: 0.7059 - val_loss: 1.1250 - val_acc: 0.4706\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8405 - acc: 0.7059 - val_loss: 1.1103 - val_acc: 0.4706\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8313 - acc: 0.6863 - val_loss: 1.0972 - val_acc: 0.4118\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8246 - acc: 0.6863 - val_loss: 1.0863 - val_acc: 0.4118\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8164 - acc: 0.6863 - val_loss: 1.0741 - val_acc: 0.4118\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8088 - acc: 0.6863 - val_loss: 1.0612 - val_acc: 0.4118\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8015 - acc: 0.6863 - val_loss: 1.0494 - val_acc: 0.4118\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7939 - acc: 0.6863 - val_loss: 1.0365 - val_acc: 0.4118\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7870 - acc: 0.6863 - val_loss: 1.0266 - val_acc: 0.4118\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7819 - acc: 0.6863 - val_loss: 1.0149 - val_acc: 0.4118\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7748 - acc: 0.6863 - val_loss: 1.0060 - val_acc: 0.4118\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7699 - acc: 0.6863 - val_loss: 0.9973 - val_acc: 0.4118\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7657 - acc: 0.6863 - val_loss: 0.9889 - val_acc: 0.4118\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7616 - acc: 0.6863 - val_loss: 0.9827 - val_acc: 0.4118\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7575 - acc: 0.6667 - val_loss: 0.9732 - val_acc: 0.4118\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7533 - acc: 0.6667 - val_loss: 0.9648 - val_acc: 0.4118\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7499 - acc: 0.6667 - val_loss: 0.9610 - val_acc: 0.4118\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7472 - acc: 0.6667 - val_loss: 0.9544 - val_acc: 0.4706\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7454 - acc: 0.6667 - val_loss: 0.9474 - val_acc: 0.4706\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7406 - acc: 0.6667 - val_loss: 0.9428 - val_acc: 0.4706\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7380 - acc: 0.6667 - val_loss: 0.9365 - val_acc: 0.4706\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7345 - acc: 0.6667 - val_loss: 0.9306 - val_acc: 0.4706\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7321 - acc: 0.6667 - val_loss: 0.9260 - val_acc: 0.5294\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7308 - acc: 0.6471 - val_loss: 0.9193 - val_acc: 0.5294\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7307 - acc: 0.6275 - val_loss: 0.9182 - val_acc: 0.5294\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.7237 - acc: 0.6471 - val_loss: 0.9136 - val_acc: 0.5294\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7245 - acc: 0.6471 - val_loss: 0.9122 - val_acc: 0.5294\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7198 - acc: 0.6471 - val_loss: 0.9084 - val_acc: 0.5294\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7184 - acc: 0.6471 - val_loss: 0.9053 - val_acc: 0.5294\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7159 - acc: 0.6667 - val_loss: 0.9021 - val_acc: 0.5294\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7140 - acc: 0.6667 - val_loss: 0.8962 - val_acc: 0.5294\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7109 - acc: 0.6471 - val_loss: 0.8936 - val_acc: 0.5294\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7099 - acc: 0.6471 - val_loss: 0.8881 - val_acc: 0.5294\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7063 - acc: 0.6275 - val_loss: 0.8842 - val_acc: 0.5294\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7096 - acc: 0.6275 - val_loss: 0.8799 - val_acc: 0.5294\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7024 - acc: 0.6275 - val_loss: 0.8759 - val_acc: 0.5294\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.7012 - acc: 0.6275 - val_loss: 0.8747 - val_acc: 0.5294\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6982 - acc: 0.6275 - val_loss: 0.8711 - val_acc: 0.5294\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6960 - acc: 0.6275 - val_loss: 0.8678 - val_acc: 0.5294\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6939 - acc: 0.6275 - val_loss: 0.8644 - val_acc: 0.5294\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6921 - acc: 0.6275 - val_loss: 0.8614 - val_acc: 0.5294\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6906 - acc: 0.6275 - val_loss: 0.8607 - val_acc: 0.4706\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6881 - acc: 0.6275 - val_loss: 0.8557 - val_acc: 0.4706\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6853 - acc: 0.6275 - val_loss: 0.8524 - val_acc: 0.4706\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6847 - acc: 0.6471 - val_loss: 0.8479 - val_acc: 0.5294\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6820 - acc: 0.6275 - val_loss: 0.8441 - val_acc: 0.5294\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6797 - acc: 0.6275 - val_loss: 0.8401 - val_acc: 0.5294\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6797 - acc: 0.6275 - val_loss: 0.8357 - val_acc: 0.5294\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6762 - acc: 0.5882 - val_loss: 0.8318 - val_acc: 0.5294\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6732 - acc: 0.5882 - val_loss: 0.8304 - val_acc: 0.4706\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6711 - acc: 0.5882 - val_loss: 0.8282 - val_acc: 0.4706\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6720 - acc: 0.6078 - val_loss: 0.8284 - val_acc: 0.4706\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6698 - acc: 0.6078 - val_loss: 0.8273 - val_acc: 0.4706\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6667 - acc: 0.6275 - val_loss: 0.8262 - val_acc: 0.4706\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6654 - acc: 0.6667 - val_loss: 0.8238 - val_acc: 0.4706\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6645 - acc: 0.6667 - val_loss: 0.8222 - val_acc: 0.4706\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6614 - acc: 0.6667 - val_loss: 0.8183 - val_acc: 0.4706\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6618 - acc: 0.6667 - val_loss: 0.8138 - val_acc: 0.4706\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6583 - acc: 0.6275 - val_loss: 0.8124 - val_acc: 0.4706\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6630 - acc: 0.6667 - val_loss: 0.8088 - val_acc: 0.4706\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6553 - acc: 0.6275 - val_loss: 0.8063 - val_acc: 0.4706\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6539 - acc: 0.6078 - val_loss: 0.8048 - val_acc: 0.4706\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6523 - acc: 0.6471 - val_loss: 0.8032 - val_acc: 0.4706\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6509 - acc: 0.6471 - val_loss: 0.8020 - val_acc: 0.4706\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6504 - acc: 0.6471 - val_loss: 0.7991 - val_acc: 0.4706\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6487 - acc: 0.6275 - val_loss: 0.7977 - val_acc: 0.4706\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6490 - acc: 0.6275 - val_loss: 0.7982 - val_acc: 0.4706\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6486 - acc: 0.6275 - val_loss: 0.7985 - val_acc: 0.4706\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6444 - acc: 0.6667 - val_loss: 0.7960 - val_acc: 0.4706\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6430 - acc: 0.6667 - val_loss: 0.7939 - val_acc: 0.4706\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6467 - acc: 0.6667 - val_loss: 0.7904 - val_acc: 0.4706\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6411 - acc: 0.6471 - val_loss: 0.7883 - val_acc: 0.4706\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6397 - acc: 0.6275 - val_loss: 0.7877 - val_acc: 0.4706\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8179 - acc: 0.5556\n",
      "Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Congelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_clasificacion2.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_clasificacion2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_clasificacion2.fit(X_train, y_train, epochs=100, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder_clasificacion2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef566bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.6870 - acc: 0.6863 - val_loss: 0.9050 - val_acc: 0.5882\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7020 - acc: 0.6667 - val_loss: 0.9108 - val_acc: 0.4118\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3870 - acc: 0.8039 - val_loss: 0.8804 - val_acc: 0.4118\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3088 - acc: 0.9020 - val_loss: 0.8448 - val_acc: 0.4118\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2381 - acc: 0.9216 - val_loss: 0.8146 - val_acc: 0.4118\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2542 - acc: 0.9020 - val_loss: 0.7240 - val_acc: 0.5882\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2010 - acc: 0.9608 - val_loss: 1.0387 - val_acc: 0.4706\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1918 - acc: 0.9020 - val_loss: 0.6964 - val_acc: 0.6471\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1386 - acc: 0.9804 - val_loss: 0.8465 - val_acc: 0.5294\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1102 - acc: 0.9804 - val_loss: 0.9602 - val_acc: 0.5294\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0890 - acc: 0.9804 - val_loss: 0.9634 - val_acc: 0.5294\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0704 - acc: 1.0000 - val_loss: 1.0236 - val_acc: 0.5294\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0531 - acc: 1.0000 - val_loss: 1.2813 - val_acc: 0.5294\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0373 - acc: 1.0000 - val_loss: 1.0273 - val_acc: 0.5294\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0260 - acc: 1.0000 - val_loss: 1.2934 - val_acc: 0.5294\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2386 - acc: 0.5556\n",
      "Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Descongelar los pesos de todas las capas a excepción de la última\n",
    "for layer in encoder_clasificacion2.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder_clasificacion2.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder_clasificacion2.fit(X_train, y_train, epochs=15, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy_encoder_clasificacion2 = encoder_clasificacion2.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy_encoder_clasificacion2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "358a4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clasificacion2.save(\"encoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8ec5fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "2/2 [==============================] - 1s 158ms/step - loss: 0.6755 - acc: 0.5882 - val_loss: 0.6710 - val_acc: 0.7059\n",
      "Epoch 2/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3437 - acc: 0.8824 - val_loss: 0.7513 - val_acc: 0.7647\n",
      "Epoch 3/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0859 - acc: 0.9804 - val_loss: 1.7384 - val_acc: 0.5882\n",
      "Epoch 4/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2443 - acc: 0.9412 - val_loss: 0.8544 - val_acc: 0.7647\n",
      "Epoch 5/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.8227 - val_acc: 0.8235\n",
      "Epoch 6/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.8533 - val_acc: 0.8235\n",
      "Epoch 7/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.8755 - val_acc: 0.8235\n",
      "Epoch 8/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.9049 - val_acc: 0.8235\n",
      "Epoch 9/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.9323 - val_acc: 0.8235\n",
      "Epoch 10/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.9565 - val_acc: 0.8235\n",
      "Epoch 11/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9822 - val_acc: 0.8235\n",
      "Epoch 12/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0117 - val_acc: 0.8235\n",
      "Epoch 13/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.0354 - val_acc: 0.8235\n",
      "Epoch 14/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8.1135e-04 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.8235\n",
      "Epoch 15/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.5987e-04 - acc: 1.0000 - val_loss: 1.0819 - val_acc: 0.8235\n",
      "Epoch 16/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.3672e-04 - acc: 1.0000 - val_loss: 1.1042 - val_acc: 0.8235\n",
      "Epoch 17/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.4313e-04 - acc: 1.0000 - val_loss: 1.1298 - val_acc: 0.8235\n",
      "Epoch 18/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.6163e-04 - acc: 1.0000 - val_loss: 1.1525 - val_acc: 0.8235\n",
      "Epoch 19/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.9667e-04 - acc: 1.0000 - val_loss: 1.1737 - val_acc: 0.8235\n",
      "Epoch 20/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.4696e-04 - acc: 1.0000 - val_loss: 1.1960 - val_acc: 0.8235\n",
      "Epoch 21/115\n",
      "2/2 [==============================] - 1s 984ms/step - loss: 2.0462e-04 - acc: 1.0000 - val_loss: 1.2188 - val_acc: 0.8235\n",
      "Epoch 22/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.7023e-04 - acc: 1.0000 - val_loss: 1.2390 - val_acc: 0.8235\n",
      "Epoch 23/115\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4151e-04 - acc: 1.0000 - val_loss: 1.2567 - val_acc: 0.8235\n",
      "Epoch 24/115\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.1839e-04 - acc: 1.0000 - val_loss: 1.2784 - val_acc: 0.8235\n",
      "Epoch 25/115\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 9.8109e-05 - acc: 1.0000 - val_loss: 1.2964 - val_acc: 0.8235\n",
      "Epoch 26/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 8.2558e-05 - acc: 1.0000 - val_loss: 1.3124 - val_acc: 0.8235\n",
      "Epoch 27/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.9319e-05 - acc: 1.0000 - val_loss: 1.3319 - val_acc: 0.8235\n",
      "Epoch 28/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.8634e-05 - acc: 1.0000 - val_loss: 1.3501 - val_acc: 0.8235\n",
      "Epoch 29/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.9470e-05 - acc: 1.0000 - val_loss: 1.3677 - val_acc: 0.8235\n",
      "Epoch 30/115\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.2004e-05 - acc: 1.0000 - val_loss: 1.3878 - val_acc: 0.8235\n",
      "Epoch 31/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3.5388e-05 - acc: 1.0000 - val_loss: 1.4042 - val_acc: 0.8235\n",
      "Epoch 32/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.9855e-05 - acc: 1.0000 - val_loss: 1.4233 - val_acc: 0.8235\n",
      "Epoch 33/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.5447e-05 - acc: 1.0000 - val_loss: 1.4413 - val_acc: 0.8235\n",
      "Epoch 34/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.1576e-05 - acc: 1.0000 - val_loss: 1.4603 - val_acc: 0.8235\n",
      "Epoch 35/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8436e-05 - acc: 1.0000 - val_loss: 1.4807 - val_acc: 0.8235\n",
      "Epoch 36/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.5711e-05 - acc: 1.0000 - val_loss: 1.4996 - val_acc: 0.8235\n",
      "Epoch 37/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.3446e-05 - acc: 1.0000 - val_loss: 1.5154 - val_acc: 0.8235\n",
      "Epoch 38/115\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.1432e-05 - acc: 1.0000 - val_loss: 1.5372 - val_acc: 0.8235\n",
      "Epoch 39/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 9.7598e-06 - acc: 1.0000 - val_loss: 1.5552 - val_acc: 0.8235\n",
      "Epoch 40/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 8.4250e-06 - acc: 1.0000 - val_loss: 1.5715 - val_acc: 0.8235\n",
      "Epoch 41/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7.3294e-06 - acc: 1.0000 - val_loss: 1.5892 - val_acc: 0.8235\n",
      "Epoch 42/115\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.3445e-06 - acc: 1.0000 - val_loss: 1.6077 - val_acc: 0.8235\n",
      "Epoch 43/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.4825e-06 - acc: 1.0000 - val_loss: 1.6265 - val_acc: 0.8235\n",
      "Epoch 44/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7287e-06 - acc: 1.0000 - val_loss: 1.6473 - val_acc: 0.8235\n",
      "Epoch 45/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.0414e-06 - acc: 1.0000 - val_loss: 1.6659 - val_acc: 0.8235\n",
      "Epoch 46/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.5160e-06 - acc: 1.0000 - val_loss: 1.6847 - val_acc: 0.8235\n",
      "Epoch 47/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3.0459e-06 - acc: 1.0000 - val_loss: 1.7015 - val_acc: 0.8235\n",
      "Epoch 48/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.6594e-06 - acc: 1.0000 - val_loss: 1.7211 - val_acc: 0.8235\n",
      "Epoch 49/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.3103e-06 - acc: 1.0000 - val_loss: 1.7402 - val_acc: 0.8235\n",
      "Epoch 50/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.0052e-06 - acc: 1.0000 - val_loss: 1.7577 - val_acc: 0.8235\n",
      "Epoch 51/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.7569e-06 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 0.8235\n",
      "Epoch 52/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5448e-06 - acc: 1.0000 - val_loss: 1.7899 - val_acc: 0.8235\n",
      "Epoch 53/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.3573e-06 - acc: 1.0000 - val_loss: 1.8075 - val_acc: 0.8235\n",
      "Epoch 54/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1840e-06 - acc: 1.0000 - val_loss: 1.8238 - val_acc: 0.8235\n",
      "Epoch 55/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0347e-06 - acc: 1.0000 - val_loss: 1.8416 - val_acc: 0.8235\n",
      "Epoch 56/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 9.0479e-07 - acc: 1.0000 - val_loss: 1.8572 - val_acc: 0.8235\n",
      "Epoch 57/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7.8759e-07 - acc: 1.0000 - val_loss: 1.8719 - val_acc: 0.8235\n",
      "Epoch 58/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.8707e-07 - acc: 1.0000 - val_loss: 1.8900 - val_acc: 0.8235\n",
      "Epoch 59/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.0313e-07 - acc: 1.0000 - val_loss: 1.9036 - val_acc: 0.8235\n",
      "Epoch 60/115\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.3211e-07 - acc: 1.0000 - val_loss: 1.9235 - val_acc: 0.8235\n",
      "Epoch 61/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.6518e-07 - acc: 1.0000 - val_loss: 1.9378 - val_acc: 0.8235\n",
      "Epoch 62/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.1236e-07 - acc: 1.0000 - val_loss: 1.9559 - val_acc: 0.8235\n",
      "Epoch 63/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.6325e-07 - acc: 1.0000 - val_loss: 1.9725 - val_acc: 0.8235\n",
      "Epoch 64/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.2404e-07 - acc: 1.0000 - val_loss: 1.9840 - val_acc: 0.8235\n",
      "Epoch 65/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.8882e-07 - acc: 1.0000 - val_loss: 2.0010 - val_acc: 0.8235\n",
      "Epoch 66/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.5654e-07 - acc: 1.0000 - val_loss: 2.0178 - val_acc: 0.8235\n",
      "Epoch 67/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.2762e-07 - acc: 1.0000 - val_loss: 2.0313 - val_acc: 0.8235\n",
      "Epoch 68/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.0260e-07 - acc: 1.0000 - val_loss: 2.0441 - val_acc: 0.8235\n",
      "Epoch 69/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.8126e-07 - acc: 1.0000 - val_loss: 2.0595 - val_acc: 0.8235\n",
      "Epoch 70/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.6142e-07 - acc: 1.0000 - val_loss: 2.0732 - val_acc: 0.8235\n",
      "Epoch 71/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4483e-07 - acc: 1.0000 - val_loss: 2.0898 - val_acc: 0.8235\n",
      "Epoch 72/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2844e-07 - acc: 1.0000 - val_loss: 2.1039 - val_acc: 0.8235\n",
      "Epoch 73/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1393e-07 - acc: 1.0000 - val_loss: 2.1170 - val_acc: 0.8235\n",
      "Epoch 74/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0233e-07 - acc: 1.0000 - val_loss: 2.1285 - val_acc: 0.8235\n",
      "Epoch 75/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.2224e-08 - acc: 1.0000 - val_loss: 2.1402 - val_acc: 0.8235\n",
      "Epoch 76/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8.3591e-08 - acc: 1.0000 - val_loss: 2.1522 - val_acc: 0.8235\n",
      "Epoch 77/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.5536e-08 - acc: 1.0000 - val_loss: 2.1638 - val_acc: 0.8235\n",
      "Epoch 78/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.8487e-08 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.8235\n",
      "Epoch 79/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.1905e-08 - acc: 1.0000 - val_loss: 2.1849 - val_acc: 0.8235\n",
      "Epoch 80/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.6360e-08 - acc: 1.0000 - val_loss: 2.1938 - val_acc: 0.8235\n",
      "Epoch 81/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.0738e-08 - acc: 1.0000 - val_loss: 2.2057 - val_acc: 0.8235\n",
      "Epoch 82/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.6473e-08 - acc: 1.0000 - val_loss: 2.2186 - val_acc: 0.8235\n",
      "Epoch 83/115\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 4.2086e-08 - acc: 1.0000 - val_loss: 2.2300 - val_acc: 0.8235\n",
      "Epoch 84/115\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3.8579e-08 - acc: 1.0000 - val_loss: 2.2430 - val_acc: 0.8235\n",
      "Epoch 85/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3.5106e-08 - acc: 1.0000 - val_loss: 2.2563 - val_acc: 0.8235\n",
      "Epoch 86/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.1898e-08 - acc: 1.0000 - val_loss: 2.2696 - val_acc: 0.8235\n",
      "Epoch 87/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.9324e-08 - acc: 1.0000 - val_loss: 2.2822 - val_acc: 0.8235\n",
      "Epoch 88/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.6876e-08 - acc: 1.0000 - val_loss: 2.2936 - val_acc: 0.8235\n",
      "Epoch 89/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.4789e-08 - acc: 1.0000 - val_loss: 2.3050 - val_acc: 0.8235\n",
      "Epoch 90/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.3140e-08 - acc: 1.0000 - val_loss: 2.3167 - val_acc: 0.8235\n",
      "Epoch 91/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.1497e-08 - acc: 1.0000 - val_loss: 2.3283 - val_acc: 0.8235\n",
      "Epoch 92/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.0148e-08 - acc: 1.0000 - val_loss: 2.3406 - val_acc: 0.8235\n",
      "Epoch 93/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.8867e-08 - acc: 1.0000 - val_loss: 2.3516 - val_acc: 0.8235\n",
      "Epoch 94/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.7847e-08 - acc: 1.0000 - val_loss: 2.3576 - val_acc: 0.8235\n",
      "Epoch 95/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.6340e-08 - acc: 1.0000 - val_loss: 2.3636 - val_acc: 0.8235\n",
      "Epoch 96/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.4993e-08 - acc: 1.0000 - val_loss: 2.3741 - val_acc: 0.8235\n",
      "Epoch 97/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.4174e-08 - acc: 1.0000 - val_loss: 2.3827 - val_acc: 0.8235\n",
      "Epoch 98/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.3215e-08 - acc: 1.0000 - val_loss: 2.3927 - val_acc: 0.8235\n",
      "Epoch 99/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2238e-08 - acc: 1.0000 - val_loss: 2.4043 - val_acc: 0.8235\n",
      "Epoch 100/115\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1664e-08 - acc: 1.0000 - val_loss: 2.4105 - val_acc: 0.8235\n",
      "Epoch 101/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0828e-08 - acc: 1.0000 - val_loss: 2.4168 - val_acc: 0.8235\n",
      "Epoch 102/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.0015e-08 - acc: 1.0000 - val_loss: 2.4302 - val_acc: 0.8235\n",
      "Epoch 103/115\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.4968e-09 - acc: 1.0000 - val_loss: 2.4387 - val_acc: 0.8235\n",
      "Epoch 104/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 8.9970e-09 - acc: 1.0000 - val_loss: 2.4495 - val_acc: 0.8235\n",
      "Epoch 105/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 8.5988e-09 - acc: 1.0000 - val_loss: 2.4586 - val_acc: 0.8235\n",
      "Epoch 106/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.1504e-09 - acc: 1.0000 - val_loss: 2.4705 - val_acc: 0.8235\n",
      "Epoch 107/115\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 7.8615e-09 - acc: 1.0000 - val_loss: 2.4815 - val_acc: 0.8235\n",
      "Epoch 108/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.6211e-09 - acc: 1.0000 - val_loss: 2.4933 - val_acc: 0.8235\n",
      "Epoch 109/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.4439e-09 - acc: 1.0000 - val_loss: 2.5025 - val_acc: 0.8235\n",
      "Epoch 110/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7.2461e-09 - acc: 1.0000 - val_loss: 2.5136 - val_acc: 0.8235\n",
      "Epoch 111/115\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.0872e-09 - acc: 1.0000 - val_loss: 2.5236 - val_acc: 0.8235\n",
      "Epoch 112/115\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.8638e-09 - acc: 1.0000 - val_loss: 2.5329 - val_acc: 0.8235\n",
      "Epoch 113/115\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.6972e-09 - acc: 1.0000 - val_loss: 2.5430 - val_acc: 0.8235\n",
      "Epoch 114/115\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.5959e-09 - acc: 1.0000 - val_loss: 2.5523 - val_acc: 0.8235\n",
      "Epoch 115/115\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.4308e-09 - acc: 1.0000 - val_loss: 2.5622 - val_acc: 0.8235\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.1188 - acc: 0.6111\n",
      "Accuracy: 61.11%\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "input_layer4 = layers.Input(shape=(410,))\n",
    "# Capas red encoder\n",
    "encoded4 = layers.Dense(300, activation=\"relu\")(input_layer4)\n",
    "encoded4 = layers.Dense(250, activation=\"relu\")(encoded4)\n",
    "encoded4 = layers.Dense(200, activation=\"relu\")(encoded4)\n",
    "encoded4 = layers.Dense(150, activation=\"relu\")(encoded4)\n",
    "encoded4 = layers.Dense(100, activation=\"relu\")(encoded4)\n",
    "encoded4 = layers.Dense(50, activation=\"relu\")(encoded4)\n",
    "encoded4 = layers.Dense(1, activation=\"sigmoid\")(encoded4)\n",
    "\n",
    "encoder4 = models.Model(input_layer4, encoded4)\n",
    "\n",
    "# Entrenar el modelo\n",
    "encoder4.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "encoder4.fit(X_train, y_train, epochs=115, validation_split=0.25)\n",
    "\n",
    "# Precisión en partición de test\n",
    "loss, accuracy = encoder4.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:0.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
